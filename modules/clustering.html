

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  
    <title>2.3. 聚类(Clustering) &#8212; scikit-learn 0.20.1 documentation</title>
  <!-- htmltitle is before nature.css - we use this hack to load bootstrap first -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="../static/css/bootstrap.min.css" media="screen" />
  <link rel="stylesheet" href="../static/css/bootstrap-responsive.css"/>

    <link rel="stylesheet" href="../static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../static/gallery.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../static/documentation_options.js"></script>
    <script type="text/javascript" src="../static/jquery.js"></script>
    <script type="text/javascript" src="../static/underscore.js"></script>
    <script type="text/javascript" src="../static/doctools.js"></script>
    <script type="text/javascript" src="../static/js/copybutton.js"></script>
    <script type="text/javascript" src="../static/js/extra.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_SVG"></script>
    <link rel="shortcut icon" href="../static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2.4. 双向聚类(Biclustering)" href="biclustering.html" />
    <link rel="prev" title="2.2. 流形学习(Manifold learning)" href="manifold.html" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <script src="../static/js/bootstrap.min.js" type="text/javascript"></script>
  <script>
     VERSION_SUBDIR = (function(groups) {
         return groups ? groups[1] : null;
     })(location.href.match(/^https?:\/\/scikit-learn.org\/([^\/]+)/));
  </script>
  <link rel="canonical" href="http://scikit-learn.org/stable/modules/clustering.html" />

  <script type="text/javascript">
    $("div.buttonNext, div.buttonPrevious").hover(
       function () {
           $(this).css('background-color', '#FF9C34');
       },
       function () {
           $(this).css('background-color', '#A7D6E2');
       }
    );
    function showMenu() {
      var topNav = document.getElementById("scikit-navbar");
      if (topNav.className === "navbar") {
          topNav.className += " responsive";
      } else {
          topNav.className = "navbar";
      }
    };
  </script>

  <!-- 百度站长统计代码 -->
  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?e7836e37a4cb7584127a787e9b44e3f1";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


  </head><body>

<div class="header-wrapper">
    <div class="header">
        <p class="logo"><a href="../index.html">
            <img src="../static/scikit-learn-logo-small.png" alt="Logo"/>
        </a>
        </p><div class="navbar" id="scikit-navbar">
            <ul>
                <li><a href="../index.html">首页</a></li>
                <li><a href="../install.html">安装</a></li>
                <li class="btn-li"><div class="btn-group">
              <a href="../documentation.html">文档</a>
              <a class="btn dropdown-toggle" data-toggle="dropdown">
                 <span class="caret"></span>
              </a>
              <ul class="dropdown-menu">
            <li class="link-title">Scikit-learn <script>document.write(DOCUMENTATION_OPTIONS.VERSION + (VERSION_SUBDIR ? " (" + VERSION_SUBDIR + ")" : ""));</script></li>
            <li><a href="../tutorial/index.html">教程</a></li>
            <li><a href="../user_guide.html">用户指南</a></li>
            <li><a href="classes.html">API</a></li>
            <li><a href="../glossary.html">术语表</a></li>
            <li><a href="../faq.html">FAQ</a></li>
            <li><a href="../developers/contributing.html">贡献者</a></li>
            <li class="divider"></li>
                <script>if (VERSION_SUBDIR != "stable") document.write('<li><a href="http://scikit-learn.org/stable/documentation.html">稳定版</a></li>')</script>
                <script>if (VERSION_SUBDIR != "dev") document.write('<li><a href="http://scikit-learn.org/dev/documentation.html">开发版</a></li>')</script>
                <li><a href="http://scikit-learn.org/dev/versions.html">所有可用版本</a></li>
                <li><a href="../downloads/scikit-learn-docs.pdf">PDF 文档</a></li>
              </ul>
            </div>
        </li>
            <li><a href="../auto_examples/index.html">案例</a></li>
            </ul>
            <a href="javascript:void(0);" onclick="showMenu()">
                <div class="nav-icon">
                    <div class="hamburger-line"></div>
                    <div class="hamburger-line"></div>
                    <div class="hamburger-line"></div>
                </div>
            </a>
            <div class="search_form">
                <div class="gcse-search" id="cse" style="width: 100%;"></div>
            </div>
        </div> <!-- end navbar --></div>
</div>


<!-- GitHub "fork me" ribbon -->
<a href="https://github.com/scikit-learn/scikit-learn">
  <img class="fork-me"
       style="position: absolute; top: 0; right: 0; border: 0;"
       src="../static/img/forkme.png"
       alt="Fork me on GitHub" />
</a>

<div class="content-wrapper">
    <div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
        <div class="rel">
    
        <div class="rellink">
        <a href="manifold.html"
        accesskey="P">Previous
        <br/>
        <span class="smallrellink">
        2.2. 流形学习(Man...
        </span>
            <span class="hiddenrellink">
            2.2. 流形学习(Manifold learning)
            </span>
        </a>
        </div>
            <div class="spacer">
            &nbsp;
            </div>
        <div class="rellink">
        <a href="biclustering.html"
        accesskey="N">Next
        <br/>
        <span class="smallrellink">
        2.4. 双向聚类(Bic...
        </span>
            <span class="hiddenrellink">
            2.4. 双向聚类(Biclustering)
            </span>
        </a>
        </div>

    <!-- Ad a link to the 'up' page -->
        <div class="spacer">
        &nbsp;
        </div>
        <div class="rellink">
        <a href="../unsupervised_learning.html">
        Up
        <br/>
        <span class="smallrellink">
        2. 无监督学习(Unsu...
        </span>
            <span class="hiddenrellink">
            2. 无监督学习(Unsupervised learning)
            </span>
            
        </a>
        </div>
    </div>
    
      <p class="doc-version"><b>scikit-learn v0.20.1</b><br/>
      <a href="http://scikit-learn.org/dev/versions.html">其他版本</a></p>
    <p class="citing">该中文文档由人工智能社区的<a href="http://www.studyai.com/antares" target="_blank">Antares</a>翻译!</p>
    <ul>
<li><a class="reference internal" href="#">2.3. 聚类(Clustering)</a><ul>
<li><a class="reference internal" href="#id2">2.3.1. 聚类算法一览</a></li>
<li><a class="reference internal" href="#k">2.3.2. K-均值</a><ul>
<li><a class="reference internal" href="#mini-batch-kmeans">2.3.2.1. 小批量 K-均值</a></li>
</ul>
</li>
<li><a class="reference internal" href="#affinity-propagation">2.3.3. 吸引子传播</a></li>
<li><a class="reference internal" href="#mean-shift">2.3.4. 均值漂移</a></li>
<li><a class="reference internal" href="#spectral-clustering">2.3.5. 谱聚类</a><ul>
<li><a class="reference internal" href="#id9">2.3.5.1. 不同的标签分配策略</a></li>
<li><a class="reference internal" href="#id10">2.3.5.2. 谱聚类用于图聚类问题</a></li>
</ul>
</li>
<li><a class="reference internal" href="#hierarchical-clustering">2.3.6. 层次聚类(Hierarchical clustering)</a><ul>
<li><a class="reference internal" href="#different-linkage-type-ward-complete-average-and-single-linkage">2.3.6.1. Different linkage type: Ward, complete, average, and single linkage</a></li>
<li><a class="reference internal" href="#id13">2.3.6.2. 添加连通性约束</a></li>
<li><a class="reference internal" href="#id14">2.3.6.3. 改变聚类测度</a></li>
</ul>
</li>
<li><a class="reference internal" href="#dbscan">2.3.7. DBSCAN</a></li>
<li><a class="reference internal" href="#birch">2.3.8. Birch</a></li>
<li><a class="reference internal" href="#clustering-evaluation">2.3.9. 聚类算法性能评估</a><ul>
<li><a class="reference internal" href="#adjusted-rand-index">2.3.9.1. Adjusted Rand index</a><ul>
<li><a class="reference internal" href="#id20">2.3.9.1.1. 优点</a></li>
<li><a class="reference internal" href="#id21">2.3.9.1.2. 缺点</a></li>
<li><a class="reference internal" href="#id22">2.3.9.1.3. 数学表达形式</a></li>
</ul>
</li>
<li><a class="reference internal" href="#mutual-info-score">2.3.9.2. 基于互信息的得分</a><ul>
<li><a class="reference internal" href="#id24">2.3.9.2.1. 优点</a></li>
<li><a class="reference internal" href="#id25">2.3.9.2.2. 缺点</a></li>
<li><a class="reference internal" href="#id26">2.3.9.2.3. 数学表达形式</a></li>
</ul>
</li>
<li><a class="reference internal" href="#homogeneity-completeness-and-v-measure">2.3.9.3. Homogeneity, completeness and V-measure</a><ul>
<li><a class="reference internal" href="#id31">2.3.9.3.1. 优点</a></li>
<li><a class="reference internal" href="#id32">2.3.9.3.2. 缺点</a></li>
<li><a class="reference internal" href="#id33">2.3.9.3.3. 数学表达形式</a></li>
</ul>
</li>
<li><a class="reference internal" href="#fowlkes-mallows-scores">2.3.9.4. Fowlkes-Mallows scores</a><ul>
<li><a class="reference internal" href="#id35">2.3.9.4.1. 优点</a></li>
<li><a class="reference internal" href="#id36">2.3.9.4.2. 缺点</a></li>
</ul>
</li>
<li><a class="reference internal" href="#silhouette-coefficient">2.3.9.5. Silhouette Coefficient</a><ul>
<li><a class="reference internal" href="#id38">2.3.9.5.1. 优点</a></li>
<li><a class="reference internal" href="#id39">2.3.9.5.2. 缺点</a></li>
</ul>
</li>
<li><a class="reference internal" href="#calinski-harabaz-index">2.3.9.6. Calinski-Harabaz Index</a><ul>
<li><a class="reference internal" href="#id41">2.3.9.6.1. 优点</a></li>
<li><a class="reference internal" href="#id42">2.3.9.6.2. 缺点</a></li>
</ul>
</li>
<li><a class="reference internal" href="#davies-bouldin-index">2.3.9.7. Davies-Bouldin Index</a><ul>
<li><a class="reference internal" href="#id44">2.3.9.7.1. 优点</a></li>
<li><a class="reference internal" href="#id45">2.3.9.7.2. 缺点</a></li>
</ul>
</li>
<li><a class="reference internal" href="#contingency-matrix">2.3.9.8. Contingency Matrix</a><ul>
<li><a class="reference internal" href="#id47">2.3.9.8.1. 优点</a></li>
<li><a class="reference internal" href="#id48">2.3.9.8.2. 缺点</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

    </div>
</div>

<input type="checkbox" id="nav-trigger" class="nav-trigger" checked />
<label for="nav-trigger"></label>




      <div class="content">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="clustering">
<span id="id1"></span><h1>2.3. 聚类(Clustering)<a class="headerlink" href="#clustering" title="Permalink to this headline">¶</a></h1>
<p><a class="reference internal" href="classes.html#module-sklearn.cluster" title="sklearn.cluster"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.cluster</span></code></a> 模块提供了对无标签数据的聚类算法
(<a class="reference external" href="https://en.wikipedia.org/wiki/Cluster_analysis">Clustering</a>)。</p>
<p>该模块中每一个聚类算法都有两个变体: 一个是类(class)另一个是函数(function)。
类实现了 <code class="docutils literal notranslate"><span class="pre">fit</span></code> 方法来从训练数据中学习聚类；函数接受训练数据返回对应于不同聚类的一个整数标签数组。
对类来说，训练过程得到的标签数据可以在属性  <code class="docutils literal notranslate"><span class="pre">labels_</span></code> 中找到。</p>
<div class="topic">
<p class="topic-title first">输入数据</p>
<p>需要注意的一点是，在该模块中实现的算法可以以不同类型的矩阵作为输入。
所有方法都接受标准形状的数据矩阵，shape为 <code class="docutils literal notranslate"><span class="pre">[n_samples,</span> <span class="pre">n_features]</span></code> 。
这些内容可以从 <a class="reference internal" href="classes.html#module-sklearn.feature_extraction" title="sklearn.feature_extraction"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.feature_extraction</span></code></a> 模块中的类中获得。
对于 <a class="reference internal" href="generated/sklearn.cluster.AffinityPropagation.html#sklearn.cluster.AffinityPropagation" title="sklearn.cluster.AffinityPropagation"><code class="xref py py-class docutils literal notranslate"><span class="pre">AffinityPropagation</span></code></a>, <a class="reference internal" href="generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering" title="sklearn.cluster.SpectralClustering"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpectralClustering</span></code></a>
和 <a class="reference internal" href="generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN" title="sklearn.cluster.DBSCAN"><code class="xref py py-class docutils literal notranslate"><span class="pre">DBSCAN</span></code></a> 类，你还可以输入shape为 <code class="docutils literal notranslate"><span class="pre">[n_samples,</span> <span class="pre">n_samples]</span></code> 的相似矩阵。
这些可以从 <a class="reference internal" href="classes.html#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise</span></code></a> 模块中的函数中获得。</p>
</div>
<div class="section" id="id2">
<h2>2.3.1. 聚类算法一览<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<div class="figure align-center" id="id49">
<a class="reference external image-reference" href="../auto_examples/cluster/plot_cluster_comparison.html"><img alt="../images/sphx_glr_plot_cluster_comparison_0011.png" src="../images/sphx_glr_plot_cluster_comparison_0011.png" style="width: 1050.0px; height: 625.0px;" /></a>
<p class="caption"><span class="caption-text">scikit-learn中的聚类算法比较</span></p>
</div>
<table border="1" class="colwidths-given docutils">
<colgroup>
<col width="15%" />
<col width="16%" />
<col width="20%" />
<col width="27%" />
<col width="22%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Method name</th>
<th class="head">Parameters</th>
<th class="head">Scalability</th>
<th class="head">Usecase</th>
<th class="head">Geometry (metric used)</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><a class="reference internal" href="#k-means"><span class="std std-ref">K-Means</span></a></td>
<td>number of clusters</td>
<td>Very large <code class="docutils literal notranslate"><span class="pre">n_samples</span></code>, medium <code class="docutils literal notranslate"><span class="pre">n_clusters</span></code> with
<a class="reference internal" href="#mini-batch-kmeans"><span class="std std-ref">MiniBatch code</span></a></td>
<td>General-purpose, even cluster size, flat geometry, not too many clusters</td>
<td>Distances between points</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#affinity-propagation"><span class="std std-ref">Affinity propagation</span></a></td>
<td>damping, sample preference</td>
<td>Not scalable with n_samples</td>
<td>Many clusters, uneven cluster size, non-flat geometry</td>
<td>Graph distance (e.g. nearest-neighbor graph)</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#mean-shift"><span class="std std-ref">Mean-shift</span></a></td>
<td>bandwidth</td>
<td>Not scalable with <code class="docutils literal notranslate"><span class="pre">n_samples</span></code></td>
<td>Many clusters, uneven cluster size, non-flat geometry</td>
<td>Distances between points</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#spectral-clustering"><span class="std std-ref">Spectral clustering</span></a></td>
<td>number of clusters</td>
<td>Medium <code class="docutils literal notranslate"><span class="pre">n_samples</span></code>, small <code class="docutils literal notranslate"><span class="pre">n_clusters</span></code></td>
<td>Few clusters, even cluster size, non-flat geometry</td>
<td>Graph distance (e.g. nearest-neighbor graph)</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#hierarchical-clustering"><span class="std std-ref">Ward hierarchical clustering</span></a></td>
<td>number of clusters</td>
<td>Large <code class="docutils literal notranslate"><span class="pre">n_samples</span></code> and <code class="docutils literal notranslate"><span class="pre">n_clusters</span></code></td>
<td>Many clusters, possibly connectivity constraints</td>
<td>Distances between points</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#hierarchical-clustering"><span class="std std-ref">Agglomerative clustering</span></a></td>
<td>number of clusters, linkage type, distance</td>
<td>Large <code class="docutils literal notranslate"><span class="pre">n_samples</span></code> and <code class="docutils literal notranslate"><span class="pre">n_clusters</span></code></td>
<td>Many clusters, possibly connectivity constraints, non Euclidean
distances</td>
<td>Any pairwise distance</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#dbscan"><span class="std std-ref">DBSCAN</span></a></td>
<td>neighborhood size</td>
<td>Very large <code class="docutils literal notranslate"><span class="pre">n_samples</span></code>, medium <code class="docutils literal notranslate"><span class="pre">n_clusters</span></code></td>
<td>Non-flat geometry, uneven cluster sizes</td>
<td>Distances between nearest points</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="mixture.html#mixture"><span class="std std-ref">Gaussian mixtures</span></a></td>
<td>many</td>
<td>Not scalable</td>
<td>Flat geometry, good for density estimation</td>
<td>Mahalanobis distances to  centers</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#birch"><span class="std std-ref">Birch</span></a></td>
<td>branching factor, threshold, optional global clusterer.</td>
<td>Large <code class="docutils literal notranslate"><span class="pre">n_clusters</span></code> and <code class="docutils literal notranslate"><span class="pre">n_samples</span></code></td>
<td>Large dataset, outlier removal, data reduction.</td>
<td>Euclidean distance between points</td>
</tr>
</tbody>
</table>
<p>非平面几何聚类(Non-flat geometry clustering)在集群具有特定形状时非常有用，
i.e. 一个非平坦的流形，而标准的欧氏距离不是正确的度量.这种情况出现在上图中最上面两行中。</p>
<p>用于聚类的高斯混合模型在 :ref:<a href="#id3"><span class="problematic" id="id4">`</span></a>专门讨论混合模型的文档 &lt;mixture&gt;`中进行了描述。
KMeans可以看做是高斯混合模型的特例，其中每个分量的协方差相等。</p>
</div>
<div class="section" id="k">
<span id="k-means"></span><h2>2.3.2. K-均值<a class="headerlink" href="#k" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><code class="xref py py-class docutils literal notranslate"><span class="pre">KMeans</span></code></a> 算法通过尝试将样本分离成n个方差相等的组来对数据进行聚类，最小化了一个称为惯性(<a class="reference external" href="inertia">inertia</a>)或
聚类内和平方(within-cluster sum-of-squares)的准则。
该算法需要指定簇数。它可以很好地扩展到大量的样本，并且已经在许多不同的应用领域得到了广泛的应用。</p>
<p>k-均值算法将 <span class="math notranslate nohighlight">\(N\)</span> 个样本的集合 <span class="math notranslate nohighlight">\(X\)</span> 划分为 <span class="math notranslate nohighlight">\(K\)</span> 个不相交的簇 <span class="math notranslate nohighlight">\(C\)</span> ，
每个簇由聚类中样本的均值 <span class="math notranslate nohighlight">\(\mu_j\)</span> 描述。
这些均值通常被称为簇的“质心(centroids)”；请注意，它们通常不是 <span class="math notranslate nohighlight">\(X\)</span> 内的样本点，尽管它们处在同一个空间。
K-均值算法的目标是选择那些可以最小化惯性(<em>inertia</em>)或最小化簇内平方和准则(within-cluster sum of squared criterion) 的质心 :</p>
<div class="math notranslate nohighlight">
\[\sum_{i=0}^{n}\min_{\mu_j \in C}(||x_i - \mu_j||^2)\]</div>
<p>惯性(<em>inertia</em>), 或 簇内平方和准则(within-cluster sum of squared criterion) ,
可以被认为是一种对簇内 内相干(internally coherent) 程度的度量。
它有以下缺点:</p>
<ul class="simple">
<li>惯性(<em>inertia</em>)假设团簇是凸的和各向同性的，但情况并不总是如此。它在细长的团簇或不规则形状的流形上效果很差。</li>
<li>惯性(<em>inertia</em>)不是一个规范化的度量：我们只知道较低的值更好，而零是最优的。但是在高维空间中，欧几里得距离往往会膨胀。
(这是 维数灾难(“curse of dimensionality”)的一个实例)。
在k-均值聚类前运行 <a class="reference external" href="PCA">PCA</a> 等降维算法可以缓解这一问题，加快计算速度。</li>
</ul>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_kmeans_assumptions.html"><img alt="../images/sphx_glr_plot_kmeans_assumptions_0011.png" class="align-center" src="../images/sphx_glr_plot_kmeans_assumptions_0011.png" style="width: 600.0px; height: 600.0px;" /></a>
<p>K-均值常被称为劳埃德算法(Lloyd’s algorithm)。基本上讲，该算法分为三个步骤。第一步选择初始质心，
最基本的方法是从数据集 <span class="math notranslate nohighlight">\(k\)</span> 中选择k个样本。完成了第一步初始化后，K-means将会在接下来的两步中循环执行：
第二步将每个样本分配到最近的质心。
第三步通过获取分配给每个先前质心的所有样本的平均值来创建新的质心。计算旧质心和新质心之间的差值。
算法重复第二、第三这两个步骤，直到新旧质心之间的差值小于一个阈值为止。
换句话说，它会重复，直到质心没有明显的移动。</p>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_kmeans_digits.html"><img alt="../images/sphx_glr_plot_kmeans_digits_0011.png" class="align-right" src="../images/sphx_glr_plot_kmeans_digits_0011.png" style="width: 224.0px; height: 168.0px;" /></a>
<p>K-均值等价于具有小的、完全相等的对角协方差矩阵的期望最大化算法。</p>
<p>K-均值算法还可以通过这个 <a class="reference external" href="https://en.wikipedia.org/wiki/Voronoi_diagram">Voronoi diagrams</a> 概念进行理解。
首先使用当前质心计算点的 Voronoi 图。Voronoi 图中的每个段(segment)都成为一个单独分开的簇(separate cluster)。
其次，质心被更新为每个段(segment)的平均值。然后，该算法重复此操作，直到满足停止条件。
通常情况下，当每两次迭代之间的目标函数的相对减小小于给定的容忍值(tolerance value)时，算法停止。
我们的实现与上述过程是有点儿区别的: 当质心移动小于容忍值(tolerance)时，迭代停止。</p>
<p>给定足够的时间，K-means 总是能够收敛的，但这可能是局部最小的。
这很大程度上取决于质心的初始化。因此，通常会进行几次不同的质心初始化的计算。
帮助解决这个问题的一种方法是 k-means++ 初始化方案，它已经在 scikit-learn 中实现（使用 <code class="docutils literal notranslate"><span class="pre">init='k-means++'</span></code> 参数）。
这种方法通常将初始化质心彼此远离，导致比随机初始化更好的结果，如参考文献所示。</p>
<p>The algorithm supports sample weights, which can be given by a parameter
<code class="docutils literal notranslate"><span class="pre">sample_weight</span></code>. This allows to assign more weight to some samples when
computing cluster centers and values of inertia. For example, assigning a
weight of 2 to a sample is equivalent to adding a duplicate of that sample
to the dataset <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>为了允许 K-means 并行运行，给出一个参数称为 <code class="docutils literal notranslate"><span class="pre">n_jobs</span></code> 。给这个参数一个正值可以使用多个处理器（默认值: 1）。
如果给 -1 则使用所有可用的处理器，-2 使用少一个，等等。
并行化通常以增加内存的代价加速计算（在这种情况下，需要存储多个质心副本，每个job使用一个）。</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">当 <cite>numpy</cite> 使用 <cite>Accelerate</cite> 框架时，K-Means 的并行版本在 OS X 上会崩溃。
这是预期的行为(expected behavior): <cite>Accelerate</cite> 可以在 fork 之后调用，
但是您需要使用 Python binary（该多进程在 posix 下不执行）来执行子进程。</p>
</div>
<p>K-means 可用于矢量量化(vector quantization)。这是使用训练好的 <a class="reference internal" href="generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><code class="xref py py-class docutils literal notranslate"><span class="pre">KMeans</span></code></a> 类的 <code class="docutils literal notranslate"><span class="pre">transform</span></code> 变换方法获得的结果。</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py"><span class="std std-ref">Demonstration of k-means assumptions</span></a>: Demonstrating when
k-means performs intuitively and when it does not</li>
<li><a class="reference internal" href="../auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py"><span class="std std-ref">A demo of K-Means clustering on the handwritten digits data</span></a>: 聚类手写数字</li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">“k-means++: The advantages of careful seeding”</a>
Arthur, David, and Sergei Vassilvitskii,
<em>Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete
algorithms</em>, Society for Industrial and Applied Mathematics (2007)</li>
</ul>
</div>
<div class="section" id="mini-batch-kmeans">
<span id="id5"></span><h3>2.3.2.1. 小批量 K-均值<a class="headerlink" href="#mini-batch-kmeans" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans" title="sklearn.cluster.MiniBatchKMeans"><code class="xref py py-class docutils literal notranslate"><span class="pre">MiniBatchKMeans</span></code></a> 是 <a class="reference internal" href="generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><code class="xref py py-class docutils literal notranslate"><span class="pre">KMeans</span></code></a> 算法的一个变体，它使用 mini-batches 来减少计算时间，同时仍然尝试优化相同的目标函数(objective function)。
小批量样本(Mini-batches)是输入数据的子集，在每个训练迭代中进行随机抽样。
这些mini-batches大大减少了收敛到局部解所需的计算量。 与其他降低 k-means 收敛时间的算法作对比，mini-batch k-means 产生的结果通常只比标准算法略差。</p>
<p>该算法在两个主要步骤之间进行迭代，类似于 vanilla k-means 。 在第一步，从数据集中随机抽取 <span class="math notranslate nohighlight">\(b\)</span> 个样本形成一个 mini-batch。
然后将它们分配到最近的质心。 在第二步，更新质心。与 k-means 相反，这是在每个样本的基础上完成的。
对 mini-batch 中的每个样本，通过取该样本的流平均值(streaming average)和分配给该质心的所有先前样本来更新分配给该样本的质心。
这具有随时间降低质心变化率的效果。执行这些步骤直到达到收敛或达到预定次数的迭代。</p>
<p><a class="reference internal" href="generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans" title="sklearn.cluster.MiniBatchKMeans"><code class="xref py py-class docutils literal notranslate"><span class="pre">MiniBatchKMeans</span></code></a> 收敛速度比 <a class="reference internal" href="generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><code class="xref py py-class docutils literal notranslate"><span class="pre">KMeans</span></code></a> 更快，但是结果的质量会降低。在实践中，质量差异可能相当小，如下面给的案例和引用的参考。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/cluster/plot_mini_batch_kmeans.html"><img alt="../images/sphx_glr_plot_mini_batch_kmeans_0011.png" src="../images/sphx_glr_plot_mini_batch_kmeans_0011.png" style="width: 800.0px; height: 300.0px;" /></a>
</div>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_mini_batch_kmeans.html#sphx-glr-auto-examples-cluster-plot-mini-batch-kmeans-py"><span class="std std-ref">Comparison of the K-Means and MiniBatchKMeans clustering algorithms</span></a>:  KMeans 与 MiniBatchKMeans 的对比</li>
<li><a class="reference internal" href="../auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py"><span class="std std-ref">Clustering text documents using k-means</span></a>: 使用 sparse MiniBatchKMeans 进行文档聚类</li>
<li><a class="reference internal" href="../auto_examples/cluster/plot_dict_face_patches.html#sphx-glr-auto-examples-cluster-plot-dict-face-patches-py"><span class="std std-ref">Online learning of a dictionary of parts of faces</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf">“Web Scale K-Means clustering”</a>
D. Sculley, <em>Proceedings of the 19th international conference on World
wide web</em> (2010)</li>
</ul>
</div>
</div>
</div>
<div class="section" id="affinity-propagation">
<span id="id6"></span><h2>2.3.3. 吸引子传播<a class="headerlink" href="#affinity-propagation" title="Permalink to this headline">¶</a></h2>
<p>(译者注：Affinity Propagation Clustering 可翻译为：仿射传播聚类，吸引子传播聚类，相似性传播聚类，亲和力传播聚类，以下简称 AP聚类)</p>
<p><a class="reference internal" href="generated/sklearn.cluster.AffinityPropagation.html#sklearn.cluster.AffinityPropagation" title="sklearn.cluster.AffinityPropagation"><code class="xref py py-class docutils literal notranslate"><span class="pre">AffinityPropagation</span></code></a> 聚类方法是通过在样本对之间发送消息直到收敛来创建聚类。
然后使用少量示例样本作为聚类中心来描述数据集， 聚类中心是数据集中最能代表一类数据的样本。
在样本对之间发送的消息表示一个样本作为另一个样本的示例样本的 适合程度(suitability)，适合程度值在根据通信的反馈不断更新。
更新迭代直到收敛，完成聚类中心的选取，因此也给出了最终聚类。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/cluster/plot_affinity_propagation.html"><img alt="../images/sphx_glr_plot_affinity_propagation_0011.png" src="../images/sphx_glr_plot_affinity_propagation_0011.png" style="width: 320.0px; height: 240.0px;" /></a>
</div>
<p>Affinity Propagation 算法比较有趣的是可以根据提供的数据决定聚类的数目。 因此有两个比较重要的参数:
<em>preference</em>: 决定使用多少个示例样本; <em>damping factor</em>: 阻尼因子,用于减少吸引信息和归属信息以防止更新减少吸引度和归属度信息时数据振荡。</p>
<p>AP聚类算法主要的缺点是算法的复杂度。 AP聚类算法的时间复杂度是 <span class="math notranslate nohighlight">\(O(N^2 T)\)</span> , 其中 <span class="math notranslate nohighlight">\(N\)</span> 是样本的个数 ，
<span class="math notranslate nohighlight">\(T\)</span> 是收敛之前迭代的次数。如果使用密集的相似性矩阵空间复杂度是 <span class="math notranslate nohighlight">\(O(N^2)\)</span> ，如果使用稀疏的相似性矩阵空间复杂度可以降低。
这使得AP聚类最适合中小型数据集(small to medium sized datasets)。</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_affinity_propagation.html#sphx-glr-auto-examples-cluster-plot-affinity-propagation-py"><span class="std std-ref">Demo of affinity propagation clustering algorithm</span></a>: Affinity
Propagation on a synthetic 2D datasets with 3 classes.</li>
<li><a class="reference internal" href="../auto_examples/applications/plot_stock_market.html#sphx-glr-auto-examples-applications-plot-stock-market-py"><span class="std std-ref">Visualizing the stock market structure</span></a> Affinity Propagation on
Financial time series to find groups of companies</li>
</ul>
</div>
<p><strong>算法描述:</strong>
样本之间传递的信息有两种。 第一种是 responsibility <span class="math notranslate nohighlight">\(r(i, k)\)</span>, 样本 <span class="math notranslate nohighlight">\(k\)</span> 适合作为样本 <span class="math notranslate nohighlight">\(i\)</span> 的典型代表(exemplar)的累计证据。
第二种是 availability <span class="math notranslate nohighlight">\(a(i, k)\)</span>, 样本 <span class="math notranslate nohighlight">\(i\)</span> 应该选择样本 <span class="math notranslate nohighlight">\(k\)</span> 作为它的典型代表(exemplar)的累计证据，
并考虑对所有其他样本来说 <span class="math notranslate nohighlight">\(k\)</span> 作为exemplar的累计证据。
用这种方式, exemplars被选择是因为这些exemplars满足了两个条件: (1)它们与很多样本足够相似，(2)它们被很多样本选择作为自己的代表。</p>
<p>更正式一点, 一个样本 <span class="math notranslate nohighlight">\(k\)</span> 要成为样本 <span class="math notranslate nohighlight">\(i\)</span> 的exemplar的 responsibility 由下式给出:</p>
<div class="math notranslate nohighlight">
\[r(i, k) \leftarrow s(i, k) - max [ a(i, k') + s(i, k') \forall k' \neq k ]\]</div>
<p>其中 <span class="math notranslate nohighlight">\(s(i, k)\)</span> 是样本 <span class="math notranslate nohighlight">\(k\)</span> 和样本 <span class="math notranslate nohighlight">\(i\)</span> 之间的相似度。</p>
<p>样本 <span class="math notranslate nohighlight">\(k\)</span> 要成为样本 <span class="math notranslate nohighlight">\(i\)</span> 的exemplar的 availability 由下式给出:</p>
<div class="math notranslate nohighlight">
\[a(i, k) \leftarrow min [0, r(k, k) + \sum_{i'~s.t.~i' \notin \{i, k\}}{r(i', k)}]\]</div>
<p>在开始的时候, <span class="math notranslate nohighlight">\(r\)</span> 和 <span class="math notranslate nohighlight">\(a\)</span> 中的所有值被设为 0, 并且迭代计算到收敛为止。
为了防止更新messages时出现数据振荡，在迭代过程中引入阻尼因子 <span class="math notranslate nohighlight">\(\lambda\)</span> :</p>
<div class="math notranslate nohighlight">
\[r_{t+1}(i, k) = \lambda\cdot r_{t}(i, k) + (1-\lambda)\cdot r_{t+1}(i, k)\]</div>
<div class="math notranslate nohighlight">
\[a_{t+1}(i, k) = \lambda\cdot a_{t}(i, k) + (1-\lambda)\cdot a_{t+1}(i, k)\]</div>
<p>其中 <span class="math notranslate nohighlight">\(t\)</span> 是迭代次数。</p>
</div>
<div class="section" id="mean-shift">
<span id="id7"></span><h2>2.3.4. 均值漂移<a class="headerlink" href="#mean-shift" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.cluster.MeanShift.html#sklearn.cluster.MeanShift" title="sklearn.cluster.MeanShift"><code class="xref py py-class docutils literal notranslate"><span class="pre">MeanShift</span></code></a> 聚类算法旨在于发现一个样本密度平滑的 <em>blobs</em> 。 均值漂移算法是一种基于质心的算法，
其工作原理是更新质心的候选点，使其成为给定区域内点的均值。
然后，这些候选者在后处理阶段被过滤以消除近似重复(near-duplicates)，从而形成最终质心集合。</p>
<p>给定第 <span class="math notranslate nohighlight">\(t\)</span> 次迭代中的候选质心 <span class="math notranslate nohighlight">\(x_i\)</span> , 候选质心的位置将按照如下公式更新:</p>
<div class="math notranslate nohighlight">
\[x_i^{t+1} = m(x_i^t)\]</div>
<p>其中 <span class="math notranslate nohighlight">\(N(x_i)\)</span> 是围绕 <span class="math notranslate nohighlight">\(x_i\)</span> 的给定距离内的样本邻域， <span class="math notranslate nohighlight">\(m\)</span> 是为指向点密度最大增加的区域的每一个质心计算的均值漂移向量
(<em>mean shift</em> vector)。
该均值漂移向量由下面的公式计算, 通过漂移向量可以高效的把一个质心更新到它的邻域内样本均值所在的地方：</p>
<div class="math notranslate nohighlight">
\[m(x_i) = \frac{\sum_{x_j \in N(x_i)}K(x_j - x_i)x_j}{\sum_{x_j \in N(x_i)}K(x_j - x_i)}\]</div>
<p>该算法自动设置聚类的数量, 而不是依赖参数 <code class="docutils literal notranslate"><span class="pre">bandwidth</span></code> 来决定要搜索的区域的大小。
此参数可以手动设置，但可以使用提供的 <code class="docutils literal notranslate"><span class="pre">estimate_bandwidth</span></code> 函数进行估计，如果没有设置带宽，则调用该函数。</p>
<p>该算法不具有很高的可扩展性(not highly scalable)，因为在算法执行过程中需要多个最近邻搜索。
该算法保证收敛，但当质心变化较小时，该算法将停止迭代。</p>
<p>标记一个新的样本是通过为给定的样本找到最近的质心来完成的。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/cluster/plot_mean_shift.html"><img alt="../images/sphx_glr_plot_mean_shift_0011.png" src="../images/sphx_glr_plot_mean_shift_0011.png" style="width: 320.0px; height: 240.0px;" /></a>
</div>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_mean_shift.html#sphx-glr-auto-examples-cluster-plot-mean-shift-py"><span class="std std-ref">A demo of the mean-shift clustering algorithm</span></a>: 带有3个类的合成2D数据集上的均值漂移聚类。</li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.8968&amp;rep=rep1&amp;type=pdf">“Mean shift: A robust approach toward feature space analysis.”</a>
D. Comaniciu and P. Meer, <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (2002)</li>
</ul>
</div>
</div>
<div class="section" id="spectral-clustering">
<span id="id8"></span><h2>2.3.5. 谱聚类<a class="headerlink" href="#spectral-clustering" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering" title="sklearn.cluster.SpectralClustering"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpectralClustering</span></code></a> 是在样本之间进行affinity matrix的低维度嵌入，后面紧跟一个在低维空间中运行的 KMeans。
如果affinity matrix是稀疏的并且 <a class="reference external" href="https://github.com/pyamg/pyamg">pyamg</a>  模块已经安装好，则这是非常有效的。
SpectralClustering 需要指定聚类数。这个算法适用于聚类数少时，在聚类数多是不建议使用。</p>
<p>对于两个聚类，它解决了相似图上正规化割集(<a class="reference external" href="http://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf">normalised cuts</a>)问题的一个凸松弛问题：
将图形切割成两个，使得切割的边缘的权重比每个簇内的边缘的权重小。
在图像处理时，这个criteria是特别有趣的: 图的顶点是像素，相似图的边缘是图像梯度的函数。</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/cluster/plot_segmentation_toy.html"><img alt="noisy_img" src="../images/sphx_glr_plot_segmentation_toy_0011.png" style="width: 240.0px; height: 240.0px;" /></a> <a class="reference external" href="../auto_examples/cluster/plot_segmentation_toy.html"><img alt="segmented_img" src="../images/sphx_glr_plot_segmentation_toy_0021.png" style="width: 240.0px; height: 240.0px;" /></a></strong></p><div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p>Transforming distance to well-behaved similarities</p>
<p>请注意，如果你的相似矩阵的值分布不均匀，例如:存在负值或者像是一个距离矩阵而不是相似性矩阵，
那么 spectral problem 将会变得奇异，并且不能解决。 在这种情况下，建议对矩阵的 entries 进行变换。
比如在有符号的距离矩阵情况下 通常使用 heat kernel:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">similarity</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">beta</span> <span class="o">*</span> <span class="n">distance</span> <span class="o">/</span> <span class="n">distance</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
</pre></div>
</div>
<p class="last">请看关于这个应用的例子。</p>
</div>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_segmentation_toy.html#sphx-glr-auto-examples-cluster-plot-segmentation-toy-py"><span class="std std-ref">Spectral clustering for image segmentation</span></a>: 利用谱聚类从含噪背景中分割目标。</li>
<li><a class="reference internal" href="../auto_examples/cluster/plot_coin_segmentation.html#sphx-glr-auto-examples-cluster-plot-coin-segmentation-py"><span class="std std-ref">Segmenting the picture of greek coins in regions</span></a>: 谱聚类分割在区域中的硬币图像。</li>
</ul>
</div>
<div class="section" id="id9">
<h3>2.3.5.1. 不同的标签分配策略<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p>可以使用不同的标签分配策略, 对应于 <a class="reference internal" href="generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering" title="sklearn.cluster.SpectralClustering"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpectralClustering</span></code></a> 类的 <code class="docutils literal notranslate"><span class="pre">assign_labels</span></code> 参数。
<code class="docutils literal notranslate"><span class="pre">&quot;kmeans&quot;</span></code> 可以匹配更精细的数据细节，但是可能更加不稳定。 特别是，除非你控置 <code class="docutils literal notranslate"><span class="pre">random_state</span></code>
否则可能无法复现运行的结果 ，因为它取决于随机初始化。另一方面， 使用 <code class="docutils literal notranslate"><span class="pre">&quot;discretize&quot;</span></code> 策略是 100% 可以复现的，
但是它往往会产生相当均匀的几何形状的边缘。</p>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><code class="docutils literal notranslate"><span class="pre">assign_labels=&quot;kmeans&quot;</span></code></th>
<th class="head"><code class="docutils literal notranslate"><span class="pre">assign_labels=&quot;discretize&quot;</span></code></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><a class="reference external" href="../auto_examples/cluster/plot_coin_segmentation.html"><img alt="coin_kmeans" src="../images/sphx_glr_plot_coin_segmentation_0011.png" style="width: 325.0px; height: 325.0px;" /></a></td>
<td><a class="reference external" href="../auto_examples/cluster/plot_coin_segmentation.html"><img alt="coin_discretize" src="../images/sphx_glr_plot_coin_segmentation_0021.png" style="width: 325.0px; height: 325.0px;" /></a></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="id10">
<h3>2.3.5.2. 谱聚类用于图聚类问题<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>谱聚类还可以通过谱嵌入对图进行聚类。在这种情况下，affinity matrix 是图的邻接矩阵，SpectralClustering
由 <cite>affinity=’precomputed’</cite> 进行初始化</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="k">import</span> <span class="n">SpectralClustering</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sc</span> <span class="o">=</span> <span class="n">SpectralClustering</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">affinity</span><span class="o">=</span><span class="s1">&#39;precomputed&#39;</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<span class="gp">... </span>                        <span class="n">assign_labels</span><span class="o">=</span><span class="s1">&#39;discretize&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sc</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">adjacency_matrix</span><span class="p">)</span>  
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323">“A Tutorial on Spectral Clustering”</a>
Ulrike von Luxburg, 2007</li>
<li><a class="reference external" href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324">“Normalized cuts and image segmentation”</a>
Jianbo Shi, Jitendra Malik, 2000</li>
<li><a class="reference external" href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.1501">“A Random Walks View of Spectral Segmentation”</a>
Marina Meila, Jianbo Shi, 2001</li>
<li><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.8100">“On Spectral Clustering: Analysis and an algorithm”</a>
Andrew Y. Ng, Michael I. Jordan, Yair Weiss, 2001</li>
</ul>
</div>
</div>
</div>
<div class="section" id="hierarchical-clustering">
<span id="id11"></span><h2>2.3.6. 层次聚类(Hierarchical clustering)<a class="headerlink" href="#hierarchical-clustering" title="Permalink to this headline">¶</a></h2>
<p>层次聚类是一种通过不断合并或分割嵌套聚类来构建嵌套聚类的通用聚类算法。
聚类的层次结构被表示为一棵树(或树状图)。树的根是收集所有样本的唯一簇，叶是只有一个样本的簇。
请看 <a class="reference external" href="https://en.wikipedia.org/wiki/Hierarchical_clustering">维基百科的相关词条</a> 获得更多信息。</p>
<p>聚合聚类(<a class="reference internal" href="generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering" title="sklearn.cluster.AgglomerativeClustering"><code class="xref py py-class docutils literal notranslate"><span class="pre">AgglomerativeClustering</span></code></a>)对象使用自下而上的方法执行分层聚类：每个observation从自己的簇开始，
簇依次合并在一起。链接准则(linkage criteria)确定用于合并策略的度量:</p>
<ul class="simple">
<li><strong>Ward</strong> 最小化所有簇内的平方差总和。这是一种 方差最小化(variance-minimizing) 的方法，
在这点上，这是与k-means 的目标函数相似的，但是它用了聚合分层(agglomerative hierarchical)的方法处理。</li>
<li><strong>Maximum</strong> or <strong>complete linkage</strong> 最小化每两个簇的样本之间的最大距离。</li>
<li><strong>Average linkage</strong> 最小化每两个簇的样本之间的平均距离。</li>
<li><strong>Single linkage</strong> 最小化每两个簇中最近的样本之间的距离。</li>
</ul>
<p><a class="reference internal" href="generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering" title="sklearn.cluster.AgglomerativeClustering"><code class="xref py py-class docutils literal notranslate"><span class="pre">AgglomerativeClustering</span></code></a> 在与连接矩阵(connectivity matrix)联合使用时，也可以扩大到大量的样本，但是 在样本之间没有添加连接约束时，
计算代价很大:每一个步骤都要考虑所有可能的合并。</p>
<div class="topic">
<p class="topic-title first"><a class="reference internal" href="generated/sklearn.cluster.FeatureAgglomeration.html#sklearn.cluster.FeatureAgglomeration" title="sklearn.cluster.FeatureAgglomeration"><code class="xref py py-class docutils literal notranslate"><span class="pre">FeatureAgglomeration</span></code></a></p>
<p><a class="reference internal" href="generated/sklearn.cluster.FeatureAgglomeration.html#sklearn.cluster.FeatureAgglomeration" title="sklearn.cluster.FeatureAgglomeration"><code class="xref py py-class docutils literal notranslate"><span class="pre">FeatureAgglomeration</span></code></a> 类使用 agglomerative clustering 将看上去相似的特征组合在一起，
从而减少特征的数量。这是一个降维工具, 请看 <a class="reference internal" href="unsupervised_reduction.html#data-reduction"><span class="std std-ref">无监督维数约减(Unsupervised dimensionality reduction)</span></a>。</p>
</div>
<div class="section" id="different-linkage-type-ward-complete-average-and-single-linkage">
<h3>2.3.6.1. Different linkage type: Ward, complete, average, and single linkage<a class="headerlink" href="#different-linkage-type-ward-complete-average-and-single-linkage" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering" title="sklearn.cluster.AgglomerativeClustering"><code class="xref py py-class docutils literal notranslate"><span class="pre">AgglomerativeClustering</span></code></a> 支持 Ward, single, average, 和 complete linkage 策略。</p>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_linkage_comparison.html"><img alt="../images/sphx_glr_plot_linkage_comparison_0011.png" src="../images/sphx_glr_plot_linkage_comparison_0011.png" style="width: 589.1px; height: 623.5px;" /></a>
<p>聚合聚类存在 “rich get richer” 现象导致聚类大小不均匀(uneven cluster sizes)。这方面 single linkage 是最坏的策略，Ward 给出了最规则的大小。
然而，在 Ward 中 affinity (or distance used in clustering) 不能被改变，对于 non Euclidean metrics 来说 average linkage 是一个好的选择。
Single linkage,虽然对噪声数据没有鲁棒性，但可以非常有效地进行计算，因此对于提供更大数据集的分层聚类非常有用。
Single linkage 也可以很好地表现在非球形(non-globular)数据上。</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_digits_linkage.html#sphx-glr-auto-examples-cluster-plot-digits-linkage-py"><span class="std std-ref">Various Agglomerative Clustering on a 2D embedding of digits</span></a>: 在一个真实的数据集中探索不同的linkage策略。</li>
</ul>
</div>
</div>
<div class="section" id="id13">
<h3>2.3.6.2. 添加连通性约束<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering" title="sklearn.cluster.AgglomerativeClustering"><code class="xref py py-class docutils literal notranslate"><span class="pre">AgglomerativeClustering</span></code></a> 类中一个有趣的特点是可以使用连接矩阵(connectivity matrix)将连接约束添加到算法中(只有相邻的聚类可以合并到一起)，
连接矩阵为每一个样本给定了相邻的样本。 例如，在下面的瑞典卷卷(swiss-roll) 的例子中，连接约束禁止在不相邻的 swiss roll 上合并，
从而防止形成在 roll 上 重复折叠的聚类。</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/cluster/plot_ward_structured_vs_unstructured.html"><img alt="unstructured" src="../images/sphx_glr_plot_ward_structured_vs_unstructured_0011.png" style="width: 313.6px; height: 235.2px;" /></a> <a class="reference external" href="../auto_examples/cluster/plot_ward_structured_vs_unstructured.html"><img alt="structured" src="../images/sphx_glr_plot_ward_structured_vs_unstructured_0021.png" style="width: 313.6px; height: 235.2px;" /></a></strong></p><p>这些约束对于强加一定的局部结构是很有用的，但是这也使得算法更快，特别是当样本数量巨大时。</p>
<p>连通性的限制是通过连接矩阵(connectivity matrix)来实现的:一个 scipy sparse matrix(仅在一行和一列的交集处具有应该连接在一起的数据集的索引)。
这个矩阵可以通过先验信息构建:例如，你可能通过仅仅将从一个连接指向另一个的链接合并页面来聚类页面。也可以从数据中学习到,
例如使用 <a class="reference internal" href="generated/sklearn.neighbors.kneighbors_graph.html#sklearn.neighbors.kneighbors_graph" title="sklearn.neighbors.kneighbors_graph"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.neighbors.kneighbors_graph</span></code></a> 限制与最近邻的合并，
就像 <a class="reference internal" href="../auto_examples/cluster/plot_agglomerative_clustering.html#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py"><span class="std std-ref">这个例子</span></a>
中那样, 或者使用 <a class="reference internal" href="generated/sklearn.feature_extraction.image.grid_to_graph.html#sklearn.feature_extraction.image.grid_to_graph" title="sklearn.feature_extraction.image.grid_to_graph"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.feature_extraction.image.grid_to_graph</span></code></a> 仅合并图像上相邻的像素点，
就像 <a class="reference internal" href="../auto_examples/cluster/plot_coin_ward_segmentation.html#sphx-glr-auto-examples-cluster-plot-coin-ward-segmentation-py"><span class="std std-ref">这个例子</span></a> 。</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_coin_ward_segmentation.html#sphx-glr-auto-examples-cluster-plot-coin-ward-segmentation-py"><span class="std std-ref">A demo of structured Ward hierarchical clustering on an image of coins</span></a>: 使用 Ward 聚类 分割硬币图像。</li>
<li><a class="reference internal" href="../auto_examples/cluster/plot_ward_structured_vs_unstructured.html#sphx-glr-auto-examples-cluster-plot-ward-structured-vs-unstructured-py"><span class="std std-ref">Hierarchical clustering: structured vs unstructured ward</span></a>: 瑞士卷上的Ward算法示例，结构化方法与非结构化方法的比较。</li>
<li><a class="reference internal" href="../auto_examples/cluster/plot_feature_agglomeration_vs_univariate_selection.html#sphx-glr-auto-examples-cluster-plot-feature-agglomeration-vs-univariate-selection-py"><span class="std std-ref">Feature agglomeration vs. univariate selection</span></a>:
基于Ward层次聚类的特征聚类降维实例。</li>
<li><a class="reference internal" href="../auto_examples/cluster/plot_agglomerative_clustering.html#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py"><span class="std std-ref">Agglomerative clustering with and without structure</span></a></li>
</ul>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p><strong>Connectivity constraints with single, average and complete linkage</strong></p>
<p class="last">连接约束 和 complete or average linkage 可以增强聚合聚类中的 ‘rich getting richer’ 现象。
特别是，当它们使用函数 <a class="reference internal" href="generated/sklearn.neighbors.kneighbors_graph.html#sklearn.neighbors.kneighbors_graph" title="sklearn.neighbors.kneighbors_graph"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.neighbors.kneighbors_graph</span></code></a> 进行构建时。 在少量聚类的限制下，
更倾向于给出一些 macroscopically occupied clusters
并且几乎是空的 (讨论内容请查看 <a class="reference internal" href="../auto_examples/cluster/plot_agglomerative_clustering.html#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-py"><span class="std std-ref">Agglomerative clustering with and without structure</span></a>)。
在这个问题上，Single linkage 是最脆弱的 linkage 选项。</p>
</div>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_agglomerative_clustering.html"><img alt="../images/sphx_glr_plot_agglomerative_clustering_0011.png" src="../images/sphx_glr_plot_agglomerative_clustering_0011.png" style="width: 380.0px; height: 152.0px;" /></a>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_agglomerative_clustering.html"><img alt="../images/sphx_glr_plot_agglomerative_clustering_0021.png" src="../images/sphx_glr_plot_agglomerative_clustering_0021.png" style="width: 380.0px; height: 152.0px;" /></a>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_agglomerative_clustering.html"><img alt="../images/sphx_glr_plot_agglomerative_clustering_0031.png" src="../images/sphx_glr_plot_agglomerative_clustering_0031.png" style="width: 380.0px; height: 152.0px;" /></a>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_agglomerative_clustering.html"><img alt="../images/sphx_glr_plot_agglomerative_clustering_0041.png" src="../images/sphx_glr_plot_agglomerative_clustering_0041.png" style="width: 380.0px; height: 152.0px;" /></a>
</div>
<div class="section" id="id14">
<h3>2.3.6.3. 改变聚类测度<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h3>
<p>Single, average 和 complete linkage 可以使用各种距离 (or affinities), 特别是 欧氏距离(l2),
曼哈顿距离(Manhattan distance)(or 城市区块距离(Cityblock), or l1), 余弦距离(cosine distance),
或者 任何预先计算的亲和度(affinity)矩阵。</p>
<ul class="simple">
<li><em>l1</em> distance 有利于稀疏特征或者稀疏噪声: 例如很多特征都是0，在文本挖掘中统计稀有词汇的出现就会出现这种情况。</li>
<li><em>cosine</em> distance 非常有趣因为它对信号的全局放缩具有不变性。</li>
</ul>
<p>选择度量的准则是使用一个准则，使不同类中的样本之间的距离最大化，并使每个类内的距离最小化。</p>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_agglomerative_clustering_metrics.html"><img alt="../images/sphx_glr_plot_agglomerative_clustering_metrics_0051.png" src="../images/sphx_glr_plot_agglomerative_clustering_metrics_0051.png" style="width: 204.8px; height: 153.6px;" /></a>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_agglomerative_clustering_metrics.html"><img alt="../images/sphx_glr_plot_agglomerative_clustering_metrics_0061.png" src="../images/sphx_glr_plot_agglomerative_clustering_metrics_0061.png" style="width: 204.8px; height: 153.6px;" /></a>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_agglomerative_clustering_metrics.html"><img alt="../images/sphx_glr_plot_agglomerative_clustering_metrics_0071.png" src="../images/sphx_glr_plot_agglomerative_clustering_metrics_0071.png" style="width: 204.8px; height: 153.6px;" /></a>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_agglomerative_clustering_metrics.html#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-metrics-py"><span class="std std-ref">Agglomerative clustering with different metrics</span></a></li>
</ul>
</div>
</div>
</div>
<div class="section" id="dbscan">
<span id="id15"></span><h2>2.3.7. DBSCAN<a class="headerlink" href="#dbscan" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN" title="sklearn.cluster.DBSCAN"><code class="xref py py-class docutils literal notranslate"><span class="pre">DBSCAN</span></code></a> 算法将聚类视为被低密度区域分隔的高密度区域。由于这个相当普遍的观点，
DBSCAN发现的聚类可以是任何形状的，与假设聚类是 convex shaped 的 K-means 相反。
DBSCAN 的核心概念是 <em>core samples</em> , 是指位于高密度区域的样本。 因此一个聚类是一组核心样本，
每个核心样本彼此靠近（通过一定距离度量测量） 和一组接近核心样本的非核心样本（但本身不是核心样本）。
算法中的两个参数, <code class="docutils literal notranslate"><span class="pre">min_samples</span></code>  和 <code class="docutils literal notranslate"><span class="pre">eps</span></code> ,正式的定义了我们所说的 <a href="#id16"><span class="problematic" id="id17">*</span></a>dense*（稠密）。
较高的 <code class="docutils literal notranslate"><span class="pre">min_samples</span></code> 或者较低的 <code class="docutils literal notranslate"><span class="pre">eps</span></code> 表示形成聚类需要较高的密度。</p>
<p>更正式的,我们定义核心样本(core sample)是指数据集中的一个样本，在 <code class="docutils literal notranslate"><span class="pre">eps</span></code> 距离范围内存在 <code class="docutils literal notranslate"><span class="pre">min_samples</span></code> 个其他样本，
这些样本被定义为核心样本的邻居(<em>neighbors</em>) 。 这告诉我们核心样本在向量空间的稠密区域。 一个聚类是一个核心样本的集合，
可以通过递归来构建，选取一个核心样本，寻找它所有的neighbors中的核心样本，然后寻找新获取的核心样本(<em>their</em>)的 neighbors中的核心样本，
递归这个过程。 聚类中还具有一组非核心样本，它们是聚类中核心样本的邻居的样本，但本身并不是核心样本。 显然，这些样本位于聚类的边缘。</p>
<p>根据定义，任何核心样本都是聚类的一部分，任何不是核心样本并且和任意一个核心样本距离都至少大于 <code class="docutils literal notranslate"><span class="pre">eps</span></code> 的样本被认为是 outliers。</p>
<p>在下图中，颜色表示聚类成员，大圆圈表示算法发现的核心样本。 较小的圆圈是仍然是聚类的一部分的非核心样本。 此外，异常值(outliers)由下面的黑点表示。</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/cluster/plot_dbscan.html"><img alt="dbscan_results" src="../images/sphx_glr_plot_dbscan_0011.png" style="width: 320.0px; height: 240.0px;" /></a></strong></p><div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py"><span class="std std-ref">Demo of DBSCAN clustering algorithm</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">实现</p>
<p>DBSCAN 算法是具有确定性的，当以相同的顺序给出相同的数据时总是形成相同的聚类。
然而，当以不同的顺序提供数据时聚类的结果可能不相同。首先，即使核心样本总是被分配给相同的簇，
这些簇的标签将取决于数据中遇到这些样本的顺序。第二个更重要的是，给非核心样本分派的聚簇可能因数据顺序而有所不同。
当一个非核心样本距离两个核心样本的距离都小于  <code class="docutils literal notranslate"><span class="pre">eps</span></code> 时，就会发生这种情况。
通过三角不等式可知，这两个核心样本距离一定大于  <code class="docutils literal notranslate"><span class="pre">eps</span></code> 或者处于同一个聚类中。
非核心样本将被非配到首先查找到改样本的类别，因此结果将取决于数据的顺序。</p>
<p>当前版本使用 ball trees 和 kd-trees 来确定样本点的邻域，这样避免了计算全部的距离矩阵 （0.14 之前的 scikit-learn 版本计算全部的距离矩阵）。
保留使用自定义指标(custom metrics)的可能性。细节请参照 <code class="xref py py-class docutils literal notranslate"><span class="pre">NearestNeighbors</span></code> 。</p>
</div>
<div class="topic">
<p class="topic-title first">大样本量的内存消耗</p>
<p>默认的实现方式并不是内存高效的，因为它在不能使用 kd-trees 或者 ball-trees 的情况下构建一个完整的两两相似度矩阵(pairwise similarity matrix)。
这个矩阵将消耗 n^2 个浮点数。 解决这个问题的几种机制:</p>
<ul class="simple">
<li>稀疏半径邻域图(A sparse radius neighborhood graph)(其中缺少条目被假定为距离超出 eps) 可以以内存高效的方式预先计算，
并且dbscan(参数设置为 <code class="docutils literal notranslate"><span class="pre">metric='precomputed'</span></code>)可以在这个图上运行。
请看 <a class="reference internal" href="generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors.radius_neighbors_graph" title="sklearn.neighbors.NearestNeighbors.radius_neighbors_graph"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sklearn.neighbors.NearestNeighbors.radius_neighbors_graph</span></code></a> 。</li>
<li>通过删除发生在数据中的准确副本或着使用 BIRCH 方法对数据集进行压缩。
然后，你就可以用一个数量相对少的样本集合来代表大量的样本点。再然后，你还可以在拟合DBSCAN的时候提供 <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code>。</li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li>“A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases
with Noise”
Ester, M., H. P. Kriegel, J. Sander, and X. Xu,
In Proceedings of the 2nd International Conference on Knowledge Discovery
and Data Mining, Portland, OR, AAAI Press, pp. 226–231. 1996</li>
</ul>
</div>
</div>
<div class="section" id="birch">
<span id="id18"></span><h2>2.3.8. Birch<a class="headerlink" href="#birch" title="Permalink to this headline">¶</a></h2>
<p>The <a class="reference internal" href="generated/sklearn.cluster.Birch.html#sklearn.cluster.Birch" title="sklearn.cluster.Birch"><code class="xref py py-class docutils literal notranslate"><span class="pre">Birch</span></code></a> builds a tree called the Characteristic Feature Tree (CFT)
for the given data. The data is essentially lossy compressed to a set of
Characteristic Feature nodes (CF Nodes). The CF Nodes have a number of
subclusters called Characteristic Feature subclusters (CF Subclusters)
and these CF Subclusters located in the non-terminal CF Nodes
can have CF Nodes as children.</p>
<p>The CF Subclusters hold the necessary information for clustering which prevents
the need to hold the entire input data in memory. This information includes:</p>
<ul class="simple">
<li>Number of samples in a subcluster.</li>
<li>Linear Sum - A n-dimensional vector holding the sum of all samples</li>
<li>Squared Sum - Sum of the squared L2 norm of all samples.</li>
<li>Centroids - To avoid recalculation linear sum / n_samples.</li>
<li>Squared norm of the centroids.</li>
</ul>
<p>The Birch algorithm has two parameters, the threshold and the branching factor.
The branching factor limits the number of subclusters in a node and the
threshold limits the distance between the entering sample and the existing
subclusters.</p>
<p>This algorithm can be viewed as an instance or data reduction method,
since it reduces the input data to a set of subclusters which are obtained directly
from the leaves of the CFT. This reduced data can be further processed by feeding
it into a global clusterer. This global clusterer can be set by <code class="docutils literal notranslate"><span class="pre">n_clusters</span></code>.
If <code class="docutils literal notranslate"><span class="pre">n_clusters</span></code> is set to None, the subclusters from the leaves are directly
read off, otherwise a global clustering step labels these subclusters into global
clusters (labels) and the samples are mapped to the global label of the nearest subcluster.</p>
<p><strong>Algorithm description:</strong></p>
<ul class="simple">
<li>A new sample is inserted into the root of the CF Tree which is a CF Node.
It is then merged with the subcluster of the root, that has the smallest
radius after merging, constrained by the threshold and branching factor conditions.
If the subcluster has any child node, then this is done repeatedly till it reaches
a leaf. After finding the nearest subcluster in the leaf, the properties of this
subcluster and the parent subclusters are recursively updated.</li>
<li>If the radius of the subcluster obtained by merging the new sample and the
nearest subcluster is greater than the square of the threshold and if the
number of subclusters is greater than the branching factor, then a space is temporarily
allocated to this new sample. The two farthest subclusters are taken and
the subclusters are divided into two groups on the basis of the distance
between these subclusters.</li>
<li>If this split node has a parent subcluster and there is room
for a new subcluster, then the parent is split into two. If there is no room,
then this node is again split into two and the process is continued
recursively, till it reaches the root.</li>
</ul>
<p><strong>Birch or MiniBatchKMeans?</strong></p>
<blockquote>
<div><ul class="simple">
<li>Birch does not scale very well to high dimensional data. As a rule of thumb if
<code class="docutils literal notranslate"><span class="pre">n_features</span></code> is greater than twenty, it is generally better to use MiniBatchKMeans.</li>
<li>If the number of instances of data needs to be reduced, or if one wants a
large number of subclusters either as a preprocessing step or otherwise,
Birch is more useful than MiniBatchKMeans.</li>
</ul>
</div></blockquote>
<p><strong>如何使用partial_fit?</strong></p>
<p>To avoid the computation of global clustering, for every call of <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code>
the user is advised</p>
<blockquote>
<div><ol class="arabic simple">
<li>To set <code class="docutils literal notranslate"><span class="pre">n_clusters=None</span></code> initially</li>
<li>Train all data by multiple calls to partial_fit.</li>
<li>Set <code class="docutils literal notranslate"><span class="pre">n_clusters</span></code> to a required value using
<code class="docutils literal notranslate"><span class="pre">brc.set_params(n_clusters=n_clusters)</span></code>.</li>
<li>Call <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code> finally with no arguments, i.e. <code class="docutils literal notranslate"><span class="pre">brc.partial_fit()</span></code>
which performs the global clustering.</li>
</ol>
</div></blockquote>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_birch_vs_minibatchkmeans.html"><img alt="../images/sphx_glr_plot_birch_vs_minibatchkmeans_0011.png" src="../images/sphx_glr_plot_birch_vs_minibatchkmeans_0011.png" /></a>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li>Tian Zhang, Raghu Ramakrishnan, Maron Livny
BIRCH: An efficient data clustering method for large databases.
<a class="reference external" href="http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf">http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf</a></li>
<li>Roberto Perdisci
JBirch - Java implementation of BIRCH clustering algorithm
<a class="reference external" href="https://code.google.com/archive/p/jbirch">https://code.google.com/archive/p/jbirch</a></li>
</ul>
</div>
</div>
<div class="section" id="clustering-evaluation">
<span id="id19"></span><h2>2.3.9. 聚类算法性能评估<a class="headerlink" href="#clustering-evaluation" title="Permalink to this headline">¶</a></h2>
<p>评估聚类算法的性能并不像计算错误数或监督分类算法的精确度和召回率那样简单。
特别是，任何评估指标都不应该考虑聚类标签的绝对值。
In particular any evaluation metric should not
take the absolute values of the cluster labels into account but rather
if this clustering define separations of the data similar to some ground
truth set of classes or satisfying some assumption such that members
belong to the same class are more similar that members of different
classes according to some similarity metric.
(译者注：这句英文简直逆天了，看了五分钟都没看出咋断句，不译了,o(∩_∩)o 哈哈)。</p>
<div class="section" id="adjusted-rand-index">
<span id="adjusted-rand-score"></span><h3>2.3.9.1. Adjusted Rand index<a class="headerlink" href="#adjusted-rand-index" title="Permalink to this headline">¶</a></h3>
<p>给定真实的类分配(ground truth class assignments): <code class="docutils literal notranslate"><span class="pre">labels_true</span></code> 和 我们的聚类算法对同样的样本集预测出的类分配：<code class="docutils literal notranslate"><span class="pre">labels_pred</span></code>,
<strong>adjusted Rand index</strong> 是一个用来度量上述两种分配的相似度(<strong>similarity</strong>)的函数，ignoring permutations 和 <strong>with chance normalization</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">metrics</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">adjusted_rand_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>  
<span class="go">0.24...</span>
</pre></div>
</div>
<p>可以在预测的标签中排列(permute) 0 和 1，重命名为 2 到 3， 得到相同的分数</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">labels_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">adjusted_rand_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>  
<span class="go">0.24...</span>
</pre></div>
</div>
<p>更进一步, 函数 <a class="reference internal" href="generated/sklearn.metrics.adjusted_rand_score.html#sklearn.metrics.adjusted_rand_score" title="sklearn.metrics.adjusted_rand_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">adjusted_rand_score</span></code></a> 是 <strong>对称的(symmetric)</strong>: 交换参数(argument)不会改变得分(score)。
它可以作为 <strong>共识度量(consensus measure)</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">adjusted_rand_score</span><span class="p">(</span><span class="n">labels_pred</span><span class="p">,</span> <span class="n">labels_true</span><span class="p">)</span>  
<span class="go">0.24...</span>
</pre></div>
</div>
<p>完美的标记(perfect labeling)得分为 1.0</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">labels_pred</span> <span class="o">=</span> <span class="n">labels_true</span><span class="p">[:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">adjusted_rand_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>
<span class="go">1.0</span>
</pre></div>
</div>
<p>坏的标记 (e.g. independent labelings) 具有负值或接近 0.0 的得分:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">labels_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">adjusted_rand_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>  
<span class="go">-0.12...</span>
</pre></div>
</div>
<div class="section" id="id20">
<h4>2.3.9.1.1. 优点<a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><strong>随机(均匀)标签分配的 ARI 得分接近于 0.0</strong>
对于 <code class="docutils literal notranslate"><span class="pre">n_clusters</span></code> 和 <code class="docutils literal notranslate"><span class="pre">n_samples</span></code> 的任何值（这不是 raw Rand index 或者 V-measure 的情况）。</li>
<li><strong>得分被界定在 [-1, 1] 的区间内</strong>: 负值是坏的(独立性标签), 相似的聚类有一个正的 ARI， 1.0 是完美的匹配得分。</li>
<li><strong>没有对簇的结构做任何假定</strong>:  可以用于比较聚类算法，比如 假定了各向同性的blob shapes的k-means方法的结果 和 寻找具有
“folded”形状的谱聚类方法的结果进行比较。</li>
</ul>
</div>
<div class="section" id="id21">
<h4>2.3.9.1.2. 缺点<a class="headerlink" href="#id21" title="Permalink to this headline">¶</a></h4>
<ul>
<li><p class="first">与惯性(inertia)方法不同, <strong>ARI 需要 ground truth classes 的相关知识</strong>
而在实践中几乎不可得到，或者需要人工标注者手动分配（如在监督学习环境中）。</p>
<p>然而，ARI 还可以在纯粹无监督的设置中作为可用于 聚类模型选择 的共识索引的构建模块。</p>
</li>
</ul>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_adjusted_for_chance_measures.html#sphx-glr-auto-examples-cluster-plot-adjusted-for-chance-measures-py"><span class="std std-ref">Adjustment for chance in clustering performance evaluation</span></a>: 分析数据集大小对随机分配聚类度量值的影响。</li>
</ul>
</div>
</div>
<div class="section" id="id22">
<h4>2.3.9.1.3. 数学表达形式<a class="headerlink" href="#id22" title="Permalink to this headline">¶</a></h4>
<p>如果 C 是 ground truth class assignment 以及 K 是聚类算法给出的class assignment, 让我们定义 <span class="math notranslate nohighlight">\(a\)</span> 和 <span class="math notranslate nohighlight">\(b\)</span> 如下:</p>
<ul class="simple">
<li><span class="math notranslate nohighlight">\(a\)</span>, 在C中相同集合和在K中相同集合的元素对的数量(the number of pairs of elements that are in the same set
in C and in the same set in K)</li>
<li><span class="math notranslate nohighlight">\(b\)</span>, 在C中不同集合和在K中不同集合的元素对的数量(the number of pairs of elements that are in different sets
in C and in different sets in K)</li>
</ul>
<p>那么，原始的 (未调整的,unadjusted) Rand index 由下式给出:</p>
<div class="math notranslate nohighlight">
\[\text{RI} = \frac{a + b}{C_2^{n_{samples}}}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(C_2^{n_{samples}}\)</span> 是在(未排序的)数据集中所有可能的元素对的总数量。</p>
<p>然而，RI 评分不能保证随机标签分配(random label assignments)将获得接近零的值（特别是如果簇的数量与样本数量有相同的数量级）。</p>
<p>为了抵消这种影响，我们可以通过定义调整后的 Rand index(adjusted Rand index,即ARI) 来
对随机标签分配的 期望 RI <span class="math notranslate nohighlight">\(E[\text{RI}]\)</span> 打折(discount), 如下所示:</p>
<div class="math notranslate nohighlight">
\[\text{ARI} = \frac{\text{RI} - E[\text{RI}]}{\max(\text{RI}) - E[\text{RI}]}\]</div>
<div class="topic">
<p class="topic-title first">参考文献</p>
<ul class="simple">
<li><a class="reference external" href="http://link.springer.com/article/10.1007%2FBF01908075">Comparing Partitions</a>
L. Hubert and P. Arabie, Journal of Classification 1985</li>
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index">Wikipedia entry for the adjusted Rand index</a></li>
</ul>
</div>
</div>
</div>
<div class="section" id="mutual-info-score">
<span id="id23"></span><h3>2.3.9.2. 基于互信息的得分<a class="headerlink" href="#mutual-info-score" title="Permalink to this headline">¶</a></h3>
<p>给定真实的类分配(class assignments): <code class="docutils literal notranslate"><span class="pre">labels_true</span></code> 和 我们的聚类算法对同样的样本集预测出的类分配：<code class="docutils literal notranslate"><span class="pre">labels_pred</span></code>,
<strong>互信息(Mutual Information)</strong> 是一个函数，用于度量两个分配集合的一致性，忽略了排列组合(the <strong>agreement</strong> of the two
assignments, ignoring permutations)。
这种度量方法的两个不同的归一化版本目前可用: <strong>Normalized Mutual Information (NMI)</strong> 和 <strong>Adjusted Mutual Information (AMI)</strong>。
NMI 在文献中可以经常看到, 而 AMI 最近才被提出 and is <strong>normalized against chance</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">metrics</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">adjusted_mutual_info_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>  
<span class="go">0.22504...</span>
</pre></div>
</div>
<p>可以在预测出的标签(predicted labels)中排列 0 和 1, 重命名为 2 到 3， 并得到相同的得分</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">labels_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">adjusted_mutual_info_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>  
<span class="go">0.22504...</span>
</pre></div>
</div>
<p>所有的函数, <a class="reference internal" href="generated/sklearn.metrics.mutual_info_score.html#sklearn.metrics.mutual_info_score" title="sklearn.metrics.mutual_info_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">mutual_info_score</span></code></a>, <a class="reference internal" href="generated/sklearn.metrics.adjusted_mutual_info_score.html#sklearn.metrics.adjusted_mutual_info_score" title="sklearn.metrics.adjusted_mutual_info_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">adjusted_mutual_info_score</span></code></a> 和 <a class="reference internal" href="generated/sklearn.metrics.normalized_mutual_info_score.html#sklearn.metrics.normalized_mutual_info_score" title="sklearn.metrics.normalized_mutual_info_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">normalized_mutual_info_score</span></code></a>
都是对称的(symmetric): 交换函数的参数不会改变得分。 因此它们可以用作 <strong>consensus measure</strong>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">adjusted_mutual_info_score</span><span class="p">(</span><span class="n">labels_pred</span><span class="p">,</span> <span class="n">labels_true</span><span class="p">)</span>  
<span class="go">0.22504...</span>
</pre></div>
</div>
<p>完美标签分配(Perfect labeling)的得分是 1.0:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">labels_pred</span> <span class="o">=</span> <span class="n">labels_true</span><span class="p">[:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">adjusted_mutual_info_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>  
<span class="go">1.0</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">normalized_mutual_info_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>  
<span class="go">1.0</span>
</pre></div>
</div>
<p>这对于 <code class="docutils literal notranslate"><span class="pre">mutual_info_score</span></code> 是不成立的, 因此该得分更难于判断:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">mutual_info_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>  
<span class="go">0.69...</span>
</pre></div>
</div>
<p>坏的标签分配 (e.g. independent labelings) 具有负的得分(non-positive scores):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">labels_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">adjusted_mutual_info_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>  
<span class="go">-0.10526...</span>
</pre></div>
</div>
<div class="section" id="id24">
<h4>2.3.9.2.1. 优点<a class="headerlink" href="#id24" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><strong>随机(均匀)标签分配有一个接近于0的 AMI得分。</strong>
对于 <code class="docutils literal notranslate"><span class="pre">n_clusters</span></code> 和 <code class="docutils literal notranslate"><span class="pre">n_samples</span></code> 的任何值（这不是 raw Mutual Information 或者 V-measure 的情况）。</li>
<li><strong>上界为 1</strong> :  得分值接近于 0 表明两个标签分配集合很大程度上是独立的(largely independent), 而得分值接近于 1 表明两个标签分配集合
具有很大的一致性(significant agreement)。 更进一步, 正好是1的AMI表示两个标签分配相等。 (with or without permutation).</li>
</ul>
</div>
<div class="section" id="id25">
<h4>2.3.9.2.2. 缺点<a class="headerlink" href="#id25" title="Permalink to this headline">¶</a></h4>
<ul>
<li><p class="first">与惯性(inertia)方法不同, <strong>基于互信息的度量(MI-based measures) 需要 ground truth classes 的相关知识</strong>
而在实践中几乎不可得到，或者需要人工标注者手动分配（如在监督学习环境中）。</p>
<p>然而，MI-based measures 还可以在纯粹无监督的设置中作为可用于 聚类模型选择 的共识索引的构建模块。</p>
</li>
<li><p class="first">NMI and MI are not adjusted against chance.</p>
</li>
</ul>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_adjusted_for_chance_measures.html#sphx-glr-auto-examples-cluster-plot-adjusted-for-chance-measures-py"><span class="std std-ref">Adjustment for chance in clustering performance evaluation</span></a>:</li>
</ul>
<blockquote>
<div>分析数据集大小对随机分配聚类度量值的影响。 此示例还包括 Adjusted Rand Index。</div></blockquote>
</div>
</div>
<div class="section" id="id26">
<h4>2.3.9.2.3. 数学表达形式<a class="headerlink" href="#id26" title="Permalink to this headline">¶</a></h4>
<p>假定我们有两个标签分配集合(of the same N objects), <span class="math notranslate nohighlight">\(U\)</span> 和 <span class="math notranslate nohighlight">\(V\)</span>.
它们的熵(entropy)是划分集(partition set)的不确定性量(the amount of uncertainty), 定义如下:</p>
<div class="math notranslate nohighlight">
\[H(U) = - \sum_{i=1}^{|U|}P(i)\log(P(i))\]</div>
<p>其中 <span class="math notranslate nohighlight">\(P(i) = |U_i| / N\)</span> 是从 <span class="math notranslate nohighlight">\(U\)</span> 集合中随机挑选的一个object落到 <span class="math notranslate nohighlight">\(U_i\)</span> 集合中的概率。
对于 <span class="math notranslate nohighlight">\(V\)</span> 集合也是一样的:</p>
<div class="math notranslate nohighlight">
\[H(V) = - \sum_{j=1}^{|V|}P'(j)\log(P'(j))\]</div>
<p>其中 <span class="math notranslate nohighlight">\(P'(j) = |V_j| / N\)</span>。 <span class="math notranslate nohighlight">\(U\)</span> 和 <span class="math notranslate nohighlight">\(V\)</span> 之间的互信息(mutual information)的计算公式如下 :</p>
<div class="math notranslate nohighlight">
\[\text{MI}(U, V) = \sum_{i=1}^{|U|}\sum_{j=1}^{|V|}P(i, j)\log\left(\frac{P(i,j)}{P(i)P'(j)}\right)\]</div>
<p>其中 <span class="math notranslate nohighlight">\(P(i, j) = |U_i \cap V_j| / N\)</span> 是随机选择的object落到这两类集合 <span class="math notranslate nohighlight">\(U_i\)</span> 和 <span class="math notranslate nohighlight">\(V_j\)</span> 中的概率。</p>
<p>互信息还可以用set cardinality的形式来表述 :</p>
<div class="math notranslate nohighlight">
\[\text{MI}(U, V) = \sum_{i=1}^{|U|} \sum_{j=1}^{|V|} \frac{|U_i \cap V_j|}{N}\log\left(\frac{N|U_i \cap V_j|}{|U_i||V_j|}\right)\]</div>
<p>归一化的互信息定义如下:</p>
<div class="math notranslate nohighlight">
\[\text{NMI}(U, V) = \frac{\text{MI}(U, V)}{\text{mean}(H(U), H(V))}\]</div>
<p>不管两个标签分配集合(label assignments)之间的互信息的实际量有多大，互信息的值包括归一化互信息的值没有针对偶然性进行调整(not adjusted for chance)
而且 倾向于随着不同标签(聚类)的数量的增加而增加。</p>
<p>互信息的期望值可以用等式 [VEB2009] 计算。在这个等式中, <span class="math notranslate nohighlight">\(a_i = |U_i|\)</span> (<span class="math notranslate nohighlight">\(U_i\)</span> 集合中的元素数量) 和
<span class="math notranslate nohighlight">\(b_j = |V_j|\)</span> ( <span class="math notranslate nohighlight">\(V_j\)</span> 集合中的元素数量)。</p>
<div class="math notranslate nohighlight">
\[E[\text{MI}(U,V)]=\sum_{i=1}^{|U|} \sum_{j=1}^{|V|} \sum_{n_{ij}=(a_i+b_j-N)^+
}^{\min(a_i, b_j)} \frac{n_{ij}}{N}\log \left( \frac{ N.n_{ij}}{a_i b_j}\right)
\frac{a_i!b_j!(N-a_i)!(N-b_j)!}{N!n_{ij}!(a_i-n_{ij})!(b_j-n_{ij})!
(N-a_i-b_j+n_{ij})!}\]</div>
<p>使用了互信息期望值后, 经过调整的互信息的计算将使用与 ARI(adjusted Rand index) 类似的形式进行 :</p>
<div class="math notranslate nohighlight">
\[\text{AMI} = \frac{\text{MI} - E[\text{MI}]}{\text{mean}(H(U), H(V)) - E[\text{MI}]}\]</div>
<p>对于归一化互信息和调整后的互信息，归一化值通常是每个聚类的熵的一些广义均值(<em>generalized</em> mean)。
有各种广义均值存在，并没有明确的规则说某一个优先于其他的。这个决定很大程度上是取决于各个领域的基础；
例如，在社区检测(community detection)中，算术平均值是最常见的。每一种归一化方法提供 “qualitatively similar behaviours” [YAT2016]。
在我们的实现中, 这是通过参数 <code class="docutils literal notranslate"><span class="pre">average_method</span></code> 进行控制的。</p>
<p>Vinh et al. (2010) 对各种 NMI 和 AMI 的变体 用它们使用的平均方法(averaging method) 进行了命名 [VEB2010] 。
他们在论文里说的 ‘sqrt’ 和 ‘sum’ 平均 分别是 几何 和 算数 平均；我们使用这些更广泛的通用名称。</p>
<div class="topic">
<p class="topic-title first">参考文献</p>
<ul class="simple">
<li>Strehl, Alexander, and Joydeep Ghosh (2002). “Cluster ensembles – a
knowledge reuse framework for combining multiple partitions”. Journal of
Machine Learning Research 3: 583–617.
<a class="reference external" href="http://strehl.com/download/strehl-jmlr02.pdf">doi:10.1162/153244303321897735</a>.</li>
<li>[VEB2009] Vinh, Epps, and Bailey, (2009). “Information theoretic measures
for clusterings comparison”. Proceedings of the 26th Annual International
Conference on Machine Learning - ICML ‘09.
<a class="reference external" href="https://dl.acm.org/citation.cfm?doid=1553374.1553511">doi:10.1145/1553374.1553511</a>.
ISBN 9781605585161.</li>
<li>[VEB2010] Vinh, Epps, and Bailey, (2010). “Information Theoretic Measures for
Clusterings Comparison: Variants, Properties, Normalization and
Correction for Chance”. JMLR
&lt;<a class="reference external" href="http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf">http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf</a>&gt;</li>
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Mutual_Information">Wikipedia entry for the (normalized) Mutual Information</a></li>
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Adjusted_Mutual_Information">Wikipedia entry for the Adjusted Mutual Information</a></li>
<li>[YAT2016] Yang, Algesheimer, and Tessone, (2016). “A comparative analysis of
community
detection algorithms on artificial networks”. Scientific Reports 6: 30750.
<a class="reference external" href="https://www.nature.com/articles/srep30750">doi:10.1038/srep30750</a>.</li>
</ul>
</div>
</div>
</div>
<div class="section" id="homogeneity-completeness-and-v-measure">
<span id="homogeneity-completeness"></span><h3>2.3.9.3. Homogeneity, completeness and V-measure<a class="headerlink" href="#homogeneity-completeness-and-v-measure" title="Permalink to this headline">¶</a></h3>
<p>Given the knowledge of the ground truth class assignments of the samples,
it is possible to define some intuitive metric using conditional entropy
analysis.</p>
<p>In particular Rosenberg and Hirschberg (2007) define the following two
desirable objectives for any cluster assignment:</p>
<ul class="simple">
<li><strong>homogeneity</strong>: each cluster contains only members of a single class.</li>
<li><strong>completeness</strong>: all members of a given class are assigned to the same
cluster.</li>
</ul>
<p>We can turn those concept as scores <a class="reference internal" href="generated/sklearn.metrics.homogeneity_score.html#sklearn.metrics.homogeneity_score" title="sklearn.metrics.homogeneity_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">homogeneity_score</span></code></a> and
<a class="reference internal" href="generated/sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score" title="sklearn.metrics.completeness_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">completeness_score</span></code></a>. Both are bounded below by 0.0 and above by
1.0 (higher is better):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">metrics</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">homogeneity_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>  
<span class="go">0.66...</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">completeness_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span> 
<span class="go">0.42...</span>
</pre></div>
</div>
<p>Their harmonic mean called <strong>V-measure</strong> is computed by
<a class="reference internal" href="generated/sklearn.metrics.v_measure_score.html#sklearn.metrics.v_measure_score" title="sklearn.metrics.v_measure_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">v_measure_score</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">v_measure_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>    
<span class="go">0.51...</span>
</pre></div>
</div>
<p>The V-measure is actually equivalent to the mutual information (NMI)
discussed above, with the aggregation function being the arithmetic mean <a class="reference internal" href="#b2011" id="id30">[B2011]</a>.</p>
<p>Homogeneity, completeness and V-measure can be computed at once using
<a class="reference internal" href="generated/sklearn.metrics.homogeneity_completeness_v_measure.html#sklearn.metrics.homogeneity_completeness_v_measure" title="sklearn.metrics.homogeneity_completeness_v_measure"><code class="xref py py-func docutils literal notranslate"><span class="pre">homogeneity_completeness_v_measure</span></code></a> as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">homogeneity_completeness_v_measure</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>
<span class="gp">... </span>                                                     
<span class="go">(0.66..., 0.42..., 0.51...)</span>
</pre></div>
</div>
<p>The following clustering assignment is slightly better, since it is
homogeneous but not complete:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">labels_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">homogeneity_completeness_v_measure</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>
<span class="gp">... </span>                                                     
<span class="go">(1.0, 0.68..., 0.81...)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><a class="reference internal" href="generated/sklearn.metrics.v_measure_score.html#sklearn.metrics.v_measure_score" title="sklearn.metrics.v_measure_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">v_measure_score</span></code></a> is <strong>symmetric</strong>: it can be used to evaluate
the <strong>agreement</strong> of two independent assignments on the same dataset.</p>
<p>This is not the case for <a class="reference internal" href="generated/sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score" title="sklearn.metrics.completeness_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">completeness_score</span></code></a> and
<a class="reference internal" href="generated/sklearn.metrics.homogeneity_score.html#sklearn.metrics.homogeneity_score" title="sklearn.metrics.homogeneity_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">homogeneity_score</span></code></a>: both are bound by the relationship:</p>
<div class="last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">homogeneity_score</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">==</span> <span class="n">completeness_score</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="id31">
<h4>2.3.9.3.1. 优点<a class="headerlink" href="#id31" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><strong>Bounded scores</strong>: 0.0 is as bad as it can be, 1.0 is a perfect score.</li>
<li>Intuitive interpretation: clustering with bad V-measure can be
<strong>qualitatively analyzed in terms of homogeneity and completeness</strong>
to better feel what ‘kind’ of mistakes is done by the assignment.</li>
<li><strong>No assumption is made on the cluster structure</strong>: can be used
to compare clustering algorithms such as k-means which assumes isotropic
blob shapes with results of spectral clustering algorithms which can
find cluster with “folded” shapes.</li>
</ul>
</div>
<div class="section" id="id32">
<h4>2.3.9.3.2. 缺点<a class="headerlink" href="#id32" title="Permalink to this headline">¶</a></h4>
<ul>
<li><p class="first">The previously introduced metrics are <strong>not normalized with regards to
random labeling</strong>: this means that depending on the number of samples,
clusters and ground truth classes, a completely random labeling will
not always yield the same values for homogeneity, completeness and
hence v-measure. In particular <strong>random labeling won’t yield zero
scores especially when the number of clusters is large</strong>.</p>
<p>This problem can safely be ignored when the number of samples is more
than a thousand and the number of clusters is less than 10. <strong>For
smaller sample sizes or larger number of clusters it is safer to use
an adjusted index such as the Adjusted Rand Index (ARI)</strong>.</p>
</li>
</ul>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/cluster/plot_adjusted_for_chance_measures.html"><img alt="../images/sphx_glr_plot_adjusted_for_chance_measures_0011.png" src="../images/sphx_glr_plot_adjusted_for_chance_measures_0011.png" style="width: 640.0px; height: 480.0px;" /></a>
</div>
<ul class="simple">
<li>These metrics <strong>require the knowledge of the ground truth classes</strong> while
almost never available in practice or requires manual assignment by
human annotators (as in the supervised learning setting).</li>
</ul>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_adjusted_for_chance_measures.html#sphx-glr-auto-examples-cluster-plot-adjusted-for-chance-measures-py"><span class="std std-ref">Adjustment for chance in clustering performance evaluation</span></a>: Analysis of
the impact of the dataset size on the value of clustering measures
for random assignments.</li>
</ul>
</div>
</div>
<div class="section" id="id33">
<h4>2.3.9.3.3. 数学表达形式<a class="headerlink" href="#id33" title="Permalink to this headline">¶</a></h4>
<p>Homogeneity and completeness scores are formally given by:</p>
<div class="math notranslate nohighlight">
\[h = 1 - \frac{H(C|K)}{H(C)}\]</div>
<div class="math notranslate nohighlight">
\[c = 1 - \frac{H(K|C)}{H(K)}\]</div>
<p>where <span class="math notranslate nohighlight">\(H(C|K)\)</span> is the <strong>conditional entropy of the classes given
the cluster assignments</strong> and is given by:</p>
<div class="math notranslate nohighlight">
\[H(C|K) = - \sum_{c=1}^{|C|} \sum_{k=1}^{|K|} \frac{n_{c,k}}{n}
\cdot \log\left(\frac{n_{c,k}}{n_k}\right)\]</div>
<p>and <span class="math notranslate nohighlight">\(H(C)\)</span> is the <strong>entropy of the classes</strong> and is given by:</p>
<div class="math notranslate nohighlight">
\[H(C) = - \sum_{c=1}^{|C|} \frac{n_c}{n} \cdot \log\left(\frac{n_c}{n}\right)\]</div>
<p>with <span class="math notranslate nohighlight">\(n\)</span> the total number of samples, <span class="math notranslate nohighlight">\(n_c\)</span> and <span class="math notranslate nohighlight">\(n_k\)</span>
the number of samples respectively belonging to class <span class="math notranslate nohighlight">\(c\)</span> and
cluster <span class="math notranslate nohighlight">\(k\)</span>, and finally <span class="math notranslate nohighlight">\(n_{c,k}\)</span> the number of samples
from class <span class="math notranslate nohighlight">\(c\)</span> assigned to cluster <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>The <strong>conditional entropy of clusters given class</strong> <span class="math notranslate nohighlight">\(H(K|C)\)</span> and the
<strong>entropy of clusters</strong> <span class="math notranslate nohighlight">\(H(K)\)</span> are defined in a symmetric manner.</p>
<p>Rosenberg and Hirschberg further define <strong>V-measure</strong> as the <strong>harmonic
mean of homogeneity and completeness</strong>:</p>
<div class="math notranslate nohighlight">
\[v = 2 \cdot \frac{h \cdot c}{h + c}\]</div>
<div class="topic">
<p class="topic-title first">References</p>
<ul class="simple">
<li><a class="reference external" href="http://aclweb.org/anthology/D/D07/D07-1043.pdf">V-Measure: A conditional entropy-based external cluster evaluation
measure</a>
Andrew Rosenberg and Julia Hirschberg, 2007</li>
</ul>
<table class="docutils citation" frame="void" id="b2011" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id30">[B2011]</a></td><td><a class="reference external" href="http://www.cs.columbia.edu/~hila/hila-thesis-distributed.pdf">Identication and Characterization of Events in Social Media</a>, Hila
Becker, PhD Thesis.</td></tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="section" id="fowlkes-mallows-scores">
<span id="id34"></span><h3>2.3.9.4. Fowlkes-Mallows scores<a class="headerlink" href="#fowlkes-mallows-scores" title="Permalink to this headline">¶</a></h3>
<p>The Fowlkes-Mallows index (<a class="reference internal" href="generated/sklearn.metrics.fowlkes_mallows_score.html#sklearn.metrics.fowlkes_mallows_score" title="sklearn.metrics.fowlkes_mallows_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.metrics.fowlkes_mallows_score</span></code></a>) can be
used when the ground truth class assignments of the samples is known. The
Fowlkes-Mallows score FMI is defined as the geometric mean of the
pairwise precision and recall:</p>
<div class="math notranslate nohighlight">
\[\text{FMI} = \frac{\text{TP}}{\sqrt{(\text{TP} + \text{FP}) (\text{TP} + \text{FN})}}\]</div>
<p>Where <code class="docutils literal notranslate"><span class="pre">TP</span></code> is the number of <strong>True Positive</strong> (i.e. the number of pair
of points that belong to the same clusters in both the true labels and the
predicted labels), <code class="docutils literal notranslate"><span class="pre">FP</span></code> is the number of <strong>False Positive</strong> (i.e. the number
of pair of points that belong to the same clusters in the true labels and not
in the predicted labels) and <code class="docutils literal notranslate"><span class="pre">FN</span></code> is the number of <strong>False Negative</strong> (i.e the
number of pair of points that belongs in the same clusters in the predicted
labels and not in the true labels).</p>
<p>The score ranges from 0 to 1. A high value indicates a good similarity
between two clusters.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">metrics</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">fowlkes_mallows_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>  <span class="c1"># doctest: +ELLIPSIS</span>
<span class="go">0.47140...</span>
</pre></div>
</div>
<p>One can permute 0 and 1 in the predicted labels, rename 2 to 3 and get
the same score:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">labels_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">fowlkes_mallows_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>  
<span class="go">0.47140...</span>
</pre></div>
</div>
<p>Perfect labeling is scored 1.0:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">labels_pred</span> <span class="o">=</span> <span class="n">labels_true</span><span class="p">[:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">fowlkes_mallows_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>  
<span class="go">1.0</span>
</pre></div>
</div>
<p>Bad (e.g. independent labelings) have zero scores:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">labels_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">fowlkes_mallows_score</span><span class="p">(</span><span class="n">labels_true</span><span class="p">,</span> <span class="n">labels_pred</span><span class="p">)</span>  
<span class="go">0.0</span>
</pre></div>
</div>
<div class="section" id="id35">
<h4>2.3.9.4.1. 优点<a class="headerlink" href="#id35" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><strong>Random (uniform) label assignments have a FMI score close to 0.0</strong>
for any value of <code class="docutils literal notranslate"><span class="pre">n_clusters</span></code> and <code class="docutils literal notranslate"><span class="pre">n_samples</span></code> (which is not the
case for raw Mutual Information or the V-measure for instance).</li>
<li><strong>Upper-bounded at 1</strong>:  Values close to zero indicate two label
assignments that are largely independent, while values close to one
indicate significant agreement. Further, values of exactly 0 indicate
<strong>purely</strong> independent label assignments and a FMI of exactly 1 indicates
that the two label assignments are equal (with or without permutation).</li>
<li><strong>No assumption is made on the cluster structure</strong>: can be used
to compare clustering algorithms such as k-means which assumes isotropic
blob shapes with results of spectral clustering algorithms which can
find cluster with “folded” shapes.</li>
</ul>
</div>
<div class="section" id="id36">
<h4>2.3.9.4.2. 缺点<a class="headerlink" href="#id36" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Contrary to inertia, <strong>FMI-based measures require the knowledge
of the ground truth classes</strong> while almost never available in practice or
requires manual assignment by human annotators (as in the supervised learning
setting).</li>
</ul>
<div class="topic">
<p class="topic-title first">References</p>
<ul class="simple">
<li>E. B. Fowkles and C. L. Mallows, 1983. “A method for comparing two
hierarchical clusterings”. Journal of the American Statistical Association.
<a class="reference external" href="http://wildfire.stat.ucla.edu/pdflibrary/fowlkes.pdf">http://wildfire.stat.ucla.edu/pdflibrary/fowlkes.pdf</a></li>
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Fowlkes-Mallows_index">Wikipedia entry for the Fowlkes-Mallows Index</a></li>
</ul>
</div>
</div>
</div>
<div class="section" id="silhouette-coefficient">
<span id="id37"></span><h3>2.3.9.5. Silhouette Coefficient<a class="headerlink" href="#silhouette-coefficient" title="Permalink to this headline">¶</a></h3>
<p>If the ground truth labels are not known, evaluation must be performed using
the model itself. The Silhouette Coefficient
(<a class="reference internal" href="generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score" title="sklearn.metrics.silhouette_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.metrics.silhouette_score</span></code></a>)
is an example of such an evaluation, where a
higher Silhouette Coefficient score relates to a model with better defined
clusters. The Silhouette Coefficient is defined for each sample and is composed
of two scores:</p>
<ul class="simple">
<li><strong>a</strong>: The mean distance between a sample and all other points in the same
class.</li>
<li><strong>b</strong>: The mean distance between a sample and all other points in the <em>next
nearest cluster</em>.</li>
</ul>
<p>The Silhouette Coefficient <em>s</em> for a single sample is then given as:</p>
<div class="math notranslate nohighlight">
\[s = \frac{b - a}{max(a, b)}\]</div>
<p>The Silhouette Coefficient for a set of samples is given as the mean of the
Silhouette Coefficient for each sample.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">metrics</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">pairwise_distances</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">target</span>
</pre></div>
</div>
<p>In normal usage, the Silhouette Coefficient is applied to the results of a
cluster analysis.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="k">import</span> <span class="n">KMeans</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kmeans_model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans_model</span><span class="o">.</span><span class="n">labels_</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">silhouette_score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">)</span>
<span class="gp">... </span>                                                     <span class="c1"># doctest: +ELLIPSIS</span>
<span class="go">0.55...</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">References</p>
<ul class="simple">
<li>Peter J. Rousseeuw (1987). “Silhouettes: a Graphical Aid to the
Interpretation and Validation of Cluster Analysis”. Computational
and Applied Mathematics 20: 53–65.
<a class="reference external" href="https://doi.org/10.1016/0377-0427(87)90125-7">doi:10.1016/0377-0427(87)90125-7</a>.</li>
</ul>
</div>
<div class="section" id="id38">
<h4>2.3.9.5.1. 优点<a class="headerlink" href="#id38" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>The score is bounded between -1 for incorrect clustering and +1 for highly
dense clustering. Scores around zero indicate overlapping clusters.</li>
<li>The score is higher when clusters are dense and well separated, which relates
to a standard concept of a cluster.</li>
</ul>
</div>
<div class="section" id="id39">
<h4>2.3.9.5.2. 缺点<a class="headerlink" href="#id39" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>The Silhouette Coefficient is generally higher for convex clusters than other
concepts of clusters, such as density based clusters like those obtained
through DBSCAN.</li>
</ul>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py"><span class="std std-ref">Selecting the number of clusters with silhouette analysis on KMeans clustering</span></a> : In this example
the silhouette analysis is used to choose an optimal value for n_clusters.</li>
</ul>
</div>
</div>
</div>
<div class="section" id="calinski-harabaz-index">
<span id="id40"></span><h3>2.3.9.6. Calinski-Harabaz Index<a class="headerlink" href="#calinski-harabaz-index" title="Permalink to this headline">¶</a></h3>
<p>If the ground truth labels are not known, the Calinski-Harabaz index
(<a class="reference internal" href="generated/sklearn.metrics.calinski_harabaz_score.html#sklearn.metrics.calinski_harabaz_score" title="sklearn.metrics.calinski_harabaz_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.metrics.calinski_harabaz_score</span></code></a>) - also known as the Variance
Ratio Criterion - can be used to evaluate the model, where a higher
Calinski-Harabaz score relates to a model with better defined clusters.</p>
<p>For <span class="math notranslate nohighlight">\(k\)</span> clusters, the Calinski-Harabaz score <span class="math notranslate nohighlight">\(s\)</span> is given as the
ratio of the between-clusters dispersion mean and the within-cluster
dispersion:</p>
<div class="math notranslate nohighlight">
\[s(k) = \frac{\mathrm{Tr}(B_k)}{\mathrm{Tr}(W_k)} \times \frac{N - k}{k - 1}\]</div>
<p>where <span class="math notranslate nohighlight">\(B_K\)</span> is the between group dispersion matrix and <span class="math notranslate nohighlight">\(W_K\)</span>
is the within-cluster dispersion matrix defined by:</p>
<div class="math notranslate nohighlight">
\[W_k = \sum_{q=1}^k \sum_{x \in C_q} (x - c_q) (x - c_q)^T\]</div>
<div class="math notranslate nohighlight">
\[B_k = \sum_q n_q (c_q - c) (c_q - c)^T\]</div>
<p>with <span class="math notranslate nohighlight">\(N\)</span> be the number of points in our data, <span class="math notranslate nohighlight">\(C_q\)</span> be the set of
points in cluster <span class="math notranslate nohighlight">\(q\)</span>, <span class="math notranslate nohighlight">\(c_q\)</span> be the center of cluster
<span class="math notranslate nohighlight">\(q\)</span>, <span class="math notranslate nohighlight">\(c\)</span> be the center of <span class="math notranslate nohighlight">\(E\)</span>, <span class="math notranslate nohighlight">\(n_q\)</span> be the number of
points in cluster <span class="math notranslate nohighlight">\(q\)</span>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">metrics</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">pairwise_distances</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">target</span>
</pre></div>
</div>
<p>In normal usage, the Calinski-Harabaz index is applied to the results of a
cluster analysis.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="k">import</span> <span class="n">KMeans</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kmeans_model</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans_model</span><span class="o">.</span><span class="n">labels_</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">calinski_harabaz_score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>  <span class="c1"># doctest: +ELLIPSIS</span>
<span class="go">561.62...</span>
</pre></div>
</div>
<div class="section" id="id41">
<h4>2.3.9.6.1. 优点<a class="headerlink" href="#id41" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>The score is higher when clusters are dense and well separated, which relates
to a standard concept of a cluster.</li>
<li>The score is fast to compute</li>
</ul>
</div>
<div class="section" id="id42">
<h4>2.3.9.6.2. 缺点<a class="headerlink" href="#id42" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>The Calinski-Harabaz index is generally higher for convex clusters than other
concepts of clusters, such as density based clusters like those obtained
through DBSCAN.</li>
</ul>
<div class="topic">
<p class="topic-title first">References</p>
<ul class="simple">
<li>Caliński, T., &amp; Harabasz, J. (1974). “A dendrite method for cluster
analysis”. Communications in Statistics-theory and Methods 3: 1-27.
<a class="reference external" href="https://doi.org/10.1080/03610926.2011.560741">doi:10.1080/03610926.2011.560741</a>.</li>
</ul>
</div>
</div>
</div>
<div class="section" id="davies-bouldin-index">
<span id="id43"></span><h3>2.3.9.7. Davies-Bouldin Index<a class="headerlink" href="#davies-bouldin-index" title="Permalink to this headline">¶</a></h3>
<p>If the ground truth labels are not known, the Davies-Bouldin index
(<a class="reference internal" href="generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score" title="sklearn.metrics.davies_bouldin_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.metrics.davies_bouldin_score</span></code></a>) can be used to evaluate the
model, where a lower Davies-Bouldin index relates to a model with better
separation between the clusters.</p>
<p>The index is defined as the average similarity between each cluster <span class="math notranslate nohighlight">\(C_i\)</span>
for <span class="math notranslate nohighlight">\(i=1, ..., k\)</span> and its most similar one <span class="math notranslate nohighlight">\(C_j\)</span>. In the context of
this index, similarity is defined as a measure <span class="math notranslate nohighlight">\(R_{ij}\)</span> that trades off:</p>
<ul class="simple">
<li><span class="math notranslate nohighlight">\(s_i\)</span>, the average distance between each point of cluster <span class="math notranslate nohighlight">\(i\)</span> and
the centroid of that cluster – also know as cluster diameter.</li>
<li><span class="math notranslate nohighlight">\(d_{ij}\)</span>, the distance between cluster centroids <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>.</li>
</ul>
<p>A simple choice to construct <span class="math notranslate nohighlight">\(R_ij\)</span> so that it is nonnegative and
symmetric is:</p>
<div class="math notranslate nohighlight">
\[R_{ij} = \frac{s_i + s_j}{d_{ij}}\]</div>
<p>Then the Davies-Bouldin index is defined as:</p>
<div class="math notranslate nohighlight">
\[DB = \frac{1}{k} \sum_{i=1}^k \max_{i \neq j} R_{ij}\]</div>
<p>Zero is the lowest possible score. Values closer to zero indicate a better
partition.</p>
<p>In normal usage, the Davies-Bouldin index is applied to the results of a
cluster analysis as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="k">import</span> <span class="n">KMeans</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">davies_bouldin_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">davies_bouldin_score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>  <span class="c1"># doctest: +ELLIPSIS</span>
<span class="go">0.6619...</span>
</pre></div>
</div>
<div class="section" id="id44">
<h4>2.3.9.7.1. 优点<a class="headerlink" href="#id44" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>The computation of Davies-Bouldin is simpler than that of Silhouette scores.</li>
<li>The index is computed only quantities and features inherent to the dataset.</li>
</ul>
</div>
<div class="section" id="id45">
<h4>2.3.9.7.2. 缺点<a class="headerlink" href="#id45" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>The Davies-Boulding index is generally higher for convex clusters than other
concepts of clusters, such as density based clusters like those obtained from
DBSCAN.</li>
<li>The usage of centroid distance limits the distance metric to Euclidean space.</li>
<li>A good value reported by this method does not imply the best information retrieval.</li>
</ul>
<div class="topic">
<p class="topic-title first">References</p>
<ul class="simple">
<li>Davies, David L.; Bouldin, Donald W. (1979).
“A Cluster Separation Measure”
IEEE Transactions on Pattern Analysis and Machine Intelligence.
PAMI-1 (2): 224-227.
<a class="reference external" href="http://dx.doi.org/10.1109/TPAMI.1979.4766909">doi:10.1109/TPAMI.1979.4766909</a>.</li>
<li>Halkidi, Maria; Batistakis, Yannis; Vazirgiannis, Michalis (2001).
“On Clustering Validation Techniques”
Journal of Intelligent Information Systems, 17(2-3), 107-145.
<a class="reference external" href="http://dx.doi.org/10.1023/A:1012801612483">doi:10.1023/A:1012801612483</a>.</li>
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Davies–Bouldin_index">Wikipedia entry for Davies-Bouldin index</a>.</li>
</ul>
</div>
</div>
</div>
<div class="section" id="contingency-matrix">
<span id="id46"></span><h3>2.3.9.8. Contingency Matrix<a class="headerlink" href="#contingency-matrix" title="Permalink to this headline">¶</a></h3>
<p>Contingency matrix (<a class="reference internal" href="generated/sklearn.metrics.cluster.contingency_matrix.html#sklearn.metrics.cluster.contingency_matrix" title="sklearn.metrics.cluster.contingency_matrix"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.metrics.cluster.contingency_matrix</span></code></a>)
reports the intersection cardinality for every true/predicted cluster pair.
The contingency matrix provides sufficient statistics for all clustering
metrics where the samples are independent and identically distributed and
one doesn’t need to account for some instances not being clustered.</p>
<p>Here is an example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics.cluster</span> <span class="k">import</span> <span class="n">contingency_matrix</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">contingency_matrix</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">array([[2, 1, 0],</span>
<span class="go">       [0, 1, 2]])</span>
</pre></div>
</div>
<p>The first row of output array indicates that there are three samples whose
true cluster is “a”. Of them, two are in predicted cluster 0, one is in 1,
and none is in 2. And the second row indicates that there are three samples
whose true cluster is “b”. Of them, none is in predicted cluster 0, one is in
1 and two are in 2.</p>
<p>A <a class="reference internal" href="model_evaluation.html#confusion-matrix"><span class="std std-ref">confusion matrix</span></a> for classification is a square
contingency matrix where the order of rows and columns correspond to a list
of classes.</p>
<div class="section" id="id47">
<h4>2.3.9.8.1. 优点<a class="headerlink" href="#id47" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Allows to examine the spread of each true cluster across predicted
clusters and vice versa.</li>
<li>The contingency table calculated is typically utilized in the calculation
of a similarity statistic (like the others listed in this document) between
the two clusterings.</li>
</ul>
</div>
<div class="section" id="id48">
<h4>2.3.9.8.2. 缺点<a class="headerlink" href="#id48" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Contingency matrix is easy to interpret for a small number of clusters, but
becomes very hard to interpret for a large number of clusters.</li>
<li>It doesn’t give a single metric to use as an objective for clustering
optimisation.</li>
</ul>
<div class="topic">
<p class="topic-title first">References</p>
<ul class="simple">
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Contingency_table">Wikipedia entry for contingency matrix</a></li>
</ul>
</div>
</div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer">
        &copy; 2007 - 2018, scikit-learn developers (BSD License).
      <a href="../_sources/modules/clustering.rst.txt" rel="nofollow">Show this page source</a>
    </div>
     <div class="rel">
    
    <div class="buttonPrevious">
      <a href="manifold.html">Previous
      </a>
    </div>
    <div class="buttonNext">
      <a href="biclustering.html">Next
      </a>
    </div>
    
     </div>

    
    <script>
        window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
        ga('create', 'UA-22606712-2', 'auto');
        ga('set', 'anonymizeIp', true);
        ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    
    <script>
      (function() {
        var cx = '016639176250731907682:tjtqbvtvij0';
        var gcse = document.createElement('script'); gcse.type = 'text/javascript'; gcse.async = true;
        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s);
      })();
    </script>
  </body>
</html>