

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  
    <title>1.1. 广义线性模型(Generalized Linear Models) &#8212; scikit-learn 0.20.1 documentation</title>
  <!-- htmltitle is before nature.css - we use this hack to load bootstrap first -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="../static/css/bootstrap.min.css" media="screen" />
  <link rel="stylesheet" href="../static/css/bootstrap-responsive.css"/>

    <link rel="stylesheet" href="../static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../static/gallery.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../static/documentation_options.js"></script>
    <script type="text/javascript" src="../static/jquery.js"></script>
    <script type="text/javascript" src="../static/underscore.js"></script>
    <script type="text/javascript" src="../static/doctools.js"></script>
    <script type="text/javascript" src="../static/js/copybutton.js"></script>
    <script type="text/javascript" src="../static/js/extra.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_SVG"></script>
    <link rel="shortcut icon" href="../static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1.2. 线性判别分析和二次判别分析（Linear and Quadratic Discriminant Analysis）" href="lda_qda.html" />
    <link rel="prev" title="1. 监督学习(Supervised learning)" href="../supervised_learning.html" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <script src="../static/js/bootstrap.min.js" type="text/javascript"></script>
  <script>
     VERSION_SUBDIR = (function(groups) {
         return groups ? groups[1] : null;
     })(location.href.match(/^https?:\/\/scikit-learn.org\/([^\/]+)/));
  </script>
  <link rel="canonical" href="http://scikit-learn.org/stable/modules/linear_model.html" />

  <script type="text/javascript">
    $("div.buttonNext, div.buttonPrevious").hover(
       function () {
           $(this).css('background-color', '#FF9C34');
       },
       function () {
           $(this).css('background-color', '#A7D6E2');
       }
    );
    function showMenu() {
      var topNav = document.getElementById("scikit-navbar");
      if (topNav.className === "navbar") {
          topNav.className += " responsive";
      } else {
          topNav.className = "navbar";
      }
    };
  </script>

  </head><body>

<div class="header-wrapper">
    <div class="header">
        <p class="logo"><a href="../index.html">
            <img src="../static/scikit-learn-logo-small.png" alt="Logo"/>
        </a>
        </p><div class="navbar" id="scikit-navbar">
            <ul>
                <li><a href="../index.html">Home</a></li>
                <li><a href="../install.html">Installation</a></li>
                <li class="btn-li"><div class="btn-group">
              <a href="../documentation.html">Documentation</a>
              <a class="btn dropdown-toggle" data-toggle="dropdown">
                 <span class="caret"></span>
              </a>
              <ul class="dropdown-menu">
            <li class="link-title">Scikit-learn <script>document.write(DOCUMENTATION_OPTIONS.VERSION + (VERSION_SUBDIR ? " (" + VERSION_SUBDIR + ")" : ""));</script></li>
            <li><a href="../tutorial/index.html">Tutorials</a></li>
            <li><a href="../user_guide.html">User guide</a></li>
            <li><a href="classes.html">API</a></li>
            <li><a href="../glossary.html">Glossary</a></li>
            <li><a href="../faq.html">FAQ</a></li>
            <li><a href="../developers/contributing.html">Contributing</a></li>
            <li class="divider"></li>
                <script>if (VERSION_SUBDIR != "stable") document.write('<li><a href="http://scikit-learn.org/stable/documentation.html">Stable version</a></li>')</script>
                <script>if (VERSION_SUBDIR != "dev") document.write('<li><a href="http://scikit-learn.org/dev/documentation.html">Development version</a></li>')</script>
                <li><a href="http://scikit-learn.org/dev/versions.html">All available versions</a></li>
                <li><a href="../_downloads/scikit-learn-docs.pdf">PDF documentation</a></li>
              </ul>
            </div>
        </li>
            <li><a href="../auto_examples/index.html">Examples</a></li>
            </ul>
            <a href="javascript:void(0);" onclick="showMenu()">
                <div class="nav-icon">
                    <div class="hamburger-line"></div>
                    <div class="hamburger-line"></div>
                    <div class="hamburger-line"></div>
                </div>
            </a>
            <div class="search_form">
                <div class="gcse-search" id="cse" style="width: 100%;"></div>
            </div>
        </div> <!-- end navbar --></div>
</div>


<!-- GitHub "fork me" ribbon -->
<a href="https://github.com/scikit-learn/scikit-learn">
  <img class="fork-me"
       style="position: absolute; top: 0; right: 0; border: 0;"
       src="../static/img/forkme.png"
       alt="Fork me on GitHub" />
</a>

<div class="content-wrapper">
    <div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
        <div class="rel">
    
        <div class="rellink">
        <a href="../supervised_learning.html"
        accesskey="P">Previous
        <br/>
        <span class="smallrellink">
        1. 监督学习(Super...
        </span>
            <span class="hiddenrellink">
            1. 监督学习(Supervised learning)
            </span>
        </a>
        </div>
            <div class="spacer">
            &nbsp;
            </div>
        <div class="rellink">
        <a href="lda_qda.html"
        accesskey="N">Next
        <br/>
        <span class="smallrellink">
        1.2. 线性判别分析和二...
        </span>
            <span class="hiddenrellink">
            1.2. 线性判别分析和二次判别分析（Linear and Quadratic Discriminant Analysis）
            </span>
        </a>
        </div>

    <!-- Ad a link to the 'up' page -->
        <div class="spacer">
        &nbsp;
        </div>
        <div class="rellink">
        <a href="../supervised_learning.html">
        Up
        <br/>
        <span class="smallrellink">
        1. 监督学习(Super...
        </span>
            <span class="hiddenrellink">
            1. 监督学习(Supervised learning)
            </span>
            
        </a>
        </div>
    </div>
    
      <p class="doc-version"><b>scikit-learn v0.20.1</b><br/>
      <a href="http://scikit-learn.org/dev/versions.html">Other versions</a></p>
    <p class="citing">Please <b><a href="../about.html#citing-scikit-learn" style="font-size: 110%;">cite us </a></b>if you use the software.</p>
    <ul>
<li><a class="reference internal" href="#">1.1. 广义线性模型(Generalized Linear Models)</a><ul>
<li><a class="reference internal" href="#ordinary-least-squares">1.1.1. 普通最小二乘法</a><ul>
<li><a class="reference internal" href="#id2">1.1.1.1. 普通最小二乘法的复杂度</a></li>
</ul>
</li>
<li><a class="reference internal" href="#ridge-regression">1.1.2. 岭回归(Ridge Regression)</a><ul>
<li><a class="reference internal" href="#id4">1.1.2.1. 岭回归的复杂度</a></li>
<li><a class="reference internal" href="#id5">1.1.2.2. 设置正则化参数: 广义交叉验证</a></li>
</ul>
</li>
<li><a class="reference internal" href="#lasso">1.1.3. Lasso</a><ul>
<li><a class="reference internal" href="#id8">1.1.3.1. 设置正则化参数</a><ul>
<li><a class="reference internal" href="#id9">1.1.3.1.1. 使用交叉验证</a></li>
<li><a class="reference internal" href="#id10">1.1.3.1.2. 基于信息标准的模型选择</a></li>
<li><a class="reference internal" href="#svm">1.1.3.1.3. 与 SVM 的正则化参数的比较</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#multi-task-lasso">1.1.4. 多任务 Lasso</a></li>
<li><a class="reference internal" href="#elastic-net">1.1.5. 弹性网(Elastic Net)</a></li>
<li><a class="reference internal" href="#multi-task-elastic-net">1.1.6. 多任务弹性网</a></li>
<li><a class="reference internal" href="#least-angle-regression">1.1.7. 最小角回归</a></li>
<li><a class="reference internal" href="#lars-lasso">1.1.8. LARS Lasso</a><ul>
<li><a class="reference internal" href="#id17">1.1.8.1. 数学化表达式</a></li>
</ul>
</li>
<li><a class="reference internal" href="#omp">1.1.9. 正交匹配追踪法 (OMP)</a></li>
<li><a class="reference internal" href="#bayesian-regression">1.1.10. 贝叶斯回归</a><ul>
<li><a class="reference internal" href="#bayesian-ridge-regression">1.1.10.1. 贝叶斯岭回归</a></li>
<li><a class="reference internal" href="#ard">1.1.10.2. 主动相关决策理论 - ARD</a></li>
</ul>
</li>
<li><a class="reference internal" href="#logistic">1.1.11. Logistic 回归</a></li>
<li><a class="reference internal" href="#sgd">1.1.12. 随机梯度下降 - SGD</a></li>
<li><a class="reference internal" href="#perceptron">1.1.13. 感知器</a></li>
<li><a class="reference internal" href="#passive-aggressive-algorithms">1.1.14. 被动攻击算法(Passive Aggressive Algorithms)</a></li>
<li><a class="reference internal" href="#outliers">1.1.15. 鲁棒回归:处理离群点（outliers）和模型错误</a><ul>
<li><a class="reference internal" href="#id37">1.1.15.1. 各种使用场景与相关概念</a></li>
<li><a class="reference internal" href="#ransac">1.1.15.2. RANSAC:随机抽样一致性算法</a><ul>
<li><a class="reference internal" href="#id38">1.1.15.2.1. 算法细节</a></li>
</ul>
</li>
<li><a class="reference internal" href="#theil-sen">1.1.15.3. Theil-Sen 估计器: 广义中值估计器</a><ul>
<li><a class="reference internal" href="#id39">1.1.15.3.1. 理论方面的思考</a></li>
</ul>
</li>
<li><a class="reference internal" href="#huber">1.1.15.4. Huber 回归</a></li>
<li><a class="reference internal" href="#id42">1.1.15.5. 注意</a></li>
</ul>
</li>
<li><a class="reference internal" href="#polynomial-regression">1.1.16. 多项式回归:用基函数展开线性模型</a></li>
</ul>
</li>
</ul>

    </div>
</div>

<input type="checkbox" id="nav-trigger" class="nav-trigger" checked />
<label for="nav-trigger"></label>




      <div class="content">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="generalized-linear-models">
<span id="linear-model"></span><h1>1.1. 广义线性模型(Generalized Linear Models)<a class="headerlink" href="#generalized-linear-models" title="Permalink to this headline">¶</a></h1>
<p>以下是一组用于回归的方法，其中目标值(target value)被希望(认为)是输入变量的线性组合(input
variables)。 用数学语言来描述是这样的： 假设 <span class="math notranslate nohighlight">\(\hat{y}\)</span> 是目标值的预测值，则有</p>
<div class="math notranslate nohighlight">
\[\hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p\]</div>
<p>在整个模块中, 我们把向量 <span class="math notranslate nohighlight">\(w = (w_1,..., w_p)\)</span> 记作 <code class="docutils literal notranslate"><span class="pre">coef_</span></code> (系数)，并把 <span class="math notranslate nohighlight">\(w_0\)</span> 记作 <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> (截距).</p>
<p>如果你想用正则化线性模型求解分类问题, 请参考 <a class="reference internal" href="#logistic-regression"><span class="std std-ref">Logistic 回归</span></a>。</p>
<div class="section" id="ordinary-least-squares">
<span id="id1"></span><h2>1.1.1. 普通最小二乘法<a class="headerlink" href="#ordinary-least-squares" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression" title="sklearn.linear_model.LinearRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearRegression</span></code></a> 拟合一个带有系数 <span class="math notranslate nohighlight">\(w = (w_1, ..., w_p)\)</span> 的线性模型使得数据集实际观测数据和预测数据（估计值）之间的残差平方和最小。
其数学表达式为:</p>
<div class="math notranslate nohighlight">
\[\min_{w} {|| X w - y||_2}^2\]</div>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_ols.html"><img alt="../images/sphx_glr_plot_ols_0011.png" src="../images/sphx_glr_plot_ols_0011.png" style="width: 320.0px; height: 240.0px;" /></a>
</div>
<p><a class="reference internal" href="generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression" title="sklearn.linear_model.LinearRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearRegression</span></code></a> 会调用 <code class="docutils literal notranslate"><span class="pre">fit</span></code> 方法来拟合数组 X， y，并且将线性模型的系数 <span class="math notranslate nohighlight">\(w\)</span> 存储在其成员变量 <code class="docutils literal notranslate"><span class="pre">coef_</span></code> 中:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">... </span>                                      
<span class="go">LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,</span>
<span class="go">                 normalize=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span>
<span class="go">array([0.5, 0.5])</span>
</pre></div>
</div>
<p>然而，对于普通最小二乘的系数估计问题，其依赖于模型各项的相互独立性。
当各项是相关的，且设计矩阵(design matrix) <span class="math notranslate nohighlight">\(X\)</span> 的各列近似线性相关，
那么，设计矩阵会趋向于奇异矩阵，这会导致最小二乘估计对于随机误差非常敏感，产生很大的方差。
例如，在没有实验设计的情况下收集到的数据，这种多重共线性（multicollinearity）的情况可能真的会出现。</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py"><span class="std std-ref">Linear Regression Example</span></a></li>
</ul>
</div>
<div class="section" id="id2">
<h3>1.1.1.1. 普通最小二乘法的复杂度<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>该方法使用 X 的奇异值分解来计算最小二乘解。如果 X 是一个 size 为 (n, p) 的矩阵，设 <span class="math notranslate nohighlight">\(n \geq p\)</span> ，则该方法的复杂度为 <span class="math notranslate nohighlight">\(O(n p^2)\)</span>.</p>
</div>
</div>
<div class="section" id="ridge-regression">
<span id="id3"></span><h2>1.1.2. 岭回归(Ridge Regression)<a class="headerlink" href="#ridge-regression" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-class docutils literal notranslate"><span class="pre">Ridge</span></code></a> regression 通过对系数的大小施加惩罚来解决 <a class="reference internal" href="#ordinary-least-squares"><span class="std std-ref">普通最小二乘法</span></a> 的一些问题。
岭系数最小化的是带惩罚项的残差平方和，数学形式如下</p>
<div class="math notranslate nohighlight">
\[\min_{w} {{|| X w - y||_2}^2 + \alpha {||w||_2}^2}\]</div>
<p>其中, <span class="math notranslate nohighlight">\(\alpha \geq 0\)</span> 是一个控制缩减量(amount of shrinkage)的复杂度参数:
<span class="math notranslate nohighlight">\(\alpha\)</span> 的值越大, 缩减量就越大，故而线性模型的系数对共线性(collinearity)就越鲁棒.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_ridge_path.html"><img alt="../images/sphx_glr_plot_ridge_path_0011.png" src="../images/sphx_glr_plot_ridge_path_0011.png" style="width: 320.0px; height: 240.0px;" /></a>
</div>
<p>与其他线性模型一样, <a class="reference internal" href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-class docutils literal notranslate"><span class="pre">Ridge</span></code></a> 会调用 <code class="docutils literal notranslate"><span class="pre">fit</span></code> 方法来拟合数组 X， y，并且将线性模型的系数 <span class="math notranslate nohighlight">\(w\)</span> 存储在其成员变量 <code class="docutils literal notranslate"><span class="pre">coef_</span></code> 中:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> 
<span class="go">Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,</span>
<span class="go">      normalize=False, random_state=None, solver=&#39;auto&#39;, tol=0.001)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span>
<span class="go">array([0.34545455, 0.34545455])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">intercept_</span> 
<span class="go">0.13636...</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_ridge_path.html#sphx-glr-auto-examples-linear-model-plot-ridge-path-py"><span class="std std-ref">Plot Ridge coefficients as a function of the regularization</span></a></li>
<li><a class="reference internal" href="../auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py"><span class="std std-ref">Classification of text documents using sparse features</span></a></li>
</ul>
</div>
<div class="section" id="id4">
<h3>1.1.2.1. 岭回归的复杂度<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>这种方法的复杂度与 <a class="reference internal" href="#ordinary-least-squares"><span class="std std-ref">普通最小二乘法</span></a> 的复杂度是相同的.</p>
</div>
<div class="section" id="id5">
<h3>1.1.2.2. 设置正则化参数: 广义交叉验证<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV" title="sklearn.linear_model.RidgeCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">RidgeCV</span></code></a> 通过内置的 alpha 参数的交叉验证来实现岭回归。 该对象与 GridSearchCV 的使用方法相同，
只是它默认为 Generalized Cross-Validation(广义交叉验证 GCV)，这是一种有效的留一验证方法（LOO-CV）:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">RidgeCV</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">],</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>       
<span class="go">RidgeCV(alphas=[0.1, 1.0, 10.0], cv=3, fit_intercept=True, scoring=None,</span>
<span class="go">    normalize=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">alpha_</span>                                      
<span class="go">0.1</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">参考文献</p>
<ul class="simple">
<li>“Notes on Regularized Least Squares”, Rifkin &amp; Lippert (<a class="reference external" href="http://cbcl.mit.edu/publications/ps/MIT-CSAIL-TR-2007-025.pdf">technical report</a>,
<a class="reference external" href="https://www.mit.edu/~9.520/spring07/Classes/rlsslides.pdf">course slides</a>).</li>
</ul>
</div>
</div>
</div>
<div class="section" id="lasso">
<span id="id6"></span><h2>1.1.3. Lasso<a class="headerlink" href="#lasso" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code class="xref py py-class docutils literal notranslate"><span class="pre">Lasso</span></code></a> 是一个线性模型，它给出的模型具有稀疏的系数(sparse coefficients)。
它在一些场景中是狠有用的，因为它倾向于使用具有较少参数值的情况，能够有效地减少给定解决方案所依赖变量的数量。
因此，Lasso 及其变体是压缩感知(compressed sensing)领域的基础。在某些特定条件下, 它能够恢复非零权重的精确解。
(请参见 <a class="reference internal" href="../auto_examples/applications/plot_tomography_l1_reconstruction.html#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py"><span class="std std-ref">Compressive sensing: tomography reconstruction with L1 prior (Lasso)</span></a>).</p>
<p>在数学公式表达上，它由一个带有 <span class="math notranslate nohighlight">\(\ell_1\)</span> 先验的正则项的线性模型组成。 其最小化的目标函数是:</p>
<div class="math notranslate nohighlight">
\[\min_{w} { \frac{1}{2n_{samples}} ||X w - y||_2 ^ 2 + \alpha ||w||_1}\]</div>
<p>lasso estimator 解决了加上惩罚项 <span class="math notranslate nohighlight">\(\alpha ||w||_1\)</span> 的最小二乘的最小化，其中，
<span class="math notranslate nohighlight">\(\alpha\)</span> 是一个常数，<span class="math notranslate nohighlight">\(||w||_1\)</span> 是参数向量的 <span class="math notranslate nohighlight">\(\ell_1\)</span>-norm 范数。</p>
<p><a class="reference internal" href="generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code class="xref py py-class docutils literal notranslate"><span class="pre">Lasso</span></code></a> 类的实现使用了 coordinate descent （坐标下降算法）来拟合系数。 另一种实现方法在 <a class="reference internal" href="#least-angle-regression"><span class="std std-ref">最小角回归</span></a> 中:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,</span>
<span class="go">   normalize=False, positive=False, precompute=False, random_state=None,</span>
<span class="go">   selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="go">array([0.8])</span>
</pre></div>
</div>
<p>对于较低级别的任务，同样有用的是函数 <a class="reference internal" href="generated/sklearn.linear_model.lasso_path.html#sklearn.linear_model.lasso_path" title="sklearn.linear_model.lasso_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">lasso_path</span></code></a> 。它能够通过搜索所有可能的路径上的值来计算系数。</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_lasso_and_elasticnet.html#sphx-glr-auto-examples-linear-model-plot-lasso-and-elasticnet-py"><span class="std std-ref">Lasso and Elastic Net for Sparse Signals</span></a></li>
<li><a class="reference internal" href="../auto_examples/applications/plot_tomography_l1_reconstruction.html#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py"><span class="std std-ref">Compressive sensing: tomography reconstruction with L1 prior (Lasso)</span></a></li>
</ul>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>使用 Lasso 进行特征选择</strong></p>
<p class="last">由于 Lasso regression 能够产生稀疏模型，所以他可以用来执行特征选择，详情可以参见 <a class="reference internal" href="feature_selection.html#l1-feature-selection"><span class="std std-ref">基于 L1 的特征选取</span></a>。</p>
</div>
<p>下面的两篇参考文章解释了scikit-learn的坐标下降求解器(coordinate descent solver)的迭代过程，以及用于控制收敛的对偶间隙(duality gap)的计算。</p>
<div class="topic">
<p class="topic-title first">参考文献</p>
<ul class="simple">
<li>“Regularization Path For Generalized linear Models by Coordinate Descent”,
Friedman, Hastie &amp; Tibshirani, J Stat Softw, 2010 (<a class="reference external" href="https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf">Paper</a>).</li>
<li>“An Interior-Point Method for Large-Scale L1-Regularized Least Squares,”
S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky,
in IEEE Journal of Selected Topics in Signal Processing, 2007
(<a class="reference external" href="https://web.stanford.edu/~boyd/papers/pdf/l1_ls.pdf">Paper</a>)</li>
</ul>
</div>
<div class="section" id="id8">
<h3>1.1.3.1. 设置正则化参数<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">alpha</span></code> 参数控制着估计出的模型的系数的稀疏度。</p>
<div class="section" id="id9">
<h4>1.1.3.1.1. 使用交叉验证<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h4>
<p>scikit-learn 通过交叉验证来公开设置 Lasso <code class="docutils literal notranslate"><span class="pre">alpha</span></code> 参数的对象: <a class="reference internal" href="generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV" title="sklearn.linear_model.LassoCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">LassoCV</span></code></a> 和 <a class="reference internal" href="generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">LassoLarsCV</span></code></a>。
<a class="reference internal" href="generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">LassoLarsCV</span></code></a> 是基于下面解释的 <a class="reference internal" href="#least-angle-regression"><span class="std std-ref">最小角回归</span></a> 算法。</p>
<p>对于带有很多共线回归器(collinear regressors)的高维数据集， <a class="reference internal" href="generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV" title="sklearn.linear_model.LassoCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">LassoCV</span></code></a> 是经常被选择的模型。
然而，<a class="reference internal" href="generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">LassoLarsCV</span></code></a> 在寻找更有相关性的 <cite>alpha</cite> 参数值上更具有优势，
而且如果样本数量与特征数量相比非常小时，通常 <a class="reference internal" href="generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">LassoLarsCV</span></code></a> 比 <a class="reference internal" href="generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV" title="sklearn.linear_model.LassoCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">LassoCV</span></code></a> 要快。</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/linear_model/plot_lasso_model_selection.html"><img alt="lasso_cv_1" src="../images/sphx_glr_plot_lasso_model_selection_0021.png" style="width: 307.2px; height: 230.39999999999998px;" /></a> <a class="reference external" href="../auto_examples/linear_model/plot_lasso_model_selection.html"><img alt="lasso_cv_2" src="../images/sphx_glr_plot_lasso_model_selection_0031.png" style="width: 307.2px; height: 230.39999999999998px;" /></a></strong></p></div>
<div class="section" id="id10">
<h4>1.1.3.1.2. 基于信息标准的模型选择<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h4>
<p>有多种选择时，估计器 <a class="reference internal" href="generated/sklearn.linear_model.LassoLarsIC.html#sklearn.linear_model.LassoLarsIC" title="sklearn.linear_model.LassoLarsIC"><code class="xref py py-class docutils literal notranslate"><span class="pre">LassoLarsIC</span></code></a> 建议使用 Akaike information criterion （Akaike 信息准则）（AIC）
和 Bayes Information criterion （贝叶斯信息准则）（BIC）。 当使用 k-fold 交叉验证时，正则化路径只计算一次而不是 k + 1 次，
所以找到 alpha 的最优值是一种计算上更便宜的替代方法。 然而，这样的标准需要对解决方案的自由度进行适当的估计，对于大样本（渐近结果）导出，
并假设模型是正确的，即数据实际上是由该模型生成的。 当问题严重受限（比样本更多的特征）时，他们也倾向于打破。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_lasso_model_selection.html"><img alt="../images/sphx_glr_plot_lasso_model_selection_0011.png" src="../images/sphx_glr_plot_lasso_model_selection_0011.png" style="width: 320.0px; height: 240.0px;" /></a>
</div>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_lasso_model_selection.html#sphx-glr-auto-examples-linear-model-plot-lasso-model-selection-py"><span class="std std-ref">Lasso model selection: Cross-Validation / AIC / BIC</span></a></li>
</ul>
</div>
</div>
<div class="section" id="svm">
<h4>1.1.3.1.3. 与 SVM 的正则化参数的比较<a class="headerlink" href="#svm" title="Permalink to this headline">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">alpha</span></code> 和 SVM 的正则化参数 <code class="docutils literal notranslate"><span class="pre">C</span></code> 之间的等式关系是 <code class="docutils literal notranslate"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">1</span> <span class="pre">/</span> <span class="pre">C</span></code> 或者 <code class="docutils literal notranslate"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">1</span> <span class="pre">/</span> <span class="pre">(n_samples</span> <span class="pre">*</span> <span class="pre">C)</span></code> ，
并依赖于估计器和模型优化的确切的目标函数。</p>
</div>
</div>
</div>
<div class="section" id="multi-task-lasso">
<span id="id11"></span><h2>1.1.4. 多任务 Lasso<a class="headerlink" href="#multi-task-lasso" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.linear_model.MultiTaskLasso.html#sklearn.linear_model.MultiTaskLasso" title="sklearn.linear_model.MultiTaskLasso"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiTaskLasso</span></code></a> 是一个估计多元回归稀疏系数的线性模型： <code class="docutils literal notranslate"><span class="pre">y</span></code> 是一个 <code class="docutils literal notranslate"><span class="pre">(n_samples,</span> <span class="pre">n_tasks)</span></code> 的2D array，
其约束条件和其他回归问题（也称为任务）是一样的，都是所选的特征值。</p>
<p>下图比较了通过使用简单的 Lasso 或 MultiTaskLasso 得到的 W 中非零的位置。
Lasso 估计产生分散的非零值，而 MultiTaskLasso 的一整列都是非零的。</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/linear_model/plot_multi_task_lasso_support.html"><img alt="multi_task_lasso_1" src="../images/sphx_glr_plot_multi_task_lasso_support_0011.png" style="width: 384.0px; height: 240.0px;" /></a> <a class="reference external" href="../auto_examples/linear_model/plot_multi_task_lasso_support.html"><img alt="multi_task_lasso_2" src="../images/sphx_glr_plot_multi_task_lasso_support_0021.png" style="width: 307.2px; height: 230.39999999999998px;" /></a></strong></p><p class="centered">
<strong>Fitting a time-series model, imposing that any active feature be active at all times.</strong></p><div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_multi_task_lasso_support.html#sphx-glr-auto-examples-linear-model-plot-multi-task-lasso-support-py"><span class="std std-ref">Joint feature selection with multi-task Lasso</span></a></li>
</ul>
</div>
<p>在数学上，它由一个线性模型组成，以混合的 <span class="math notranslate nohighlight">\(\ell_1\)</span> <span class="math notranslate nohighlight">\(\ell_2\)</span> 先验 作为正则化器进行训练。最小化的目标函数是：</p>
<div class="math notranslate nohighlight">
\[\min_{w} { \frac{1}{2n_{samples}} ||X W - Y||_{Fro} ^ 2 + \alpha ||W||_{21}}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(Fro\)</span> indicates the Frobenius norm:</p>
<div class="math notranslate nohighlight">
\[||A||_{Fro} = \sqrt{\sum_{ij} a_{ij}^2}\]</div>
<p>和 <span class="math notranslate nohighlight">\(\ell_1\)</span> <span class="math notranslate nohighlight">\(\ell_2\)</span> reads:</p>
<div class="math notranslate nohighlight">
\[||A||_{2 1} = \sum_i \sqrt{\sum_j a_{ij}^2}\]</div>
<p><a class="reference internal" href="generated/sklearn.linear_model.MultiTaskLasso.html#sklearn.linear_model.MultiTaskLasso" title="sklearn.linear_model.MultiTaskLasso"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiTaskLasso</span></code></a> 类的实现也使用了坐标下降法(coordinate descent)对系数进行拟合的。</p>
</div>
<div class="section" id="elastic-net">
<span id="id12"></span><h2>1.1.5. 弹性网(Elastic Net)<a class="headerlink" href="#elastic-net" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet" title="sklearn.linear_model.ElasticNet"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElasticNet</span></code></a> 是一种使用 L1 和 L2 范数作为先验正则项训练的线性回归模型。
这种正则化项的组合允许学习到一个只有少量参数是非零稀疏的模型，就像 <a class="reference internal" href="generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code class="xref py py-class docutils literal notranslate"><span class="pre">Lasso</span></code></a> 一样，但是它仍然保持一些像 <a class="reference internal" href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-class docutils literal notranslate"><span class="pre">Ridge</span></code></a> 的正则性质。
我们可以利用 <code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code> 参数控制 L1 和 L2 的凸组合。</p>
<p>弹性网络在很多特征互相联系的情况下是非常有用的。Lasso 很可能只随机考虑这些特征中的一个，而弹性网络更倾向于选择两个。</p>
<p>在实践中，Lasso 和 Ridge 之间权衡的一个优势是它允许Elastic-Net在循环过程（Under rotate）中继承 Ridge 的稳定性。</p>
<p>最小化的目标函数如下所示：</p>
<div class="math notranslate nohighlight">
\[\min_{w} { \frac{1}{2n_{samples}} ||X w - y||_2 ^ 2 + \alpha \rho ||w||_1 +
\frac{\alpha(1-\rho)}{2} ||w||_2 ^ 2}\]</div>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_lasso_coordinate_descent_path.html"><img alt="../images/sphx_glr_plot_lasso_coordinate_descent_path_0011.png" src="../images/sphx_glr_plot_lasso_coordinate_descent_path_0011.png" style="width: 320.0px; height: 240.0px;" /></a>
</div>
<p><a class="reference internal" href="generated/sklearn.linear_model.ElasticNetCV.html#sklearn.linear_model.ElasticNetCV" title="sklearn.linear_model.ElasticNetCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElasticNetCV</span></code></a> 类可以被用来通过交叉验证设置 <code class="docutils literal notranslate"><span class="pre">alpha</span></code> (<span class="math notranslate nohighlight">\(\alpha\)</span>) 和 <code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code> (<span class="math notranslate nohighlight">\(\rho\)</span>) 参数。</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_lasso_and_elasticnet.html#sphx-glr-auto-examples-linear-model-plot-lasso-and-elasticnet-py"><span class="std std-ref">Lasso and Elastic Net for Sparse Signals</span></a></li>
<li><a class="reference internal" href="../auto_examples/linear_model/plot_lasso_coordinate_descent_path.html#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py"><span class="std std-ref">Lasso and Elastic Net</span></a></li>
</ul>
</div>
<p>下面的两篇参考文章解释了scikit-learn 的坐标下降求解器(coordinate descent solver)的迭代过程，以及用于控制收敛的对偶间隙(duality gap)的计算。</p>
<div class="topic">
<p class="topic-title first">参考文献</p>
<ul class="simple">
<li>“Regularization Path For Generalized linear Models by Coordinate Descent”,
Friedman, Hastie &amp; Tibshirani, J Stat Softw, 2010 (<a class="reference external" href="https://www.jstatsoft.org/article/view/v033i01/v33i01.pdf">Paper</a>).</li>
<li>“An Interior-Point Method for Large-Scale L1-Regularized Least Squares,”
S. J. Kim, K. Koh, M. Lustig, S. Boyd and D. Gorinevsky,
in IEEE Journal of Selected Topics in Signal Processing, 2007
(<a class="reference external" href="https://web.stanford.edu/~boyd/papers/pdf/l1_ls.pdf">Paper</a>)</li>
</ul>
</div>
</div>
<div class="section" id="multi-task-elastic-net">
<span id="id15"></span><h2>1.1.6. 多任务弹性网<a class="headerlink" href="#multi-task-elastic-net" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.linear_model.MultiTaskElasticNet.html#sklearn.linear_model.MultiTaskElasticNet" title="sklearn.linear_model.MultiTaskElasticNet"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiTaskElasticNet</span></code></a> 是一个针对多变量回归问题估计其稀疏系数的弹性网模型: <code class="docutils literal notranslate"><span class="pre">Y</span></code> 是一个 2D array,
其shape为 <code class="docutils literal notranslate"><span class="pre">(n_samples,</span> <span class="pre">n_tasks)</span></code>。 其约束条件和其他回归问题（也称为任务）是一样的，都是所选的特征值。</p>
<p>数学上, 它是一个混合了 <span class="math notranslate nohighlight">\(\ell_1\)</span> <span class="math notranslate nohighlight">\(\ell_2\)</span> 先验 和 <span class="math notranslate nohighlight">\(\ell_2\)</span> 先验作为正则化项的线性模型。
目标函数的最小化如下所示：</p>
<div class="math notranslate nohighlight">
\[\min_{W} { \frac{1}{2n_{samples}} ||X W - Y||_{Fro}^2 + \alpha \rho ||W||_{2 1} +
\frac{\alpha(1-\rho)}{2} ||W||_{Fro}^2}\]</div>
<p><a class="reference internal" href="generated/sklearn.linear_model.MultiTaskElasticNet.html#sklearn.linear_model.MultiTaskElasticNet" title="sklearn.linear_model.MultiTaskElasticNet"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiTaskElasticNet</span></code></a> 类的实现也使用了坐标下降法(coordinate descent)对系数进行拟合的。</p>
<p><a class="reference internal" href="generated/sklearn.linear_model.MultiTaskElasticNetCV.html#sklearn.linear_model.MultiTaskElasticNetCV" title="sklearn.linear_model.MultiTaskElasticNetCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultiTaskElasticNetCV</span></code></a> 类可以被用来通过交叉验证设置 <code class="docutils literal notranslate"><span class="pre">alpha</span></code> (<span class="math notranslate nohighlight">\(\alpha\)</span>) 和 <code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code> (<span class="math notranslate nohighlight">\(\rho\)</span>) 参数。</p>
</div>
<div class="section" id="least-angle-regression">
<span id="id16"></span><h2>1.1.7. 最小角回归<a class="headerlink" href="#least-angle-regression" title="Permalink to this headline">¶</a></h2>
<p>最小角回归 （Least-angle regression – LARS） 是对高维数据的回归算法， 由 Bradley Efron,
Trevor Hastie, Iain Johnstone 和 Robert Tibshirani 开发完成。 LARS 和前向逐步回归(forward stepwise
regression)很像。在每一步，它寻找与响应最有关联的 预测。当有很多预测有相同的关联时，它没有继续利用相同的预测，
而是在这些预测中找出应该等角的方向。</p>
<p>LARS 的优点如下 :</p>
<blockquote>
<div><ul class="simple">
<li>当 p &gt;&gt; n，该算法数值运算上非常有效。(例如当维度的数目远超点的个数)</li>
<li>它在计算上和前向选择一样快，和普通最小二乘法有相同的运算复杂度。</li>
<li>它产生了一个完整的分段线性的解决路径，在交叉验证或者其他相似的微调模型的方法上非常有用。</li>
<li>如果两个变量对响应几乎有相等的联系，则它们的系数应该有相似的增长率。因此这个算法和我们直觉上的判断一样，而且还更加稳定。</li>
<li>它很容易修改并为其他估算器生成解，比如Lasso。</li>
</ul>
</div></blockquote>
<p>LARS 的缺点如下 :</p>
<blockquote>
<div><ul class="simple">
<li>因为 LARS 是建立在循环拟合剩余变量上的，所以它对噪声非常敏感。这个问题，在 2004 年统计年鉴的文章由 Weisberg 详细讨论。</li>
</ul>
</div></blockquote>
<p>LARS 模型可以在 <a class="reference internal" href="generated/sklearn.linear_model.Lars.html#sklearn.linear_model.Lars" title="sklearn.linear_model.Lars"><code class="xref py py-class docutils literal notranslate"><span class="pre">Lars</span></code></a> ，或者它的底层实现 <a class="reference internal" href="generated/sklearn.linear_model.lars_path.html#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">lars_path</span></code></a> 中被使用。</p>
</div>
<div class="section" id="lars-lasso">
<h2>1.1.8. LARS Lasso<a class="headerlink" href="#lars-lasso" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.linear_model.LassoLars.html#sklearn.linear_model.LassoLars" title="sklearn.linear_model.LassoLars"><code class="xref py py-class docutils literal notranslate"><span class="pre">LassoLars</span></code></a> 是一个使用 LARS 算法的 lasso 模型，不同于基于坐标下降法的实现，
它可以得到一个精确解，也就是一个关于自身参数标准化后的一个分段线性解。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_lasso_lars.html"><img alt="../images/sphx_glr_plot_lasso_lars_0011.png" src="../images/sphx_glr_plot_lasso_lars_0011.png" style="width: 320.0px; height: 240.0px;" /></a>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LassoLars</span><span class="p">(</span><span class="n">alpha</span><span class="o">=.</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  
<span class="go">LassoLars(alpha=0.1, copy_X=True, eps=..., fit_intercept=True,</span>
<span class="go">     fit_path=True, max_iter=500, normalize=True, positive=False,</span>
<span class="go">     precompute=&#39;auto&#39;, verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span>    
<span class="go">array([0.717157..., 0.        ])</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_lasso_lars.html#sphx-glr-auto-examples-linear-model-plot-lasso-lars-py"><span class="std std-ref">Lasso path using LARS</span></a></li>
</ul>
</div>
<p>Lars 算法提供了一个几乎无代价的沿着正则化参数的系数的完整路径，因此常利用函数 <a class="reference internal" href="generated/sklearn.linear_model.lars_path.html#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">lars_path</span></code></a> 来取回路径。</p>
<div class="section" id="id17">
<h3>1.1.8.1. 数学化表达式<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h3>
<p>该算法和前向逐步回归(forward stepwise regression)非常相似，但是它没有在每一步包含变量，
它估计的参数是根据与其他剩余变量的联系来增加的。</p>
<p>在 LARS 的解中，没有给出一个向量的结果，而是给出一条曲线，显示参数向量的 L1 范式的每个值的解。
完全的参数路径存在 <code class="docutils literal notranslate"><span class="pre">coef_path_</span></code> 下。它的 size 是 (n_features, max_features+1)。
其中第一列通常是全 0 列。</p>
<div class="topic">
<p class="topic-title first">参考:</p>
<ul class="simple">
<li>Original Algorithm is detailed in the paper <a class="reference external" href="http://www-stat.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf">Least Angle Regression</a>
by Hastie et al.</li>
</ul>
</div>
</div>
</div>
<div class="section" id="omp">
<span id="id19"></span><h2>1.1.9. 正交匹配追踪法 (OMP)<a class="headerlink" href="#omp" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.linear_model.OrthogonalMatchingPursuit.html#sklearn.linear_model.OrthogonalMatchingPursuit" title="sklearn.linear_model.OrthogonalMatchingPursuit"><code class="xref py py-class docutils literal notranslate"><span class="pre">OrthogonalMatchingPursuit</span></code></a> and <a class="reference internal" href="generated/sklearn.linear_model.orthogonal_mp.html#sklearn.linear_model.orthogonal_mp" title="sklearn.linear_model.orthogonal_mp"><code class="xref py py-func docutils literal notranslate"><span class="pre">orthogonal_mp</span></code></a> implements the OMP
algorithm for approximating the fit of a linear model with constraints imposed
on the number of non-zero coefficients (ie. the L <sub>0</sub> pseudo-norm).</p>
<p>Being a forward feature selection method like <a class="reference internal" href="#least-angle-regression"><span class="std std-ref">最小角回归</span></a>,
orthogonal matching pursuit can approximate the optimum solution vector with a
fixed number of non-zero elements:</p>
<div class="math notranslate nohighlight">
\[\underset{\gamma}{\operatorname{arg\,min\,}}  ||y - X\gamma||_2^2 \text{ subject to } ||\gamma||_0 \leq n_{nonzero\_coefs}\]</div>
<p>Alternatively, orthogonal matching pursuit can target a specific error instead
of a specific number of non-zero coefficients. This can be expressed as:</p>
<div class="math notranslate nohighlight">
\[\underset{\gamma}{\operatorname{arg\,min\,}} ||\gamma||_0 \text{ subject to } ||y-X\gamma||_2^2 \leq \text{tol}\]</div>
<p>OMP is based on a greedy algorithm that includes at each step the atom most
highly correlated with the current residual. It is similar to the simpler
matching pursuit (MP) method, but better in that at each iteration, the
residual is recomputed using an orthogonal projection on the space of the
previously chosen dictionary elements.</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_omp.html#sphx-glr-auto-examples-linear-model-plot-omp-py"><span class="std std-ref">Orthogonal Matching Pursuit</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf">http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf</a></li>
<li><a class="reference external" href="http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf">Matching pursuits with time-frequency dictionaries</a>,
S. G. Mallat, Z. Zhang,</li>
</ul>
</div>
</div>
<div class="section" id="bayesian-regression">
<span id="id20"></span><h2>1.1.10. 贝叶斯回归<a class="headerlink" href="#bayesian-regression" title="Permalink to this headline">¶</a></h2>
<p>贝叶斯回归可以用于在估计阶段的参数正则化: 正则化参数的选择不是通过人为选择进行设置，而是根据手头的数据来调节的。</p>
<p>上述过程可以通过引入 无信息先验(<a class="reference external" href="https://en.wikipedia.org/wiki/Non-informative_prior#Uninformative_priors">uninformative priors</a>)
于模型中的超参数来完成。 在 <a href="#id46"><span class="problematic" id="id47">`Ridge Regression`_</span></a>  中使用的 <span class="math notranslate nohighlight">\(\ell_{2}\)</span> 正则项相当于在 <span class="math notranslate nohighlight">\(w\)</span> 为高斯先验条件下，
且此先验的精确度为 <span class="math notranslate nohighlight">\(\lambda^{-1}\)</span> 求最大后验估计。在这里，我们没有手工调参数 <cite>lambda</cite> ，而是让他作为一个变量，通过数据中估计得到。</p>
<p>为了得到一个全概率模型，输出 <span class="math notranslate nohighlight">\(y\)</span> 也被认为是关于 <span class="math notranslate nohighlight">\(X w\)</span> 的高斯分布:</p>
<div class="math notranslate nohighlight">
\[p(y|X,w,\alpha) = \mathcal{N}(y|X w,\alpha)\]</div>
<p>Alpha 在这里也是作为一个变量，通过数据中估计得到。</p>
<p>Bayesian Regression 的优点是:</p>
<blockquote>
<div><ul class="simple">
<li>它能根据已有的数据进行改变。</li>
<li>它能在估计过程中引入正则项的参数。</li>
</ul>
</div></blockquote>
<p>Bayesian Regression 的缺点是:</p>
<blockquote>
<div><ul class="simple">
<li>它的推断过程是非常耗时的。</li>
</ul>
</div></blockquote>
<div class="topic">
<p class="topic-title first">参考文献</p>
<ul class="simple">
<li>A good introduction to Bayesian methods is given in C. Bishop: Pattern
Recognition and Machine learning</li>
<li>Original Algorithm is detailed in the  book Bayesian learning for neural
networks by Radford M. Neal</li>
</ul>
</div>
<div class="section" id="bayesian-ridge-regression">
<span id="id21"></span><h3>1.1.10.1. 贝叶斯岭回归<a class="headerlink" href="#bayesian-ridge-regression" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.linear_model.BayesianRidge.html#sklearn.linear_model.BayesianRidge" title="sklearn.linear_model.BayesianRidge"><code class="xref py py-class docutils literal notranslate"><span class="pre">BayesianRidge</span></code></a> 类会估计一个求解回归问题的概率模型(probabilistic model)。
参数 <span class="math notranslate nohighlight">\(w\)</span> 的先验值 通过 球面高斯(spherical Gaussian)给出:</p>
<div class="math notranslate nohighlight">
\[p(w|\lambda) =
\mathcal{N}(w|0,\lambda^{-1}\mathbf{I}_{p})\]</div>
<p><span class="math notranslate nohighlight">\(\alpha\)</span> 和 <span class="math notranslate nohighlight">\(\lambda\)</span> 的先验分布选择为 <a class="reference external" href="https://en.wikipedia.org/wiki/Gamma_distribution">gamma distributions</a>,
这个分布与高斯成共轭先验关系。</p>
<p>得到的模型一般称为 贝叶斯岭回归(<em>Bayesian Ridge Regression</em>)， 并且这个与传统的 <a class="reference internal" href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-class docutils literal notranslate"><span class="pre">Ridge</span></code></a> 非常相似。
参数 <span class="math notranslate nohighlight">\(w\)</span>, <span class="math notranslate nohighlight">\(\alpha\)</span> 和 <span class="math notranslate nohighlight">\(\lambda\)</span> 是在模型拟合的时候一起被估算出来的。
剩下的超参数就是 关于 <span class="math notranslate nohighlight">\(\alpha\)</span> 和 <span class="math notranslate nohighlight">\(\lambda\)</span> 的 gamma 分布的先验了。
它们通常被选择为 无信息先验(<em>non-informative</em>) 。模型参数的估计一般利用最大化 边缘对数似然估计(<em>marginal
log likelihood</em>)。</p>
<p>默认情况下 <span class="math notranslate nohighlight">\(\alpha_1 = \alpha_2 =  \lambda_1 = \lambda_2 = 10^{-6}\)</span>.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_bayesian_ridge.html"><img alt="../images/sphx_glr_plot_bayesian_ridge_0011.png" src="../images/sphx_glr_plot_bayesian_ridge_0011.png" style="width: 300.0px; height: 250.0px;" /></a>
</div>
<p>Bayesian Ridge Regression 用于回归问题</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">BayesianRidge</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="go">BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,</span>
<span class="go">       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,</span>
<span class="go">       normalize=False, tol=0.001, verbose=False)</span>
</pre></div>
</div>
<p>当模型拟合好以后, 就可以用来预测新的观测值啦:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]])</span>
<span class="go">array([0.50000013])</span>
</pre></div>
</div>
<p>模型的权重 <span class="math notranslate nohighlight">\(w\)</span> 可以这样获得</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span>
<span class="go">array([0.49999993, 0.49999993])</span>
</pre></div>
</div>
<p>由于贝叶斯框架的缘故，权值与 <a class="reference internal" href="#ordinary-least-squares"><span class="std std-ref">普通最小二乘法</span></a> 产生的不太一样。
但是，贝叶斯岭回归对病态问题（ill-posed）的鲁棒性要更好。</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_bayesian_ridge.html#sphx-glr-auto-examples-linear-model-plot-bayesian-ridge-py"><span class="std std-ref">Bayesian Ridge Regression</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献</p>
<ul class="simple">
<li>More details can be found in the article <a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.9072&amp;rep=rep1&amp;type=pdf">Bayesian Interpolation</a>
by MacKay, David J. C.</li>
</ul>
</div>
</div>
<div class="section" id="ard">
<h3>1.1.10.2. 主动相关决策理论 - ARD<a class="headerlink" href="#ard" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.linear_model.ARDRegression.html#sklearn.linear_model.ARDRegression" title="sklearn.linear_model.ARDRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">ARDRegression</span></code></a> is very similar to <a href="#id48"><span class="problematic" id="id49">`Bayesian Ridge Regression`_</span></a>,
but can lead to sparser weights <span class="math notranslate nohighlight">\(w\)</span> <a class="footnote-reference" href="#id26" id="id22">[1]</a> <a class="footnote-reference" href="#id27" id="id23">[2]</a>.
<a class="reference internal" href="generated/sklearn.linear_model.ARDRegression.html#sklearn.linear_model.ARDRegression" title="sklearn.linear_model.ARDRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">ARDRegression</span></code></a> poses a different prior over <span class="math notranslate nohighlight">\(w\)</span>, by dropping the
assumption of the Gaussian being spherical.</p>
<p>Instead, the distribution over <span class="math notranslate nohighlight">\(w\)</span> is assumed to be an axis-parallel,
elliptical Gaussian distribution.</p>
<p>This means each weight <span class="math notranslate nohighlight">\(w_{i}\)</span> is drawn from a Gaussian distribution,
centered on zero and with a precision <span class="math notranslate nohighlight">\(\lambda_{i}\)</span>:</p>
<div class="math notranslate nohighlight">
\[p(w|\lambda) = \mathcal{N}(w|0,A^{-1})\]</div>
<p>with <span class="math notranslate nohighlight">\(diag \; (A) = \lambda = \{\lambda_{1},...,\lambda_{p}\}\)</span>.</p>
<p>In contrast to <a href="#id50"><span class="problematic" id="id51">`Bayesian Ridge Regression`_</span></a>, each coordinate of <span class="math notranslate nohighlight">\(w_{i}\)</span>
has its own standard deviation <span class="math notranslate nohighlight">\(\lambda_i\)</span>. The prior over all
<span class="math notranslate nohighlight">\(\lambda_i\)</span> is chosen to be the same gamma distribution given by
hyperparameters <span class="math notranslate nohighlight">\(\lambda_1\)</span> and <span class="math notranslate nohighlight">\(\lambda_2\)</span>.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_ard.html"><img alt="../images/sphx_glr_plot_ard_0011.png" src="../images/sphx_glr_plot_ard_0011.png" style="width: 300.0px; height: 250.0px;" /></a>
</div>
<p>ARD is also known in the literature as <em>Sparse Bayesian Learning</em> and
<em>Relevance Vector Machine</em> <a class="footnote-reference" href="#id28" id="id24">[3]</a> <a class="footnote-reference" href="#id29" id="id25">[4]</a>.</p>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_ard.html#sphx-glr-auto-examples-linear-model-plot-ard-py"><span class="std std-ref">Automatic Relevance Determination Regression (ARD)</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">References:</p>
<table class="docutils footnote" frame="void" id="id26" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id22">[1]</a></td><td>Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 7.2.1</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id27" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id23">[2]</a></td><td>David Wipf and Srikantan Nagarajan: <a class="reference external" href="http://papers.nips.cc/paper/3372-a-new-view-of-automatic-relevance-determination.pdf">A new view of automatic relevance determination</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id28" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id24">[3]</a></td><td>Michael E. Tipping: <a class="reference external" href="http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf">Sparse Bayesian Learning and the Relevance Vector Machine</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id29" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id25">[4]</a></td><td>Tristan Fletcher: <a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.651.8603&amp;rep=rep1&amp;type=pdf">Relevance Vector Machines explained</a></td></tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="section" id="logistic">
<span id="logistic-regression"></span><h2>1.1.11. Logistic 回归<a class="headerlink" href="#logistic" title="Permalink to this headline">¶</a></h2>
<p>Logistic 回归，虽然名字里有 “回归” 二字，但实际上是解决分类问题的一类线性模型。
在某些文献中，logistic 回归又被称作 logit 回归，maximum-entropy classification（MaxEnt，最大熵分类），
或 log-linear classifier（对数线性分类器）。
该模型利用函数 <a class="reference external" href="https://en.wikipedia.org/wiki/Logistic_function">logistic function</a> 将单次试验（single trial）的可能结果输出为概率。</p>
<p>scikit-learn 中 logistic regression 在 <a class="reference internal" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a> 类中实现了二分类（binary）、
一对多分类（one-vs-rest）及多项式 logistic 回归，并带有可选的 L1 和 L2 正则化。</p>
<p>作为一个优化问题，带 L2 惩罚项的二分类 logistic 回归要最小化以下代价函数（cost function）：</p>
<div class="math notranslate nohighlight">
\[\min_{w, c} \frac{1}{2}w^T w + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1) .\]</div>
<p>类似的, 带有 L1 正则化的 logistic regression 求解下面的问题：</p>
<div class="math notranslate nohighlight">
\[\min_{w, c} \|w\|_1 + C \sum_{i=1}^n \log(\exp(- y_i (X_i^T w + c)) + 1).\]</div>
<p>注意, 在这个记法中, 假定了观测 <span class="math notranslate nohighlight">\(y_i\)</span> 在集合 <span class="math notranslate nohighlight">\({-1, 1}\)</span> 中获取值(takes values in the set
<span class="math notranslate nohighlight">\({-1, 1}\)</span> at trial <span class="math notranslate nohighlight">\(i\)</span>)。</p>
<p>在 <a class="reference internal" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a> 类中实现了这些优化算法: “liblinear”, “newton-cg”, “lbfgs”, “sag” 和 “saga”。</p>
<p>The solver “liblinear” uses a coordinate descent (CD) algorithm, and relies
on the excellent C++ <a class="reference external" href="https://www.csie.ntu.edu.tw/~cjlin/liblinear/">LIBLINEAR library</a>, which is shipped with
scikit-learn. However, the CD algorithm implemented in liblinear cannot learn
a true multinomial (multiclass) model; instead, the optimization problem is
decomposed in a “one-vs-rest” fashion so separate binary classifiers are
trained for all classes. This happens under the hood, so
<a class="reference internal" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a> instances using this solver behave as multiclass
classifiers. For L1 penalization <a class="reference internal" href="generated/sklearn.svm.l1_min_c.html#sklearn.svm.l1_min_c" title="sklearn.svm.l1_min_c"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.svm.l1_min_c</span></code></a> allows to
calculate the lower bound for C in order to get a non “null” (all feature
weights to zero) model.</p>
<p>The “lbfgs”, “sag” and “newton-cg” solvers only support L2 penalization and
are found to converge faster for some high dimensional data. Setting
<cite>multi_class</cite> to “multinomial” with these solvers learns a true multinomial
logistic regression model <a class="footnote-reference" href="#id33" id="id30">[5]</a>, which means that its probability estimates
should be better calibrated than the default “one-vs-rest” setting.</p>
<p>The “sag” solver uses a Stochastic Average Gradient descent <a class="footnote-reference" href="#id34" id="id31">[6]</a>. It is faster
than other solvers for large datasets, when both the number of samples and the
number of features are large.</p>
<p>The “saga” solver <a class="footnote-reference" href="#id35" id="id32">[7]</a> is a variant of “sag” that also supports the
non-smooth <cite>penalty=”l1”</cite> option. This is therefore the solver of choice
for sparse multinomial logistic regression.</p>
<p>In a nutshell, the following table summarizes the penalties supported by each solver:</p>
<table border="1" class="docutils">
<colgroup>
<col width="30%" />
<col width="17%" />
<col width="13%" />
<col width="17%" />
<col width="11%" />
<col width="12%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>&#160;</td>
<td colspan="5"><strong>Solvers</strong></td>
</tr>
<tr class="row-even"><td><strong>Penalties</strong></td>
<td><strong>‘liblinear’</strong></td>
<td><strong>‘lbfgs’</strong></td>
<td><strong>‘newton-cg’</strong></td>
<td><strong>‘sag’</strong></td>
<td><strong>‘saga’</strong></td>
</tr>
<tr class="row-odd"><td>Multinomial + L2 penalty</td>
<td>no</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
</tr>
<tr class="row-even"><td>OVR + L2 penalty</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
</tr>
<tr class="row-odd"><td>Multinomial + L1 penalty</td>
<td>no</td>
<td>no</td>
<td>no</td>
<td>no</td>
<td>yes</td>
</tr>
<tr class="row-even"><td>OVR + L1 penalty</td>
<td>yes</td>
<td>no</td>
<td>no</td>
<td>no</td>
<td>yes</td>
</tr>
<tr class="row-odd"><td><strong>Behaviors</strong></td>
<td colspan="5">&#160;</td>
</tr>
<tr class="row-even"><td>Penalize the intercept (bad)</td>
<td>yes</td>
<td>no</td>
<td>no</td>
<td>no</td>
<td>no</td>
</tr>
<tr class="row-odd"><td>Faster for large datasets</td>
<td>no</td>
<td>no</td>
<td>no</td>
<td>yes</td>
<td>yes</td>
</tr>
<tr class="row-even"><td>Robust to unscaled datasets</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
<td>no</td>
<td>no</td>
</tr>
</tbody>
</table>
<p>The “saga” solver is often the best choice but requires scaling. The “liblinear” solver is
used by default for historical reasons.</p>
<p>对于大数据集，还可以用 <a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a> ，并使用对数损失（’log’ loss）。</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_logistic_l1_l2_sparsity.html#sphx-glr-auto-examples-linear-model-plot-logistic-l1-l2-sparsity-py"><span class="std std-ref">L1 Penalty and Sparsity in Logistic Regression</span></a></li>
<li><a class="reference internal" href="../auto_examples/linear_model/plot_logistic_path.html#sphx-glr-auto-examples-linear-model-plot-logistic-path-py"><span class="std std-ref">Regularization path of L1- Logistic Regression</span></a></li>
<li><a class="reference internal" href="../auto_examples/linear_model/plot_logistic_multinomial.html#sphx-glr-auto-examples-linear-model-plot-logistic-multinomial-py"><span class="std std-ref">Plot multinomial and One-vs-Rest Logistic Regression</span></a></li>
<li><a class="reference internal" href="../auto_examples/linear_model/plot_sparse_logistic_regression_20newsgroups.html#sphx-glr-auto-examples-linear-model-plot-sparse-logistic-regression-20newsgroups-py"><span class="std std-ref">Multiclass sparse logisitic regression on newgroups20</span></a></li>
<li><a class="reference internal" href="../auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html#sphx-glr-auto-examples-linear-model-plot-sparse-logistic-regression-mnist-py"><span class="std std-ref">MNIST classfification using multinomial logistic + L1</span></a></li>
</ul>
</div>
<div class="topic" id="liblinear-differences">
<p class="topic-title first">与 liblinear 的不同:</p>
<p>There might be a difference in the scores obtained between
<a class="reference internal" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a> with <code class="docutils literal notranslate"><span class="pre">solver=liblinear</span></code>
or <code class="xref py py-class docutils literal notranslate"><span class="pre">LinearSVC</span></code> and the external liblinear library directly,
when <code class="docutils literal notranslate"><span class="pre">fit_intercept=False</span></code> and the fit <code class="docutils literal notranslate"><span class="pre">coef_</span></code> (or) the data to
be predicted are zeroes. This is because for the sample(s) with
<code class="docutils literal notranslate"><span class="pre">decision_function</span></code> zero, <a class="reference internal" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a> and <code class="xref py py-class docutils literal notranslate"><span class="pre">LinearSVC</span></code>
predict the negative class, while liblinear predicts the positive class.
Note that a model with <code class="docutils literal notranslate"><span class="pre">fit_intercept=False</span></code> and having many samples with
<code class="docutils literal notranslate"><span class="pre">decision_function</span></code> zero, is likely to be a underfit, bad model and you are
advised to set <code class="docutils literal notranslate"><span class="pre">fit_intercept=True</span></code> and increase the intercept_scaling.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>利用稀疏 logistic 回归进行特征选择</strong></p>
<p class="last">带 L1 罚项的 logistic 回归 将得到稀疏模型（sparse model），
相当于进行了特征选择（feature selection），详情参见 <a class="reference internal" href="feature_selection.html#l1-feature-selection"><span class="std std-ref">基于 L1 的特征选取</span></a> 。
.</p>
</div>
<p><a class="reference internal" href="generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV" title="sklearn.linear_model.LogisticRegressionCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogisticRegressionCV</span></code></a> implements Logistic Regression with
builtin cross-validation to find out the optimal C parameter.
“newton-cg”, “sag”, “saga” and “lbfgs” solvers are found to be faster
for high-dimensional dense data, due to warm-starting. For the
multiclass case, if <cite>multi_class</cite> option is set to “ovr”, an optimal C
is obtained for each class and if the <cite>multi_class</cite> option is set to
“multinomial”, an optimal C is obtained by minimizing the cross-entropy
loss.</p>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<table class="docutils footnote" frame="void" id="id33" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id30">[5]</a></td><td>Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 4.3.4</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id34" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id31">[6]</a></td><td>Mark Schmidt, Nicolas Le Roux, and Francis Bach: <a class="reference external" href="https://hal.inria.fr/hal-00860051/document">Minimizing Finite Sums with the Stochastic Average Gradient.</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id35" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id32">[7]</a></td><td>Aaron Defazio, Francis Bach, Simon Lacoste-Julien: <a class="reference external" href="https://arxiv.org/abs/1407.0202">SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives.</a></td></tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="sgd">
<h2>1.1.12. 随机梯度下降 - SGD<a class="headerlink" href="#sgd" title="Permalink to this headline">¶</a></h2>
<p>随机梯度下降(Stochastic gradient descent)是拟合线性模型的一个简单而高效的方法。在样本量（和特征数）很大时尤为有用。
方法 <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code> 可用于 online learning （在线学习）或基于 out-of-core learning （外存的学习）。</p>
<p><a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a> 和 <a class="reference internal" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDRegressor</span></code></a> 分别用于拟合分类问题和回归问题的线性模型，可使用不同的（凸）损失函数，支持不同的罚项。
例如，设定 loss=”log” ，则 <a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a> 拟合一个逻辑斯蒂回归模型，而 loss=”hinge” 拟合线性支持向量机（SVM）。</p>
<div class="topic">
<p class="topic-title first">参考</p>
<ul class="simple">
<li><a class="reference internal" href="sgd.html#sgd"><span class="std std-ref">随机梯度下降（Stochastic Gradient Descent）</span></a></li>
</ul>
</div>
</div>
<div class="section" id="perceptron">
<span id="id36"></span><h2>1.1.13. 感知器<a class="headerlink" href="#perceptron" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron" title="sklearn.linear_model.Perceptron"><code class="xref py py-class docutils literal notranslate"><span class="pre">Perceptron</span></code></a> 是适用于大规模学习的一种简单算法。默认情况下：</p>
<blockquote>
<div><ul class="simple">
<li>不需要设置学习率（learning rate）。</li>
<li>不需要正则化处理。</li>
<li>仅使用错误样本更新模型。</li>
</ul>
</div></blockquote>
<p>最后一点表明使用合页损失（hinge loss）的感知机比 使用 hinge loss 的SGD 略快，所得模型更稀疏。</p>
</div>
<div class="section" id="passive-aggressive-algorithms">
<span id="passive-aggressive"></span><h2>1.1.14. 被动攻击算法(Passive Aggressive Algorithms)<a class="headerlink" href="#passive-aggressive-algorithms" title="Permalink to this headline">¶</a></h2>
<p>The passive-aggressive algorithms are a family of algorithms for large-scale
learning. They are similar to the Perceptron in that they do not require a
learning rate. However, contrary to the Perceptron, they include a
regularization parameter <code class="docutils literal notranslate"><span class="pre">C</span></code>.</p>
<p>For classification, <a class="reference internal" href="generated/sklearn.linear_model.PassiveAggressiveClassifier.html#sklearn.linear_model.PassiveAggressiveClassifier" title="sklearn.linear_model.PassiveAggressiveClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">PassiveAggressiveClassifier</span></code></a> can be used with
<code class="docutils literal notranslate"><span class="pre">loss='hinge'</span></code> (PA-I) or <code class="docutils literal notranslate"><span class="pre">loss='squared_hinge'</span></code> (PA-II).  For regression,
<a class="reference internal" href="generated/sklearn.linear_model.PassiveAggressiveRegressor.html#sklearn.linear_model.PassiveAggressiveRegressor" title="sklearn.linear_model.PassiveAggressiveRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">PassiveAggressiveRegressor</span></code></a> can be used with
<code class="docutils literal notranslate"><span class="pre">loss='epsilon_insensitive'</span></code> (PA-I) or
<code class="docutils literal notranslate"><span class="pre">loss='squared_epsilon_insensitive'</span></code> (PA-II).</p>
<div class="topic">
<p class="topic-title first">References:</p>
<ul class="simple">
<li><a class="reference external" href="http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf">“Online Passive-Aggressive Algorithms”</a>
K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR 7 (2006)</li>
</ul>
</div>
</div>
<div class="section" id="outliers">
<h2>1.1.15. 鲁棒回归:处理离群点（outliers）和模型错误<a class="headerlink" href="#outliers" title="Permalink to this headline">¶</a></h2>
<p>Robust regression 特别适用于回归模型包含损坏数据（corrupt data）的情况，如离群点或模型中的错误。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_theilsen.html"><img alt="../images/sphx_glr_plot_theilsen_0011.png" src="../images/sphx_glr_plot_theilsen_0011.png" style="width: 320.0px; height: 240.0px;" /></a>
</div>
<div class="section" id="id37">
<h3>1.1.15.1. 各种使用场景与相关概念<a class="headerlink" href="#id37" title="Permalink to this headline">¶</a></h3>
<p>处理包含离群点的数据时牢记以下几点:</p>
<ul>
<li><p class="first"><strong>Outliers in X or in y</strong>?</p>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Outliers in the y direction</th>
<th class="head">Outliers in the X direction</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><a class="reference external" href="../auto_examples/linear_model/plot_robust_fit.html"><img alt="y_outliers" src="../images/sphx_glr_plot_robust_fit_0031.png" style="width: 300.0px; height: 240.0px;" /></a></td>
<td><a class="reference external" href="../auto_examples/linear_model/plot_robust_fit.html"><img alt="X_outliers" src="../images/sphx_glr_plot_robust_fit_0021.png" style="width: 300.0px; height: 240.0px;" /></a></td>
</tr>
</tbody>
</table>
</li>
<li><p class="first"><strong>Fraction of outliers versus amplitude of error</strong></p>
<p>The number of outlying points matters, but also how much they are
outliers.</p>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Small outliers</th>
<th class="head">Large outliers</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><a class="reference external" href="../auto_examples/linear_model/plot_robust_fit.html"><img alt="y_outliers" src="../images/sphx_glr_plot_robust_fit_0031.png" style="width: 300.0px; height: 240.0px;" /></a></td>
<td><a class="reference external" href="../auto_examples/linear_model/plot_robust_fit.html"><img alt="large_y_outliers" src="../images/sphx_glr_plot_robust_fit_0051.png" style="width: 300.0px; height: 240.0px;" /></a></td>
</tr>
</tbody>
</table>
</li>
</ul>
<p>An important notion of robust fitting is that of breakdown point: the
fraction of data that can be outlying for the fit to start missing the
inlying data.</p>
<p>Note that in general, robust fitting in high-dimensional setting (large
<cite>n_features</cite>) is very hard. The robust models here will probably not work
in these settings.</p>
<div class="topic">
<p class="topic-title first"><strong>Trade-offs: which estimator?</strong></p>
<blockquote>
<div><p>Scikit-learn provides 3 robust regression estimators:
<a class="reference internal" href="#ransac-regression"><span class="std std-ref">RANSAC</span></a>,
<a class="reference internal" href="#theil-sen-regression"><span class="std std-ref">Theil Sen</span></a> and
<a class="reference internal" href="#huber-regression"><span class="std std-ref">HuberRegressor</span></a></p>
<ul class="simple">
<li><a class="reference internal" href="#huber-regression"><span class="std std-ref">HuberRegressor</span></a> should be faster than
<a class="reference internal" href="#ransac-regression"><span class="std std-ref">RANSAC</span></a> and <a class="reference internal" href="#theil-sen-regression"><span class="std std-ref">Theil Sen</span></a>
unless the number of samples are very large, i.e <code class="docutils literal notranslate"><span class="pre">n_samples</span></code> &gt;&gt; <code class="docutils literal notranslate"><span class="pre">n_features</span></code>.
This is because <a class="reference internal" href="#ransac-regression"><span class="std std-ref">RANSAC</span></a> and <a class="reference internal" href="#theil-sen-regression"><span class="std std-ref">Theil Sen</span></a>
fit on smaller subsets of the data. However, both <a class="reference internal" href="#theil-sen-regression"><span class="std std-ref">Theil Sen</span></a>
and <a class="reference internal" href="#ransac-regression"><span class="std std-ref">RANSAC</span></a> are unlikely to be as robust as
<a class="reference internal" href="#huber-regression"><span class="std std-ref">HuberRegressor</span></a> for the default parameters.</li>
<li><a class="reference internal" href="#ransac-regression"><span class="std std-ref">RANSAC</span></a> is faster than <a class="reference internal" href="#theil-sen-regression"><span class="std std-ref">Theil Sen</span></a>
and scales much better with the number of samples</li>
<li><a class="reference internal" href="#ransac-regression"><span class="std std-ref">RANSAC</span></a> will deal better with large
outliers in the y direction (most common situation)</li>
<li><a class="reference internal" href="#theil-sen-regression"><span class="std std-ref">Theil Sen</span></a> will cope better with
medium-size outliers in the X direction, but this property will
disappear in large dimensional settings.</li>
</ul>
</div></blockquote>
<p>When in doubt, use <a class="reference internal" href="#ransac-regression"><span class="std std-ref">RANSAC</span></a></p>
</div>
</div>
<div class="section" id="ransac">
<span id="ransac-regression"></span><h3>1.1.15.2. RANSAC:随机抽样一致性算法<a class="headerlink" href="#ransac" title="Permalink to this headline">¶</a></h3>
<p>RANSAC (RANdom SAmple Consensus) 在完整数据集的所有inliers的随机子集上拟合一个模型(
fits a model from random subsets of inliers from the complete data set.)。</p>
<p>RANSAC 是一种不确定性算法(non-deterministic algorithm),它以某个概率产生一个合理的结果(reasonable result ),
这依赖于迭代次数(请看参数 <cite>max_trials</cite> 的解释)。 它通常用于线性和非线性回归问题，在摄影测量计算机视觉领域尤其流行。</p>
<p>RANSAC 算法把完整的输入样本数据划分成两个集合：inliers 和 outliers。inliers样本可能会受噪声影响，
outliers样本可能是由于测量错误或对数据的无效假设而产生的。所以，RANSAC产生的模型是只在确定的inliers样本集上估计出来的。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_ransac.html"><img alt="../images/sphx_glr_plot_ransac_0011.png" src="../images/sphx_glr_plot_ransac_0011.png" style="width: 320.0px; height: 240.0px;" /></a>
</div>
<div class="section" id="id38">
<h4>1.1.15.2.1. 算法细节<a class="headerlink" href="#id38" title="Permalink to this headline">¶</a></h4>
<p>RANSAC算法的每个迭代执行以下步骤：</p>
<ol class="arabic simple">
<li>从原始数据中选择 <code class="docutils literal notranslate"><span class="pre">min_samples</span></code> 个随机样本，然后检查这些样本是否是有效的(请看 <code class="docutils literal notranslate"><span class="pre">is_data_valid</span></code>)。</li>
<li>在上一步选出的随机子集上拟合一个模型 (<code class="docutils literal notranslate"><span class="pre">base_estimator.fit</span></code>) 并检查估计出的模型是否有效 (请看 <code class="docutils literal notranslate"><span class="pre">is_model_valid</span></code>).</li>
<li>Classify all data as inliers or outliers by calculating the residuals
to the estimated model (<code class="docutils literal notranslate"><span class="pre">base_estimator.predict(X)</span> <span class="pre">-</span> <span class="pre">y</span></code>) - all data
samples with absolute residuals smaller than the <code class="docutils literal notranslate"><span class="pre">residual_threshold</span></code>
are considered as inliers.</li>
<li>Save fitted model as best model if number of inlier samples is
maximal. In case the current estimated model has the same number of
inliers, it is only considered as the best model if it has better score.</li>
</ol>
<p>These steps are performed either a maximum number of times (<code class="docutils literal notranslate"><span class="pre">max_trials</span></code>) or
until one of the special stop criteria are met (see <code class="docutils literal notranslate"><span class="pre">stop_n_inliers</span></code> and
<code class="docutils literal notranslate"><span class="pre">stop_score</span></code>). The final model is estimated using all inlier samples (consensus
set) of the previously determined best model.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">is_data_valid</span></code> and <code class="docutils literal notranslate"><span class="pre">is_model_valid</span></code> functions allow to identify and reject
degenerate combinations of random sub-samples. If the estimated model is not
needed for identifying degenerate cases, <code class="docutils literal notranslate"><span class="pre">is_data_valid</span></code> should be used as it
is called prior to fitting the model and thus leading to better computational
performance.</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_ransac.html#sphx-glr-auto-examples-linear-model-plot-ransac-py"><span class="std std-ref">Robust linear model estimation using RANSAC</span></a></li>
<li><a class="reference internal" href="../auto_examples/linear_model/plot_robust_fit.html#sphx-glr-auto-examples-linear-model-plot-robust-fit-py"><span class="std std-ref">Robust linear estimator fitting</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="https://en.wikipedia.org/wiki/RANSAC">https://en.wikipedia.org/wiki/RANSAC</a></li>
<li><a class="reference external" href="https://www.sri.com/sites/default/files/publications/ransac-publication.pdf">“Random Sample Consensus: A Paradigm for Model Fitting with Applications to
Image Analysis and Automated Cartography”</a>
Martin A. Fischler and Robert C. Bolles - SRI International (1981)</li>
<li><a class="reference external" href="http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf">“Performance Evaluation of RANSAC Family”</a>
Sunglok Choi, Taemin Kim and Wonpil Yu - BMVC (2009)</li>
</ul>
</div>
</div>
</div>
<div class="section" id="theil-sen">
<span id="theil-sen-regression"></span><h3>1.1.15.3. Theil-Sen 估计器: 广义中值估计器<a class="headerlink" href="#theil-sen" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor" title="sklearn.linear_model.TheilSenRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">TheilSenRegressor</span></code></a> estimator uses a generalization of the median in multiple dimensions.
因此它对多变量离群点(multivariate outliers 或者叫 多元outliers) 比较鲁棒。但是，需要注意的是该估计器的鲁棒性会随着问题的维数增大而迅速降低。
在高维情形中它会丧失鲁棒性并且变得不会比普通最小二乘法更好。</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_theilsen.html#sphx-glr-auto-examples-linear-model-plot-theilsen-py"><span class="std std-ref">Theil-Sen Regression</span></a></li>
<li><a class="reference internal" href="../auto_examples/linear_model/plot_robust_fit.html#sphx-glr-auto-examples-linear-model-plot-robust-fit-py"><span class="std std-ref">Robust linear estimator fitting</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator">https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator</a></li>
</ul>
</div>
<div class="section" id="id39">
<h4>1.1.15.3.1. 理论方面的思考<a class="headerlink" href="#id39" title="Permalink to this headline">¶</a></h4>
<p><a class="reference internal" href="generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor" title="sklearn.linear_model.TheilSenRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">TheilSenRegressor</span></code></a> is comparable to the <a class="reference internal" href="#ordinary-least-squares"><span class="std std-ref">Ordinary Least Squares
(OLS)</span></a> in terms of asymptotic efficiency and as an
unbiased estimator. In contrast to OLS, Theil-Sen is a non-parametric
method which means it makes no assumption about the underlying
distribution of the data. Since Theil-Sen is a median-based estimator, it
is more robust against corrupted data aka outliers. In univariate
setting, Theil-Sen has a breakdown point of about 29.3% in case of a
simple linear regression which means that it can tolerate arbitrary
corrupted data of up to 29.3%.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_theilsen.html"><img alt="../images/sphx_glr_plot_theilsen_0011.png" src="../images/sphx_glr_plot_theilsen_0011.png" style="width: 320.0px; height: 240.0px;" /></a>
</div>
<p>The implementation of <a class="reference internal" href="generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor" title="sklearn.linear_model.TheilSenRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">TheilSenRegressor</span></code></a> in scikit-learn follows a
generalization to a multivariate linear regression model <a class="footnote-reference" href="#f1" id="id40">[8]</a> using the
spatial median which is a generalization of the median to multiple
dimensions <a class="footnote-reference" href="#f2" id="id41">[9]</a>.</p>
<p>In terms of time and space complexity, Theil-Sen scales according to</p>
<div class="math notranslate nohighlight">
\[\binom{n_{samples}}{n_{subsamples}}\]</div>
<p>which makes it infeasible to be applied exhaustively to problems with a
large number of samples and features. Therefore, the magnitude of a
subpopulation can be chosen to limit the time and space complexity by
considering only a random subset of all possible combinations.</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_theilsen.html#sphx-glr-auto-examples-linear-model-plot-theilsen-py"><span class="std std-ref">Theil-Sen Regression</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<table class="docutils footnote" frame="void" id="f1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id40">[8]</a></td><td>Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang: <a class="reference external" href="http://home.olemiss.edu/~xdang/papers/MTSE.pdf">Theil-Sen Estimators in a Multiple Linear Regression Model.</a></td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="f2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id41">[9]</a></td><td><ol class="first last upperalpha simple" start="20">
<li>Kärkkäinen and S. Äyrämö: <a class="reference external" href="http://users.jyu.fi/~samiayr/pdf/ayramo_eurogen05.pdf">On Computation of Spatial Median for Robust Data Mining.</a></li>
</ol>
</td></tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="section" id="huber">
<span id="huber-regression"></span><h3>1.1.15.4. Huber 回归<a class="headerlink" href="#huber" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor" title="sklearn.linear_model.HuberRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">HuberRegressor</span></code></a> 类与 <a class="reference internal" href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-class docutils literal notranslate"><span class="pre">Ridge</span></code></a> 类有所不同，因为它将一个线性损失应用到那些被判断为是离群点的样本上。
如果一个样本的绝对误差小于某个指定的阈值，那么这个样本就被判断为是一个 inlier。
这与 <a class="reference internal" href="generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor" title="sklearn.linear_model.TheilSenRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">TheilSenRegressor</span></code></a> 类 和 <a class="reference internal" href="generated/sklearn.linear_model.RANSACRegressor.html#sklearn.linear_model.RANSACRegressor" title="sklearn.linear_model.RANSACRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">RANSACRegressor</span></code></a> 类是有区别的，因为它不忽略outliers的影响而是会给这些outliers一个比较小的权重。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_huber_vs_ridge.html"><img alt="../images/sphx_glr_plot_huber_vs_ridge_001.png" src="../images/sphx_glr_plot_huber_vs_ridge_001.png" style="width: 320.0px; height: 240.0px;" /></a>
</div>
<p><a class="reference internal" href="generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor" title="sklearn.linear_model.HuberRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">HuberRegressor</span></code></a> 类所要最小的损失函数由下式给出：</p>
<div class="math notranslate nohighlight">
\[\min_{w, \sigma} {\sum_{i=1}^n\left(\sigma + H_{\epsilon}\left(\frac{X_{i}w - y_{i}}{\sigma}\right)\sigma\right) + \alpha {||w||_2}^2}\]</div>
<p>其中</p>
<div class="math notranslate nohighlight">
\[\begin{split}H_{\epsilon}(z) = \begin{cases}
       z^2, &amp; \text {if } |z| &lt; \epsilon, \\
       2\epsilon|z| - \epsilon^2, &amp; \text{otherwise}
\end{cases}\end{split}\]</div>
<p>建议把参数 <code class="docutils literal notranslate"><span class="pre">epsilon</span></code> 的值设为 1.35 以达到 95% 的统计效率(statistical efficiency)。</p>
</div>
<div class="section" id="id42">
<h3>1.1.15.5. 注意<a class="headerlink" href="#id42" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor" title="sklearn.linear_model.HuberRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">HuberRegressor</span></code></a> 类 与 把损失参数设为 <cite>huber</cite> 的 <a class="reference internal" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDRegressor</span></code></a> 类 的区别主要有以下方面：</p>
<ul class="simple">
<li><a class="reference internal" href="generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor" title="sklearn.linear_model.HuberRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">HuberRegressor</span></code></a> 是尺度不变的(scaling invariant)。 一旦设置了 <code class="docutils literal notranslate"><span class="pre">epsilon</span></code> , 把 <code class="docutils literal notranslate"><span class="pre">X</span></code> 和 <code class="docutils literal notranslate"><span class="pre">y</span></code>
放大或缩小不同的值将会产生对outliers一样的鲁棒性，就跟没有缩放之前一样。但是在 <a class="reference internal" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDRegressor</span></code></a> 类中，
如果调整了 <code class="docutils literal notranslate"><span class="pre">X</span></code> 和 <code class="docutils literal notranslate"><span class="pre">y</span></code> 的尺度则 <code class="docutils literal notranslate"><span class="pre">epsilon</span></code> 也得被重新设置。</li>
<li><a class="reference internal" href="generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor" title="sklearn.linear_model.HuberRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">HuberRegressor</span></code></a> 在数据量比较小的时候应该会更有效率，而 <a class="reference internal" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDRegressor</span></code></a> 需要在训练数据集上的很多次迭代才能产生相同的鲁棒性。</li>
</ul>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_huber_vs_ridge.html#sphx-glr-auto-examples-linear-model-plot-huber-vs-ridge-py"><span class="std std-ref">HuberRegressor vs Ridge on dataset with strong outliers</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li>Peter J. Huber, Elvezio M. Ronchetti: Robust Statistics, Concomitant scale estimates, pg 172</li>
</ul>
</div>
<p>另外, 这个估计器不同于鲁邦回归的R语言实现(<a class="reference external" href="http://www.ats.ucla.edu/stat/r/dae/rreg.htm">http://www.ats.ucla.edu/stat/r/dae/rreg.htm</a>)，因为R语言的实现做了
一个加权最小二乘。基于 残差比给定阈值大多少 而产生的权重被赋予了每个样本。</p>
</div>
</div>
<div class="section" id="polynomial-regression">
<span id="id43"></span><h2>1.1.16. 多项式回归:用基函数展开线性模型<a class="headerlink" href="#polynomial-regression" title="Permalink to this headline">¶</a></h2>
<p>机器学习中的一个常见模式是使用在数据的非线性函数上训练的线性模型。这种方法保持了线性方法的一般快速性能，
同时允许它们适应范围更广的数据。</p>
<p>比如，一个简单的线性回归器可以通过从系数构造多项式特征(<strong>polynomial features</strong>)而扩展到非线性数据的拟合任务中。
在标准线性回归模型中，一个用于拟合二维数据的模型如下所示：</p>
<div class="math notranslate nohighlight">
\[\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2\]</div>
<p>如果我们想要用抛物面拟合数据而不是平面，我们可以用二阶多项式组合特征，使模型看起来像这样：</p>
<div class="math notranslate nohighlight">
\[\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_1^2 + w_5 x_2^2\]</div>
<p>上面的模型看起来仍然是一个线性模型(尽管有些令人吃惊)，为了说明这一点，我们想象着创建一个新的变量</p>
<div class="math notranslate nohighlight">
\[z = [x_1, x_2, x_1 x_2, x_1^2, x_2^2]\]</div>
<p>用上面的变量重新变换数据，原来的问题就可以写成这样：</p>
<div class="math notranslate nohighlight">
\[\hat{y}(w, x) = w_0 + w_1 z_1 + w_2 z_2 + w_3 z_3 + w_4 z_4 + w_5 z_5\]</div>
<p>我们看到由此产生的多项式回归(<em>polynomial regression</em>)其实是与我们之前见过的线性模型一样的并且可以用线性模型的求解方法解决它，
因为模型关于未知系数 <span class="math notranslate nohighlight">\(w\)</span> 是线性组合的形式。考虑到可以对由这些基本函数构建的高维空间进行线性拟合，
该模型在拟合更大范围的数据时确实有非常大的弹性和灵活性。</p>
<p>这里有个例子，它将上述idea用于一维数据, 使用了不同阶的多项式特征：</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_polynomial_interpolation.html"><img alt="../images/sphx_glr_plot_polynomial_interpolation_0011.png" src="../images/sphx_glr_plot_polynomial_interpolation_0011.png" style="width: 320.0px; height: 240.0px;" /></a>
</div>
<p>这个图片的创建使用了 <a class="reference internal" href="generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures" title="sklearn.preprocessing.PolynomialFeatures"><code class="xref py py-class docutils literal notranslate"><span class="pre">PolynomialFeatures</span></code></a> preprocessor。
这个预处理器(preprocessor)把输入数据矩阵变成一个给定阶数的新数据矩阵。其用法如下:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">PolynomialFeatures</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span>
<span class="go">array([[0, 1],</span>
<span class="go">       [2, 3],</span>
<span class="go">       [4, 5]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([[ 1.,  0.,  1.,  0.,  0.,  1.],</span>
<span class="go">       [ 1.,  2.,  3.,  4.,  6.,  9.],</span>
<span class="go">       [ 1.,  4.,  5., 16., 20., 25.]])</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">X</span></code> 的各个特征分量已经由 <span class="math notranslate nohighlight">\([x_1, x_2]\)</span> 变换为 <span class="math notranslate nohighlight">\([1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]\)</span>,
并且可被用于任意的线性模型。</p>
<p>这种类型的预处理器可以用 <a class="reference internal" href="compose.html#pipeline"><span class="std std-ref">Pipeline</span></a> 集成为一个工作流。 这样就可以用单个对象表示一个简单的多项式回归器，如下所示:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">PolynomialFeatures</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LinearRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="k">import</span> <span class="n">Pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">)),</span>
<span class="gp">... </span>                  <span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">))])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># fit to an order-3 polynomial data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s1">&#39;linear&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span>
<span class="go">array([ 3., -2.,  1., -1.])</span>
</pre></div>
</div>
<p>在多项式特征上训练好的线性模型可以准确的恢复输入多项式系数。</p>
<p>有些情况下，没有必要包括任意单个特征的更高阶特征, 而只需要所谓的 <em>interaction features</em>。
<em>interaction features</em> 是由最多 <span class="math notranslate nohighlight">\(d\)</span> 个不同的特征相乘在一起构成的。
这可以从 <a class="reference internal" href="generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures" title="sklearn.preprocessing.PolynomialFeatures"><code class="xref py py-class docutils literal notranslate"><span class="pre">PolynomialFeatures</span></code></a> 类中把参数设置成 <a href="#id44"><span class="problematic" id="id45">``</span></a>interaction_only=True``得到。</p>
<p>比如说, 当处理布尔特征的时候, 对所有的 <span class="math notranslate nohighlight">\(n\)</span>,  <span class="math notranslate nohighlight">\(x_i^n = x_i\)</span> 因此这时候高阶多项式基是没用的;
但是任意两个布尔特征的乘积 <span class="math notranslate nohighlight">\(x_i x_j\)</span> 却表达了两个布尔特征的联合(conjunction)。这样，我们就可以用线性分类器来求解异或(XOR)问题</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">Perceptron</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">PolynomialFeatures</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">^</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span>
<span class="go">array([0, 1, 1, 0])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">interaction_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span>
<span class="go">array([[1, 0, 0, 0],</span>
<span class="go">       [1, 0, 1, 0],</span>
<span class="go">       [1, 1, 0, 0],</span>
<span class="go">       [1, 1, 1, 1]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">Perceptron</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="gp">... </span>                 <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>最后，这个线性分类器的预测(“predictions”)相当完美</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([0, 1, 1, 0])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">1.0</span>
</pre></div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer">
        &copy; 2007 - 2018, scikit-learn developers (BSD License).
      <a href="../_sources/modules/linear_model.rst.txt" rel="nofollow">Show this page source</a>
    </div>
     <div class="rel">
    
    <div class="buttonPrevious">
      <a href="../supervised_learning.html">Previous
      </a>
    </div>
    <div class="buttonNext">
      <a href="lda_qda.html">Next
      </a>
    </div>
    
     </div>

    
    <script>
        window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
        ga('create', 'UA-22606712-2', 'auto');
        ga('set', 'anonymizeIp', true);
        ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    
    <script>
      (function() {
        var cx = '016639176250731907682:tjtqbvtvij0';
        var gcse = document.createElement('script'); gcse.type = 'text/javascript'; gcse.async = true;
        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s);
      })();
    </script>
  </body>
</html>