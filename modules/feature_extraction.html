

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  
    <title>4.2. 特征提取(Feature extraction) &#8212; scikit-learn 0.20.1 documentation</title>
  <!-- htmltitle is before nature.css - we use this hack to load bootstrap first -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="../static/css/bootstrap.min.css" media="screen" />
  <link rel="stylesheet" href="../static/css/bootstrap-responsive.css"/>

    <link rel="stylesheet" href="../static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../static/gallery.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../static/documentation_options.js"></script>
    <script type="text/javascript" src="../static/jquery.js"></script>
    <script type="text/javascript" src="../static/underscore.js"></script>
    <script type="text/javascript" src="../static/doctools.js"></script>
    <script type="text/javascript" src="../static/js/copybutton.js"></script>
    <script type="text/javascript" src="../static/js/extra.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_SVG"></script>
    <link rel="shortcut icon" href="../static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4.3. 预处理数据(Preprocessing data)" href="preprocessing.html" />
    <link rel="prev" title="4.1. 管道流与复合估计器(Pipelines and composite estimators)" href="compose.html" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <script src="../static/js/bootstrap.min.js" type="text/javascript"></script>
  <script>
     VERSION_SUBDIR = (function(groups) {
         return groups ? groups[1] : null;
     })(location.href.match(/^https?:\/\/scikit-learn.org\/([^\/]+)/));
  </script>
  <link rel="canonical" href="http://scikit-learn.org/stable/modules/feature_extraction.html" />

  <script type="text/javascript">
    $("div.buttonNext, div.buttonPrevious").hover(
       function () {
           $(this).css('background-color', '#FF9C34');
       },
       function () {
           $(this).css('background-color', '#A7D6E2');
       }
    );
    function showMenu() {
      var topNav = document.getElementById("scikit-navbar");
      if (topNav.className === "navbar") {
          topNav.className += " responsive";
      } else {
          topNav.className = "navbar";
      }
    };
  </script>

  </head><body>

<div class="header-wrapper">
    <div class="header">
        <p class="logo"><a href="../index.html">
            <img src="../static/scikit-learn-logo-small.png" alt="Logo"/>
        </a>
        </p><div class="navbar" id="scikit-navbar">
            <ul>
                <li><a href="../index.html">Home</a></li>
                <li><a href="../install.html">Installation</a></li>
                <li class="btn-li"><div class="btn-group">
              <a href="../documentation.html">Documentation</a>
              <a class="btn dropdown-toggle" data-toggle="dropdown">
                 <span class="caret"></span>
              </a>
              <ul class="dropdown-menu">
            <li class="link-title">Scikit-learn <script>document.write(DOCUMENTATION_OPTIONS.VERSION + (VERSION_SUBDIR ? " (" + VERSION_SUBDIR + ")" : ""));</script></li>
            <li><a href="../tutorial/index.html">Tutorials</a></li>
            <li><a href="../user_guide.html">User guide</a></li>
            <li><a href="classes.html">API</a></li>
            <li><a href="../glossary.html">Glossary</a></li>
            <li><a href="../faq.html">FAQ</a></li>
            <li><a href="../developers/contributing.html">Contributing</a></li>
            <li class="divider"></li>
                <script>if (VERSION_SUBDIR != "stable") document.write('<li><a href="http://scikit-learn.org/stable/documentation.html">Stable version</a></li>')</script>
                <script>if (VERSION_SUBDIR != "dev") document.write('<li><a href="http://scikit-learn.org/dev/documentation.html">Development version</a></li>')</script>
                <li><a href="http://scikit-learn.org/dev/versions.html">All available versions</a></li>
                <li><a href="../downloads/scikit-learn-docs.pdf">PDF documentation</a></li>
              </ul>
            </div>
        </li>
            <li><a href="../auto_examples/index.html">Examples</a></li>
            </ul>
            <a href="javascript:void(0);" onclick="showMenu()">
                <div class="nav-icon">
                    <div class="hamburger-line"></div>
                    <div class="hamburger-line"></div>
                    <div class="hamburger-line"></div>
                </div>
            </a>
            <div class="search_form">
                <div class="gcse-search" id="cse" style="width: 100%;"></div>
            </div>
        </div> <!-- end navbar --></div>
</div>


<!-- GitHub "fork me" ribbon -->
<a href="https://github.com/scikit-learn/scikit-learn">
  <img class="fork-me"
       style="position: absolute; top: 0; right: 0; border: 0;"
       src="../static/img/forkme.png"
       alt="Fork me on GitHub" />
</a>

<div class="content-wrapper">
    <div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
        <div class="rel">
    
        <div class="rellink">
        <a href="compose.html"
        accesskey="P">Previous
        <br/>
        <span class="smallrellink">
        4.1. 管道流与复合估计...
        </span>
            <span class="hiddenrellink">
            4.1. 管道流与复合估计器(Pipelines and composite estimators)
            </span>
        </a>
        </div>
            <div class="spacer">
            &nbsp;
            </div>
        <div class="rellink">
        <a href="preprocessing.html"
        accesskey="N">Next
        <br/>
        <span class="smallrellink">
        4.3. 预处理数据(Pr...
        </span>
            <span class="hiddenrellink">
            4.3. 预处理数据(Preprocessing data)
            </span>
        </a>
        </div>

    <!-- Ad a link to the 'up' page -->
        <div class="spacer">
        &nbsp;
        </div>
        <div class="rellink">
        <a href="../data_transforms.html">
        Up
        <br/>
        <span class="smallrellink">
        4. 数据集变换(Data...
        </span>
            <span class="hiddenrellink">
            4. 数据集变换(Dataset transformations)
            </span>
            
        </a>
        </div>
    </div>
    
      <p class="doc-version"><b>scikit-learn v0.20.1</b><br/>
      <a href="http://scikit-learn.org/dev/versions.html">Other versions</a></p>
    <p class="citing">Please <b><a href="../about.html#citing-scikit-learn" style="font-size: 110%;">cite us </a></b>if you use the software.</p>
    <ul>
<li><a class="reference internal" href="#">4.2. 特征提取(Feature extraction)</a><ul>
<li><a class="reference internal" href="#dict-feature-extraction">4.2.1. 从字典加载特征</a></li>
<li><a class="reference internal" href="#feature-hashing">4.2.2. 特征哈希(散列)化</a><ul>
<li><a class="reference internal" href="#id5">4.2.2.1. 实现细节</a></li>
</ul>
</li>
<li><a class="reference internal" href="#text-feature-extraction">4.2.3. 文本特征提取</a><ul>
<li><a class="reference internal" href="#id7">4.2.3.1. 词袋表示法</a></li>
<li><a class="reference internal" href="#id8">4.2.3.2. 稀疏性</a></li>
<li><a class="reference internal" href="#vectorizer">4.2.3.3. 常见 Vectorizer 的用法</a><ul>
<li><a class="reference internal" href="#stop-words">4.2.3.3.1. 使用 stop words</a></li>
</ul>
</li>
<li><a class="reference internal" href="#tfidf-term-weighting">4.2.3.4. Tf–idf term weighting</a></li>
<li><a class="reference internal" href="#id11">4.2.3.5. 解码文本文件</a></li>
<li><a class="reference internal" href="#id12">4.2.3.6. 应用和案例</a></li>
<li><a class="reference internal" href="#id13">4.2.3.7. 词袋表示法的局限性</a></li>
<li><a class="reference internal" href="#hashing-vectorizer">4.2.3.8. 用散列技巧矢量化大型语料库</a></li>
<li><a class="reference internal" href="#hashingvectorizer-scaling">4.2.3.9. 使用 HashingVectorizer 执行核外scaling</a></li>
<li><a class="reference internal" href="#customizing-the-vectorizer-classes">4.2.3.10. Customizing the vectorizer classes</a></li>
</ul>
</li>
<li><a class="reference internal" href="#image-feature-extraction">4.2.4. 图像特征提取</a><ul>
<li><a class="reference internal" href="#id16">4.2.4.1. 图像块提取</a></li>
<li><a class="reference internal" href="#id17">4.2.4.2. 图像的连接图</a></li>
</ul>
</li>
</ul>
</li>
</ul>

    </div>
</div>

<input type="checkbox" id="nav-trigger" class="nav-trigger" checked />
<label for="nav-trigger"></label>




      <div class="content">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="feature-extraction">
<span id="id1"></span><h1>4.2. 特征提取(Feature extraction)<a class="headerlink" href="#feature-extraction" title="Permalink to this headline">¶</a></h1>
<p><a class="reference internal" href="classes.html#module-sklearn.feature_extraction" title="sklearn.feature_extraction"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.feature_extraction</span></code></a> 模块可以被用于以机器学习算法支持的格式从原始数据集(如文本和图像)提取特征。</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">特征提取与特征选择 <a class="reference internal" href="feature_selection.html#feature-selection"><span class="std std-ref">特征选择(Feature selection)</span></a> 有很大的不同 ：前者是将任何形式的数据如文本，图像转换成可用于机器学习的数值型特征；
后者是一种应用在这些特征上的机器学习技术。</p>
</div>
<div class="section" id="dict-feature-extraction">
<span id="id2"></span><h2>4.2.1. 从字典加载特征<a class="headerlink" href="#dict-feature-extraction" title="Permalink to this headline">¶</a></h2>
<p>类 <a class="reference internal" href="generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">DictVectorizer</span></code></a> 可用于将 以标准Python <code class="docutils literal notranslate"><span class="pre">dict</span></code> 对象的列表形式表示的特征数组转换为
scikit-learn 估计器使用的 NumPy/SciPy 表示形式。</p>
<p>虽然处理速度不是特别快，但Python的 <code class="docutils literal notranslate"><span class="pre">dict</span></code> 优点是使用方便，稀疏（缺失的特征不需要存储），
并且除了值之外还存储特征名称。</p>
<p><a class="reference internal" href="generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">DictVectorizer</span></code></a> 类实现了对 标称型特征(categorical or  nominal or discrete features)的 one-of-K 或 “one-hot” 编码。
标称型特征是 “attribute-value” 对，其中 value的取值被限制在一个不排序的可能性的离散列表中。 (e.g. 话题标识符，对象类型，标签，名称)。</p>
<p>在下面, “city” 是一个 标称型属性(特征)，而 “temperature” 是一个传统的 数值型特征(numerical feature):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">measurements</span> <span class="o">=</span> <span class="p">[</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s1">&#39;city&#39;</span><span class="p">:</span> <span class="s1">&#39;Dubai&#39;</span><span class="p">,</span> <span class="s1">&#39;temperature&#39;</span><span class="p">:</span> <span class="mf">33.</span><span class="p">},</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s1">&#39;city&#39;</span><span class="p">:</span> <span class="s1">&#39;London&#39;</span><span class="p">,</span> <span class="s1">&#39;temperature&#39;</span><span class="p">:</span> <span class="mf">12.</span><span class="p">},</span>
<span class="gp">... </span>    <span class="p">{</span><span class="s1">&#39;city&#39;</span><span class="p">:</span> <span class="s1">&#39;San Francisco&#39;</span><span class="p">,</span> <span class="s1">&#39;temperature&#39;</span><span class="p">:</span> <span class="mf">18.</span><span class="p">},</span>
<span class="gp">... </span><span class="p">]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction</span> <span class="k">import</span> <span class="n">DictVectorizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vec</span> <span class="o">=</span> <span class="n">DictVectorizer</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">measurements</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="go">array([[ 1.,  0.,  0., 33.],</span>
<span class="go">       [ 0.,  1.,  0., 12.],</span>
<span class="go">       [ 0.,  0.,  1., 18.]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">vec</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>
<span class="go">[&#39;city=Dubai&#39;, &#39;city=London&#39;, &#39;city=San Francisco&#39;, &#39;temperature&#39;]</span>
</pre></div>
</div>
<p><a class="reference internal" href="generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer" title="sklearn.feature_extraction.DictVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">DictVectorizer</span></code></a> 类也是在自然语言处理模型中训练序列分类器的有用的表达变换(representation transformation)，
通常通过提取围绕特定兴趣词的特征窗口来工作。</p>
<p>例如，假设我们具有提取我们想要用作训练序列分类器（例如：块）的互补标签的部分语音（PoS）标签的一个算法。
以下 dict 可以是在 “坐在垫子上的猫” 的句子，围绕 “sat” 一词提取的这样一个特征窗口:</p>
<p>For example, suppose that we have a first algorithm that extracts Part of
Speech (PoS) tags that we want to use as complementary tags for training
a sequence classifier (e.g. a chunker). The following dict could be
such a window of features extracted around the word ‘sat’ in the sentence
‘The cat sat on the mat.’:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pos_window</span> <span class="o">=</span> <span class="p">[</span>
<span class="gp">... </span>    <span class="p">{</span>
<span class="gp">... </span>        <span class="s1">&#39;word-2&#39;</span><span class="p">:</span> <span class="s1">&#39;the&#39;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s1">&#39;pos-2&#39;</span><span class="p">:</span> <span class="s1">&#39;DT&#39;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s1">&#39;word-1&#39;</span><span class="p">:</span> <span class="s1">&#39;cat&#39;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s1">&#39;pos-1&#39;</span><span class="p">:</span> <span class="s1">&#39;NN&#39;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s1">&#39;word+1&#39;</span><span class="p">:</span> <span class="s1">&#39;on&#39;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s1">&#39;pos+1&#39;</span><span class="p">:</span> <span class="s1">&#39;PP&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="p">},</span>
<span class="gp">... </span>    <span class="c1"># in a real application one would extract many such dictionaries</span>
<span class="gp">... </span><span class="p">]</span>
</pre></div>
</div>
<p>上述描述可以被矢量化为适合于传递给分类器的稀疏二维矩阵（可能要在pipe之后进行 <a class="reference internal" href="generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer" title="sklearn.feature_extraction.text.TfidfTransformer"><code class="xref py py-class docutils literal notranslate"><span class="pre">text.TfidfTransformer</span></code></a> 归一化）:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vec</span> <span class="o">=</span> <span class="n">DictVectorizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pos_vectorized</span> <span class="o">=</span> <span class="n">vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">pos_window</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pos_vectorized</span>                
<span class="go">&lt;1x6 sparse matrix of type &#39;&lt;... &#39;numpy.float64&#39;&gt;&#39;</span>
<span class="go">    with 6 stored elements in Compressed Sparse ... format&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pos_vectorized</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="go">array([[1., 1., 1., 1., 1., 1.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vec</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>
<span class="go">[&#39;pos+1=PP&#39;, &#39;pos-1=NN&#39;, &#39;pos-2=DT&#39;, &#39;word+1=on&#39;, &#39;word-1=cat&#39;, &#39;word-2=the&#39;]</span>
</pre></div>
</div>
<p>你可以想象，如果一个文本语料库的每一个单词都提取了这样一个上下文，那么所得的矩阵将会非常宽（许多 one-hot-features），其中大部分通常将会是0。
为了使产生的数据结构能够适应内存，该类 <code class="docutils literal notranslate"><span class="pre">DictVectorizer</span></code> 默认使用 <code class="docutils literal notranslate"><span class="pre">scipy.sparse</span></code> 矩阵而不是 <code class="docutils literal notranslate"><span class="pre">numpy.ndarray</span></code>。</p>
</div>
<div class="section" id="feature-hashing">
<span id="id3"></span><h2>4.2.2. 特征哈希(散列)化<a class="headerlink" href="#feature-hashing" title="Permalink to this headline">¶</a></h2>
<p>类 <a class="reference internal" href="generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="xref py py-class docutils literal notranslate"><span class="pre">FeatureHasher</span></code></a> 是一种高速，低内存消耗的 向量化方法，它使用了特征散列化
(<a class="reference external" href="https://en.wikipedia.org/wiki/Feature_hashing">feature hashing</a>) 技术 ，或可称为 “散列法”(hashing trick)的技术。
该类的做法不是去构建 训练中遇到的特征 的哈希表，如向量化所做的那样, <a class="reference internal" href="generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="xref py py-class docutils literal notranslate"><span class="pre">FeatureHasher</span></code></a> 类实例 将哈希函数应用于特征，
以便直接在样本矩阵中确定它们的列索引。
结果是以牺牲可检测性(inspectability)为代价，带来速度的提高和内存使用的减少;
hasher 不记得输入特征是什么样的，也没有 <code class="docutils literal notranslate"><span class="pre">inverse_transform</span></code> 办法。</p>
<p>由于散列函数可能导致（不相关）特征之间的冲突，因此使用带符号散列函数，并且散列值的符号确定存储在特征的输出矩阵中的值的符号。
这样，碰撞可能会抵消而不是累积错误，并且任何输出特征的值的预期平均值为零。默认情况下，此机制将使用 <code class="docutils literal notranslate"><span class="pre">alternate_sign=True</span></code> 启用，
尤其对小型哈希表的大小（ <code class="docutils literal notranslate"><span class="pre">n_features</span> <span class="pre">&lt;</span> <span class="pre">10000</span></code> ）特别有用。 对于大哈希表的大小，可以禁用它，以便将输出传递给估计器，
如 <a class="reference internal" href="generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB" title="sklearn.naive_bayes.MultinomialNB"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.naive_bayes.MultinomialNB</span></code></a> 或 <a class="reference internal" href="generated/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2" title="sklearn.feature_selection.chi2"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.feature_selection.chi2</span></code></a> 特征选择器，这些特征选项器希望输入是非负的。</p>
<p><a class="reference internal" href="generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="xref py py-class docutils literal notranslate"><span class="pre">FeatureHasher</span></code></a> 类接受三种类型的输入：mappings ，<code class="docutils literal notranslate"><span class="pre">(feature,</span> <span class="pre">value)</span></code> pairs，或 strings。
其中 mappings 就像是python的 <code class="docutils literal notranslate"><span class="pre">dict</span></code> 或在 <code class="docutils literal notranslate"><span class="pre">collections</span></code> 模块中的字典的变体。
到底使用哪种参数依赖于构造器的 <code class="docutils literal notranslate"><span class="pre">input_type</span></code> 参数。
Mapping 被当作是由 <code class="docutils literal notranslate"><span class="pre">(feature,</span> <span class="pre">value)</span></code> 组成的列表(list), 而 单个字符串有一个内在的值 1 ，因此 <code class="docutils literal notranslate"><span class="pre">['feat1',</span> <span class="pre">'feat2',</span> <span class="pre">'feat3']</span></code>
被解释成 <code class="docutils literal notranslate"><span class="pre">[('feat1',</span> <span class="pre">1),</span> <span class="pre">('feat2',</span> <span class="pre">1),</span> <span class="pre">('feat3',</span> <span class="pre">1)]</span></code>。
如果一个特征在一个样本中多次出现，那么该特征关联的值就会被累加起来，比如像这样 (<code class="docutils literal notranslate"><span class="pre">('feat',</span> <span class="pre">2)</span></code> 和 <code class="docutils literal notranslate"><span class="pre">('feat',</span> <span class="pre">3.5)</span></code> 就变成了 <code class="docutils literal notranslate"><span class="pre">('feat',</span> <span class="pre">5.5)</span></code>)。
类 <a class="reference internal" href="generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="xref py py-class docutils literal notranslate"><span class="pre">FeatureHasher</span></code></a> 的输出总是 CSR 格式的 一个 <code class="docutils literal notranslate"><span class="pre">scipy.sparse</span></code> 矩阵。</p>
<p>特征散列(Feature hashing)可以被用于文档分类，但是它不像 <a class="reference internal" href="generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">text.CountVectorizer</span></code></a> 类,
<a class="reference internal" href="generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="xref py py-class docutils literal notranslate"><span class="pre">FeatureHasher</span></code></a> 类不进行单词分割 或其他预处理除了 Unicode-to-UTF-8 编码;
请看下面 <a class="reference internal" href="#hashing-vectorizer"><span class="std std-ref">用散列技巧矢量化大型语料库</span></a> , 是一个 combined tokenizer/hasher 。</p>
<p>作为一个例子，考虑一个单词级的自然语言处理任务，它需要从 <code class="docutils literal notranslate"><span class="pre">(token,</span> <span class="pre">part_of_speech)</span></code> pairs 中抽取特征。
我们可以使用一个 Python 生成器函数 来提取特征</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">token_features</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">part_of_speech</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">token</span><span class="o">.</span><span class="n">isdigit</span><span class="p">():</span>
        <span class="k">yield</span> <span class="s2">&quot;numeric&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">yield</span> <span class="s2">&quot;token=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
        <span class="k">yield</span> <span class="s2">&quot;token,pos=</span><span class="si">{}</span><span class="s2">,</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">part_of_speech</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">token</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">isupper</span><span class="p">():</span>
        <span class="k">yield</span> <span class="s2">&quot;uppercase_initial&quot;</span>
    <span class="k">if</span> <span class="n">token</span><span class="o">.</span><span class="n">isupper</span><span class="p">():</span>
        <span class="k">yield</span> <span class="s2">&quot;all_uppercase&quot;</span>
    <span class="k">yield</span> <span class="s2">&quot;pos=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">part_of_speech</span><span class="p">)</span>
</pre></div>
</div>
<p>然后, 要被传递到 <code class="docutils literal notranslate"><span class="pre">FeatureHasher.transform</span></code> 里面去的 <code class="docutils literal notranslate"><span class="pre">raw_X</span></code> 可以使用下面的方法构建:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">raw_X</span> <span class="o">=</span> <span class="p">(</span><span class="n">token_features</span><span class="p">(</span><span class="n">tok</span><span class="p">,</span> <span class="n">pos_tagger</span><span class="p">(</span><span class="n">tok</span><span class="p">))</span> <span class="k">for</span> <span class="n">tok</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">)</span>
</pre></div>
</div>
<p>然后使用下面的方法把它喂给 FeatureHasher 类的一个对象实例 (hasher)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hasher</span> <span class="o">=</span> <span class="n">FeatureHasher</span><span class="p">(</span><span class="n">input_type</span><span class="o">=</span><span class="s1">&#39;string&#39;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">hasher</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">raw_X</span><span class="p">)</span>
</pre></div>
</div>
<p>得到的输出是一个 <code class="docutils literal notranslate"><span class="pre">scipy.sparse</span></code> 类型的矩阵 <code class="docutils literal notranslate"><span class="pre">X</span></code>。</p>
<p>这里需要注意的是 由于我们使用了Python的生成器，导致在特征抽取过程中引入了懒惰性:
只有在hasher有需求的时候tokens才会被处理(tokens are only processed on demand from the hasher)。</p>
<div class="section" id="id5">
<h3>4.2.2.1. 实现细节<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="xref py py-class docutils literal notranslate"><span class="pre">FeatureHasher</span></code></a> 类使用带符号的 32-bit 变体的 MurmurHash3。 作为其结果(也因为 <code class="docutils literal notranslate"><span class="pre">scipy.sparse</span></code> 里面的限制)，
当前支持的特征的最大数量 <span class="math notranslate nohighlight">\(2^{31} - 1\)</span> 。</p>
<p>散列技巧(hashing trick)的原始形式源于Weinberger et al。 使用两个分开的哈希函数，<span class="math notranslate nohighlight">\(h\)</span> 和 <span class="math notranslate nohighlight">\(\xi\)</span> 分别确定特征的列索引和符号。
现有的实现是基于假设：MurmurHash3的符号位与其他位独立(the sign bit of MurmurHash3 is independent of its other bits)。</p>
<p>由于使用简单的模数将哈希函数转换为列索引，建议使用2次幂作为 <code class="docutils literal notranslate"><span class="pre">n_features</span></code> 参数; 否则特征不会被均匀的分布到列中。</p>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li>Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and
Josh Attenberg (2009). <a class="reference external" href="http://alex.smola.org/papers/2009/Weinbergeretal09.pdf">Feature hashing for large scale multitask learning</a>. Proc. ICML.</li>
<li><a class="reference external" href="https://github.com/aappleby/smhasher">MurmurHash3</a>.</li>
</ul>
</div>
</div>
</div>
<div class="section" id="text-feature-extraction">
<span id="id6"></span><h2>4.2.3. 文本特征提取<a class="headerlink" href="#text-feature-extraction" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id7">
<h3>4.2.3.1. 词袋表示法<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>文本分析是机器学习算法的主要应用领域。 然而，原始数据，符号文字序列不能直接传递给算法，因为它们大多数要求具有固定长度的数字矩阵特征向量，
而不是具有可变长度的原始文本文档。</p>
<p>为解决这个问题，scikit-learn 提供了从文本内容中提取数字特征的最常见方法，即：</p>
<ul class="simple">
<li><strong>tokenizing</strong> 令牌化 即对每个可能的 词令牌(token) 分成字符串并赋予整型id，例如通过使用空格和标点符号作为令牌分隔符(token separators)。</li>
<li><strong>counting</strong> 统计计数 即数出每个文档中令牌的出现次数。</li>
<li><strong>normalizing</strong> 标准化 即 对大多数样本/文档中出现的重要性递减的token进行归一化和加权</li>
</ul>
<p>在这个机制中, 特征和样本是如下定义的：</p>
<ul class="simple">
<li>每个单独的令牌发生频率（归一化或不归一化）被视为一个特征 (each <strong>individual token occurrence frequency</strong> (normalized or not)
is treated as a <strong>feature</strong>.)。</li>
<li>给定文档中所有的令牌频率向量被看做一个多元样本(the vector of all the token frequencies for a given <strong>document</strong> is
considered a multivariate <strong>sample</strong>.)。</li>
</ul>
<p>因此，文档的集合(文集：corpus of documents)可被表示为矩阵形式，每行对应一个文本文档，每列对应文集中出现的词令牌(如单个词)。</p>
<p>我们称 向量化(<strong>vectorization</strong>) 是将文本文档集合转换为数字集合特征向量的通用方法。 这种特别的策略（令牌化，计数和归一化）被称为
<strong>Bag of Words</strong> 或 “Bag of n-grams” 表示法。文档由单词的出现与否和出现频率来描述，同时完全忽略文档中单词的相对位置信息。</p>
</div>
<div class="section" id="id8">
<h3>4.2.3.2. 稀疏性<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>由于大多数文本文档通常只使用文集的词向量全集中的一个小子集，所以得到的矩阵将具有许多特征值为零（通常大于99％）。</p>
<p>例如，10,000 个短文本文档（如电子邮件）的集合将使用总共100,000个独特词的大小的词汇，而每个文档将单独使用100到1000个独特的单词。</p>
<p>为了能够将这样的矩阵存储在存储器中，并且还可以加速代数的矩阵/向量运算，实现通常将使用诸如 <code class="docutils literal notranslate"><span class="pre">scipy.sparse</span></code> 包中的稀疏实现</p>
</div>
<div class="section" id="vectorizer">
<h3>4.2.3.3. 常见 Vectorizer 的用法<a class="headerlink" href="#vectorizer" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">CountVectorizer</span></code></a> 在单个类中实现了 词语切分(tokenization) 和 出现频数统计(occurrence counting)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="k">import</span> <span class="n">CountVectorizer</span>
</pre></div>
</div>
<p>这个模型有很多参数，但参数的默认初始值是相当合理的（请参阅 <a class="reference internal" href="classes.html#text-feature-extraction-ref"><span class="std std-ref">参考文档</span></a> 了解详细信息）:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span>                     
<span class="go">CountVectorizer(analyzer=...&#39;word&#39;, binary=False, decode_error=...&#39;strict&#39;,</span>
<span class="go">        dtype=&lt;... &#39;numpy.int64&#39;&gt;, encoding=...&#39;utf-8&#39;, input=...&#39;content&#39;,</span>
<span class="go">        lowercase=True, max_df=1.0, max_features=None, min_df=1,</span>
<span class="go">        ngram_range=(1, 1), preprocessor=None, stop_words=None,</span>
<span class="go">        strip_accents=None, token_pattern=...&#39;(?u)\\b\\w\\w+\\b&#39;,</span>
<span class="go">        tokenizer=None, vocabulary=None)</span>
</pre></div>
</div>
<p>我们用该类对一个简约的文本语料库进行 分词(tokenize)和 统计单词出现频数</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
<span class="gp">... </span>    <span class="s1">&#39;This is the first document.&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;This is the second second document.&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;And the third one.&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s1">&#39;Is this the first document?&#39;</span><span class="p">,</span>
<span class="gp">... </span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span>                              
<span class="go">&lt;4x9 sparse matrix of type &#39;&lt;... &#39;numpy.int64&#39;&gt;&#39;</span>
<span class="go">    with 19 stored elements in Compressed Sparse ... format&gt;</span>
</pre></div>
</div>
<p>默认配置是 通过提取至少包含2个字母的单词来对 string 进行分词。做这一步的函数可以显式地被调用</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">analyze</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">build_analyzer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">analyze</span><span class="p">(</span><span class="s2">&quot;This is a text document to analyze.&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="s1">&#39;this&#39;</span><span class="p">,</span> <span class="s1">&#39;is&#39;</span><span class="p">,</span> <span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="s1">&#39;document&#39;</span><span class="p">,</span> <span class="s1">&#39;to&#39;</span><span class="p">,</span> <span class="s1">&#39;analyze&#39;</span><span class="p">])</span>
<span class="go">True</span>
</pre></div>
</div>
<p>analyzer 在拟合过程中找到的每个 term（项）都会被分配一个唯一的整数索引，对应于 resulting matrix 中的一列。
此列的一些说明可以被检索如下</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="s1">&#39;and&#39;</span><span class="p">,</span> <span class="s1">&#39;document&#39;</span><span class="p">,</span> <span class="s1">&#39;first&#39;</span><span class="p">,</span> <span class="s1">&#39;is&#39;</span><span class="p">,</span> <span class="s1">&#39;one&#39;</span><span class="p">,</span>
<span class="gp">... </span>     <span class="s1">&#39;second&#39;</span><span class="p">,</span> <span class="s1">&#39;the&#39;</span><span class="p">,</span> <span class="s1">&#39;third&#39;</span><span class="p">,</span> <span class="s1">&#39;this&#39;</span><span class="p">])</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>           
<span class="go">array([[0, 1, 1, 1, 0, 0, 1, 0, 1],</span>
<span class="go">       [0, 1, 0, 1, 0, 2, 1, 0, 1],</span>
<span class="go">       [1, 0, 0, 0, 1, 0, 1, 1, 0],</span>
<span class="go">       [0, 1, 1, 1, 0, 0, 1, 0, 1]]...)</span>
</pre></div>
</div>
<p>从 feature名称 到 列索引(column index) 的逆映射存储在 <code class="docutils literal notranslate"><span class="pre">vocabulary_</span></code> 属性中:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span><span class="o">.</span><span class="n">vocabulary_</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;document&#39;</span><span class="p">)</span>
<span class="go">1</span>
</pre></div>
</div>
<p>因此，在未来对 transform 方法的调用中，在 训练语料库(training corpus) 中没有看到的单词将被完全忽略：:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="s1">&#39;Something completely new.&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="gp">... </span>                          
<span class="go">array([[0, 0, 0, 0, 0, 0, 0, 0, 0]]...)</span>
</pre></div>
</div>
<p>请注意，在前面的语料库中，第一个和最后一个文档具有完全相同的词，因此被编码成相同的向量。
特别是我们丢失了 最后一个文件是一个疑问的形式 的信息。
为了保留局部的词组顺序信息，除了提取一元模型 1-grams（个别词）之外，我们还可以提取 2-grams 的单词</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">bigram_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
<span class="gp">... </span>                                    <span class="n">token_pattern</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;\b\w+\b&#39;</span><span class="p">,</span> <span class="n">min_df</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">analyze</span> <span class="o">=</span> <span class="n">bigram_vectorizer</span><span class="o">.</span><span class="n">build_analyzer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">analyze</span><span class="p">(</span><span class="s1">&#39;Bi-grams are cool!&#39;</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="s1">&#39;bi&#39;</span><span class="p">,</span> <span class="s1">&#39;grams&#39;</span><span class="p">,</span> <span class="s1">&#39;are&#39;</span><span class="p">,</span> <span class="s1">&#39;cool&#39;</span><span class="p">,</span> <span class="s1">&#39;bi grams&#39;</span><span class="p">,</span> <span class="s1">&#39;grams are&#39;</span><span class="p">,</span> <span class="s1">&#39;are cool&#39;</span><span class="p">])</span>
<span class="go">True</span>
</pre></div>
</div>
<p>由上述 向量化器(vectorizer) 提取的 vocabulary 因此会变得更大，同时可以在局部定位模式时消除歧义</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X_2</span> <span class="o">=</span> <span class="n">bigram_vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_2</span>
<span class="gp">... </span>                          
<span class="go">array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],</span>
<span class="go">       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],</span>
<span class="go">       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],</span>
<span class="go">       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]]...)</span>
</pre></div>
</div>
<p>特别是 “Is this” 的疑问形式只出现在最后一个文档中:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">feature_index</span> <span class="o">=</span> <span class="n">bigram_vectorizer</span><span class="o">.</span><span class="n">vocabulary_</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;is this&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_2</span><span class="p">[:,</span> <span class="n">feature_index</span><span class="p">]</span>     
<span class="go">array([0, 0, 0, 1]...)</span>
</pre></div>
</div>
<div class="section" id="stop-words">
<span id="id9"></span><h4>4.2.3.3.1. 使用 stop words<a class="headerlink" href="#stop-words" title="Permalink to this headline">¶</a></h4>
<p>Stop words are words like “and”, “the”, “him”, which are presumed to be
uninformative in representing the content of a text, and which may be
removed to avoid them being construed as signal for prediction.  Sometimes,
however, similar words are useful for prediction, such as in classifying
writing style or personality.</p>
<p>There are several known issues in our provided ‘english’ stop word list. See
<a class="reference internal" href="#nqy18" id="id10">[NQY18]</a>.</p>
<p>Please take care in choosing a stop word list.
Popular stop word lists may include words that are highly informative to
some tasks, such as <em>computer</em>.</p>
<p>You should also make sure that the stop word list has had the same
preprocessing and tokenization applied as the one used in the vectorizer.
The word <em>we’ve</em> is split into <em>we</em> and <em>ve</em> by CountVectorizer’s default
tokenizer, so if <em>we’ve</em> is in <code class="docutils literal notranslate"><span class="pre">stop_words</span></code>, but <em>ve</em> is not, <em>ve</em> will
be retained from <em>we’ve</em> in transformed text.  Our vectorizers will try to
identify and warn about some kinds of inconsistencies.</p>
<div class="topic">
<p class="topic-title first">References</p>
<table class="docutils citation" frame="void" id="nqy18" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id10">[NQY18]</a></td><td>J. Nothman, H. Qin and R. Yurchak (2018).
<a class="reference external" href="http://aclweb.org/anthology/W18-2502">“Stop Word Lists in Free Open-source Software Packages”</a>.
In <em>Proc. Workshop for NLP Open Source Software</em>.</td></tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="section" id="tfidf-term-weighting">
<span id="tfidf"></span><h3>4.2.3.4. Tf–idf term weighting<a class="headerlink" href="#tfidf-term-weighting" title="Permalink to this headline">¶</a></h3>
<p>在一个大的文本语料库中，一些单词将出现很多次（例如 “the”, “a”, “is” 是英文），因此对文档的实际内容没有什么有意义的信息。
如果我们直接将直接计数数据提供给分类器，那么这些非常频繁的词组(very frequent terms)会掩盖住那些我们感兴趣但却很少出现的词。</p>
<p>为了重新计算特征权重，并将其转化为适合分类器使用的浮点值，因此使用 tf-idf 变换(tf–idf transform)是非常常见的。</p>
<p>Tf means <strong>term-frequency</strong> while tf–idf means term-frequency times
<strong>inverse document-frequency</strong>:
<span class="math notranslate nohighlight">\(\text{tf-idf(t,d)}=\text{tf(t,d)} \times \text{idf(t)}\)</span>.</p>
<p>Using the <code class="docutils literal notranslate"><span class="pre">TfidfTransformer</span></code>’s default settings,
<code class="docutils literal notranslate"><span class="pre">TfidfTransformer(norm='l2',</span> <span class="pre">use_idf=True,</span> <span class="pre">smooth_idf=True,</span> <span class="pre">sublinear_tf=False)</span></code>
the term frequency, the number of times a term occurs in a given document,
is multiplied with idf component, which is computed as</p>
<p><span class="math notranslate nohighlight">\(\text{idf}(t) = log{\frac{1 + n_d}{1+\text{df}(d,t)}} + 1\)</span>,</p>
<p>where <span class="math notranslate nohighlight">\(n_d\)</span> is the total number of documents, and <span class="math notranslate nohighlight">\(\text{df}(d,t)\)</span>
is the number of documents that contain term <span class="math notranslate nohighlight">\(t\)</span>. The resulting tf-idf
vectors are then normalized by the Euclidean norm:</p>
<p><span class="math notranslate nohighlight">\(v_{norm} = \frac{v}{||v||_2} = \frac{v}{\sqrt{v{_1}^2 +
v{_2}^2 + \dots + v{_n}^2}}\)</span>.</p>
<p>This was originally a term weighting scheme developed for information retrieval
(as a ranking function for search engines results) that has also found good
use in document classification and clustering.</p>
<p>The following sections contain further explanations and examples that
illustrate how the tf-idfs are computed exactly and how the tf-idfs
computed in scikit-learn’s <a class="reference internal" href="generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer" title="sklearn.feature_extraction.text.TfidfTransformer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TfidfTransformer</span></code></a>
and <a class="reference internal" href="generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" title="sklearn.feature_extraction.text.TfidfVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TfidfVectorizer</span></code></a> differ slightly from the standard textbook
notation that defines the idf as</p>
<p><span class="math notranslate nohighlight">\(\text{idf}(t) = log{\frac{n_d}{1+\text{df}(d,t)}}.\)</span></p>
<p>In the <a class="reference internal" href="generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer" title="sklearn.feature_extraction.text.TfidfTransformer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TfidfTransformer</span></code></a> and <a class="reference internal" href="generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" title="sklearn.feature_extraction.text.TfidfVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TfidfVectorizer</span></code></a>
with <code class="docutils literal notranslate"><span class="pre">smooth_idf=False</span></code>, the
“1” count is added to the idf instead of the idf’s denominator:</p>
<p><span class="math notranslate nohighlight">\(\text{idf}(t) = log{\frac{n_d}{\text{df}(d,t)}} + 1\)</span></p>
<p>This normalization is implemented by the <a class="reference internal" href="generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer" title="sklearn.feature_extraction.text.TfidfTransformer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TfidfTransformer</span></code></a>
class:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="k">import</span> <span class="n">TfidfTransformer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span> <span class="o">=</span> <span class="n">TfidfTransformer</span><span class="p">(</span><span class="n">smooth_idf</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span>   
<span class="go">TfidfTransformer(norm=...&#39;l2&#39;, smooth_idf=False, sublinear_tf=False,</span>
<span class="go">                 use_idf=True)</span>
</pre></div>
</div>
<p>Again please see the <a class="reference internal" href="classes.html#text-feature-extraction-ref"><span class="std std-ref">reference documentation</span></a> for the details on all the parameters.</p>
<p>Let’s take an example with the following counts. The first term is present
100% of the time hence not very interesting. The two other features only
in less than 50% of the time hence probably more representative of the
content of the documents:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">counts</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>          <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>          <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>          <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>          <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>          <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tfidf</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tfidf</span>                         
<span class="go">&lt;6x3 sparse matrix of type &#39;&lt;... &#39;numpy.float64&#39;&gt;&#39;</span>
<span class="go">    with 9 stored elements in Compressed Sparse ... format&gt;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">tfidf</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>                        
<span class="go">array([[0.81940995, 0.        , 0.57320793],</span>
<span class="go">       [1.        , 0.        , 0.        ],</span>
<span class="go">       [1.        , 0.        , 0.        ],</span>
<span class="go">       [1.        , 0.        , 0.        ],</span>
<span class="go">       [0.47330339, 0.88089948, 0.        ],</span>
<span class="go">       [0.58149261, 0.        , 0.81355169]])</span>
</pre></div>
</div>
<p>Each row is normalized to have unit Euclidean norm:</p>
<p><span class="math notranslate nohighlight">\(v_{norm} = \frac{v}{||v||_2} = \frac{v}{\sqrt{v{_1}^2 +
v{_2}^2 + \dots + v{_n}^2}}\)</span></p>
<p>For example, we can compute the tf-idf of the first term in the first
document in the <cite>counts</cite> array as follows:</p>
<p><span class="math notranslate nohighlight">\(n_{d} = 6\)</span></p>
<p><span class="math notranslate nohighlight">\(\text{df}(d, t)_{\text{term1}} = 6\)</span></p>
<p><span class="math notranslate nohighlight">\(\text{idf}(d, t)_{\text{term1}} =
log \frac{n_d}{\text{df}(d, t)} + 1 = log(1)+1 = 1\)</span></p>
<p><span class="math notranslate nohighlight">\(\text{tf-idf}_{\text{term1}} = \text{tf} \times \text{idf} = 3 \times 1 = 3\)</span></p>
<p>Now, if we repeat this computation for the remaining 2 terms in the document,
we get</p>
<p><span class="math notranslate nohighlight">\(\text{tf-idf}_{\text{term2}} = 0 \times (log(6/1)+1) = 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\text{tf-idf}_{\text{term3}} = 1 \times (log(6/2)+1) \approx 2.0986\)</span></p>
<p>and the vector of raw tf-idfs:</p>
<p><span class="math notranslate nohighlight">\(\text{tf-idf}_{\text{raw}} = [3, 0, 2.0986].\)</span></p>
<p>Then, applying the Euclidean (L2) norm, we obtain the following tf-idfs
for document 1:</p>
<p><span class="math notranslate nohighlight">\(\frac{[3, 0, 2.0986]}{\sqrt{\big(3^2 + 0^2 + 2.0986^2\big)}}
= [ 0.819,  0,  0.573].\)</span></p>
<p>Furthermore, the default parameter <code class="docutils literal notranslate"><span class="pre">smooth_idf=True</span></code> adds “1” to the numerator
and  denominator as if an extra document was seen containing every term in the
collection exactly once, which prevents zero divisions:</p>
<p><span class="math notranslate nohighlight">\(\text{idf}(t) = log{\frac{1 + n_d}{1+\text{df}(d,t)}} + 1\)</span></p>
<p>Using this modification, the tf-idf of the third term in document 1 changes to
1.8473:</p>
<p><span class="math notranslate nohighlight">\(\text{tf-idf}_{\text{term3}} = 1 \times log(7/3)+1 \approx 1.8473\)</span></p>
<p>And the L2-normalized tf-idf changes to</p>
<p><span class="math notranslate nohighlight">\(\frac{[3, 0, 1.8473]}{\sqrt{\big(3^2 + 0^2 + 1.8473^2\big)}}
= [0.8515, 0, 0.5243]\)</span>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span> <span class="o">=</span> <span class="n">TfidfTransformer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="go">array([[0.85151335, 0.        , 0.52433293],</span>
<span class="go">       [1.        , 0.        , 0.        ],</span>
<span class="go">       [1.        , 0.        , 0.        ],</span>
<span class="go">       [1.        , 0.        , 0.        ],</span>
<span class="go">       [0.55422893, 0.83236428, 0.        ],</span>
<span class="go">       [0.63035731, 0.        , 0.77630514]])</span>
</pre></div>
</div>
<p>The weights of each
feature computed by the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method call are stored in a model
attribute:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">transformer</span><span class="o">.</span><span class="n">idf_</span>                       
<span class="go">array([1. ..., 2.25..., 1.84...])</span>
</pre></div>
</div>
<p>As tf–idf is very often used for text features, there is also another
class called <a class="reference internal" href="generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" title="sklearn.feature_extraction.text.TfidfVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TfidfVectorizer</span></code></a> that combines all the options of
<a class="reference internal" href="generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">CountVectorizer</span></code></a> and <a class="reference internal" href="generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer" title="sklearn.feature_extraction.text.TfidfTransformer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TfidfTransformer</span></code></a> in a single model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="k">import</span> <span class="n">TfidfVectorizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="gp">... </span>                               
<span class="go">&lt;4x9 sparse matrix of type &#39;&lt;... &#39;numpy.float64&#39;&gt;&#39;</span>
<span class="go">    with 19 stored elements in Compressed Sparse ... format&gt;</span>
</pre></div>
</div>
<p>While the tf–idf normalization is often very useful, there might
be cases where the binary occurrence markers might offer better
features. This can be achieved by using the <code class="docutils literal notranslate"><span class="pre">binary</span></code> parameter
of <a class="reference internal" href="generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">CountVectorizer</span></code></a>. In particular, some estimators such as
<a class="reference internal" href="naive_bayes.html#bernoulli-naive-bayes"><span class="std std-ref">伯努利朴素贝叶斯</span></a> explicitly model discrete boolean random
variables. Also, very short texts are likely to have noisy tf–idf values
while the binary occurrence info is more stable.</p>
<p>As usual the best way to adjust the feature extraction parameters
is to use a cross-validated grid search, for instance by pipelining the
feature extractor with a classifier:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="../auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py"><span class="std std-ref">Sample pipeline for text feature extraction and evaluation</span></a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="id11">
<h3>4.2.3.5. 解码文本文件<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<p>Text is made of characters, but files are made of bytes. These bytes represent
characters according to some <em>encoding</em>. To work with text files in Python,
their bytes must be <em>decoded</em> to a character set called Unicode.
Common encodings are ASCII, Latin-1 (Western Europe), KOI8-R (Russian)
and the universal encodings UTF-8 and UTF-16. Many others exist.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">An encoding can also be called a ‘character set’,
but this term is less accurate: several encodings can exist
for a single character set.</p>
</div>
<p>The text feature extractors in scikit-learn know how to decode text files,
but only if you tell them what encoding the files are in.
The <a class="reference internal" href="generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">CountVectorizer</span></code></a> takes an <code class="docutils literal notranslate"><span class="pre">encoding</span></code> parameter for this purpose.
For modern text files, the correct encoding is probably UTF-8,
which is therefore the default (<code class="docutils literal notranslate"><span class="pre">encoding=&quot;utf-8&quot;</span></code>).</p>
<p>If the text you are loading is not actually encoded with UTF-8, however,
you will get a <code class="docutils literal notranslate"><span class="pre">UnicodeDecodeError</span></code>.
The vectorizers can be told to be silent about decoding errors
by setting the <code class="docutils literal notranslate"><span class="pre">decode_error</span></code> parameter to either <code class="docutils literal notranslate"><span class="pre">&quot;ignore&quot;</span></code>
or <code class="docutils literal notranslate"><span class="pre">&quot;replace&quot;</span></code>. See the documentation for the Python function
<code class="docutils literal notranslate"><span class="pre">bytes.decode</span></code> for more details
(type <code class="docutils literal notranslate"><span class="pre">help(bytes.decode)</span></code> at the Python prompt).</p>
<p>If you are having trouble decoding text, here are some things to try:</p>
<ul class="simple">
<li>Find out what the actual encoding of the text is. The file might come
with a header or README that tells you the encoding, or there might be some
standard encoding you can assume based on where the text comes from.</li>
<li>You may be able to find out what kind of encoding it is in general
using the UNIX command <code class="docutils literal notranslate"><span class="pre">file</span></code>. The Python <code class="docutils literal notranslate"><span class="pre">chardet</span></code> module comes with
a script called <code class="docutils literal notranslate"><span class="pre">chardetect.py</span></code> that will guess the specific encoding,
though you cannot rely on its guess being correct.</li>
<li>You could try UTF-8 and disregard the errors. You can decode byte
strings with <code class="docutils literal notranslate"><span class="pre">bytes.decode(errors='replace')</span></code> to replace all
decoding errors with a meaningless character, or set
<code class="docutils literal notranslate"><span class="pre">decode_error='replace'</span></code> in the vectorizer. This may damage the
usefulness of your features.</li>
<li>Real text may come from a variety of sources that may have used different
encodings, or even be sloppily decoded in a different encoding than the
one it was encoded with. This is common in text retrieved from the Web.
The Python package <a class="reference external" href="https://github.com/LuminosoInsight/python-ftfy">ftfy</a> can automatically sort out some classes of
decoding errors, so you could try decoding the unknown text as <code class="docutils literal notranslate"><span class="pre">latin-1</span></code>
and then using <code class="docutils literal notranslate"><span class="pre">ftfy</span></code> to fix errors.</li>
<li>If the text is in a mish-mash of encodings that is simply too hard to sort
out (which is the case for the 20 Newsgroups dataset), you can fall back on
a simple single-byte encoding such as <code class="docutils literal notranslate"><span class="pre">latin-1</span></code>. Some text may display
incorrectly, but at least the same sequence of bytes will always represent
the same feature.</li>
</ul>
<p>For example, the following snippet uses <code class="docutils literal notranslate"><span class="pre">chardet</span></code>
(not shipped with scikit-learn, must be installed separately)
to figure out the encoding of three texts.
It then vectorizes the texts and prints the learned vocabulary.
The output is not shown here.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">chardet</span>    <span class="c1"># doctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text1</span> <span class="o">=</span> <span class="sa">b</span><span class="s2">&quot;Sei mir gegr</span><span class="se">\xc3\xbc\xc3\x9f</span><span class="s2">t mein Sauerkraut&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text2</span> <span class="o">=</span> <span class="sa">b</span><span class="s2">&quot;holdselig sind deine Ger</span><span class="se">\xfc</span><span class="s2">che&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text3</span> <span class="o">=</span> <span class="sa">b</span><span class="s2">&quot;</span><span class="se">\xff\xfe</span><span class="s2">A</span><span class="se">\x00</span><span class="s2">u</span><span class="se">\x00</span><span class="s2">f</span><span class="se">\x00</span><span class="s2"> </span><span class="se">\x00</span><span class="s2">F</span><span class="se">\x00</span><span class="s2">l</span><span class="se">\x00\xfc\x00</span><span class="s2">g</span><span class="se">\x00</span><span class="s2">e</span><span class="se">\x00</span><span class="s2">l</span><span class="se">\x00</span><span class="s2">n</span><span class="se">\x00</span><span class="s2"> </span><span class="se">\x00</span><span class="s2">d</span><span class="se">\x00</span><span class="s2">e</span><span class="se">\x00</span><span class="s2">s</span><span class="se">\x00</span><span class="s2"> </span><span class="se">\x00</span><span class="s2">G</span><span class="se">\x00</span><span class="s2">e</span><span class="se">\x00</span><span class="s2">s</span><span class="se">\x00</span><span class="s2">a</span><span class="se">\x00</span><span class="s2">n</span><span class="se">\x00</span><span class="s2">g</span><span class="se">\x00</span><span class="s2">e</span><span class="se">\x00</span><span class="s2">s</span><span class="se">\x00</span><span class="s2">,</span><span class="se">\x00</span><span class="s2"> </span><span class="se">\x00</span><span class="s2">H</span><span class="se">\x00</span><span class="s2">e</span><span class="se">\x00</span><span class="s2">r</span><span class="se">\x00</span><span class="s2">z</span><span class="se">\x00</span><span class="s2">l</span><span class="se">\x00</span><span class="s2">i</span><span class="se">\x00</span><span class="s2">e</span><span class="se">\x00</span><span class="s2">b</span><span class="se">\x00</span><span class="s2">c</span><span class="se">\x00</span><span class="s2">h</span><span class="se">\x00</span><span class="s2">e</span><span class="se">\x00</span><span class="s2">n</span><span class="se">\x00</span><span class="s2">,</span><span class="se">\x00</span><span class="s2"> </span><span class="se">\x00</span><span class="s2">t</span><span class="se">\x00</span><span class="s2">r</span><span class="se">\x00</span><span class="s2">a</span><span class="se">\x00</span><span class="s2">g</span><span class="se">\x00</span><span class="s2"> </span><span class="se">\x00</span><span class="s2">i</span><span class="se">\x00</span><span class="s2">c</span><span class="se">\x00</span><span class="s2">h</span><span class="se">\x00</span><span class="s2"> </span><span class="se">\x00</span><span class="s2">d</span><span class="se">\x00</span><span class="s2">i</span><span class="se">\x00</span><span class="s2">c</span><span class="se">\x00</span><span class="s2">h</span><span class="se">\x00</span><span class="s2"> </span><span class="se">\x00</span><span class="s2">f</span><span class="se">\x00</span><span class="s2">o</span><span class="se">\x00</span><span class="s2">r</span><span class="se">\x00</span><span class="s2">t</span><span class="se">\x00</span><span class="s2">&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoded</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">chardet</span><span class="o">.</span><span class="n">detect</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="s1">&#39;encoding&#39;</span><span class="p">])</span>
<span class="gp">... </span>           <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">,</span> <span class="n">text3</span><span class="p">)]</span>        <span class="c1"># doctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">decoded</span><span class="p">)</span><span class="o">.</span><span class="n">vocabulary_</span>    <span class="c1"># doctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">term</span> <span class="ow">in</span> <span class="n">v</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>                           <span class="c1"># doctest: +SKIP</span>
</pre></div>
</div>
<p>(Depending on the version of <code class="docutils literal notranslate"><span class="pre">chardet</span></code>, it might get the first one wrong.)</p>
<p>For an introduction to Unicode and character encodings in general,
see Joel Spolsky’s <a class="reference external" href="http://www.joelonsoftware.com/articles/Unicode.html">Absolute Minimum Every Software Developer Must Know
About Unicode</a>.</p>
</div>
<div class="section" id="id12">
<h3>4.2.3.6. 应用和案例<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h3>
<p>The bag of words representation is quite simplistic but surprisingly
useful in practice.</p>
<p>In particular in a <strong>supervised setting</strong> it can be successfully combined
with fast and scalable linear models to train <strong>document classifiers</strong>,
for instance:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="../auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py"><span class="std std-ref">Classification of text documents using sparse features</span></a></li>
</ul>
</div></blockquote>
<p>In an <strong>unsupervised setting</strong> it can be used to group similar documents
together by applying clustering algorithms such as <a class="reference internal" href="clustering.html#k-means"><span class="std std-ref">K-均值(K-means)</span></a>:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="../auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py"><span class="std std-ref">Clustering text documents using k-means</span></a></li>
</ul>
</div></blockquote>
<p>Finally it is possible to discover the main topics of a corpus by
relaxing the hard assignment constraint of clustering, for instance by
using <a class="reference internal" href="decomposition.html#nmf"><span class="std std-ref">非负矩阵分解 (NMF or NNMF)</span></a>:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="../auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py"><span class="std std-ref">Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation</span></a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="id13">
<h3>4.2.3.7. 词袋表示法的局限性<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h3>
<p>A collection of unigrams (what bag of words is) cannot capture phrases
and multi-word expressions, effectively disregarding any word order
dependence. Additionally, the bag of words model doesn’t account for potential
misspellings or word derivations.</p>
<p>N-grams to the rescue! Instead of building a simple collection of
unigrams (n=1), one might prefer a collection of bigrams (n=2), where
occurrences of pairs of consecutive words are counted.</p>
<p>One might alternatively consider a collection of character n-grams, a
representation resilient against misspellings and derivations.</p>
<p>For example, let’s say we’re dealing with a corpus of two documents:
<code class="docutils literal notranslate"><span class="pre">['words',</span> <span class="pre">'wprds']</span></code>. The second document contains a misspelling
of the word ‘words’.
A simple bag of words representation would consider these two as
very distinct documents, differing in both of the two possible features.
A character 2-gram representation, however, would find the documents
matching in 4 out of 8 features, which may help the preferred classifier
decide better:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">analyzer</span><span class="o">=</span><span class="s1">&#39;char_wb&#39;</span><span class="p">,</span> <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">counts</span> <span class="o">=</span> <span class="n">ngram_vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([</span><span class="s1">&#39;words&#39;</span><span class="p">,</span> <span class="s1">&#39;wprds&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="s1">&#39; w&#39;</span><span class="p">,</span> <span class="s1">&#39;ds&#39;</span><span class="p">,</span> <span class="s1">&#39;or&#39;</span><span class="p">,</span> <span class="s1">&#39;pr&#39;</span><span class="p">,</span> <span class="s1">&#39;rd&#39;</span><span class="p">,</span> <span class="s1">&#39;s &#39;</span><span class="p">,</span> <span class="s1">&#39;wo&#39;</span><span class="p">,</span> <span class="s1">&#39;wp&#39;</span><span class="p">])</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">counts</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="go">array([[1, 1, 1, 0, 1, 1, 1, 0],</span>
<span class="go">       [1, 1, 0, 1, 1, 1, 0, 1]])</span>
</pre></div>
</div>
<p>In the above example, <code class="docutils literal notranslate"><span class="pre">char_wb</span></code> analyzer is used, which creates n-grams
only from characters inside word boundaries (padded with space on each
side). The <code class="docutils literal notranslate"><span class="pre">char</span></code> analyzer, alternatively, creates n-grams that
span across words:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">analyzer</span><span class="o">=</span><span class="s1">&#39;char_wb&#39;</span><span class="p">,</span> <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([</span><span class="s1">&#39;jumpy fox&#39;</span><span class="p">])</span>
<span class="gp">... </span>                               
<span class="go">&lt;1x4 sparse matrix of type &#39;&lt;... &#39;numpy.int64&#39;&gt;&#39;</span>
<span class="go">   with 4 stored elements in Compressed Sparse ... format&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="s1">&#39; fox &#39;</span><span class="p">,</span> <span class="s1">&#39; jump&#39;</span><span class="p">,</span> <span class="s1">&#39;jumpy&#39;</span><span class="p">,</span> <span class="s1">&#39;umpy &#39;</span><span class="p">])</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">analyzer</span><span class="o">=</span><span class="s1">&#39;char&#39;</span><span class="p">,</span> <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([</span><span class="s1">&#39;jumpy fox&#39;</span><span class="p">])</span>
<span class="gp">... </span>                               
<span class="go">&lt;1x5 sparse matrix of type &#39;&lt;... &#39;numpy.int64&#39;&gt;&#39;</span>
<span class="go">    with 5 stored elements in Compressed Sparse ... format&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ngram_vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="s1">&#39;jumpy&#39;</span><span class="p">,</span> <span class="s1">&#39;mpy f&#39;</span><span class="p">,</span> <span class="s1">&#39;py fo&#39;</span><span class="p">,</span> <span class="s1">&#39;umpy &#39;</span><span class="p">,</span> <span class="s1">&#39;y fox&#39;</span><span class="p">])</span>
<span class="go">True</span>
</pre></div>
</div>
<p>The word boundaries-aware variant <code class="docutils literal notranslate"><span class="pre">char_wb</span></code> is especially interesting
for languages that use white-spaces for word separation as it generates
significantly less noisy features than the raw <code class="docutils literal notranslate"><span class="pre">char</span></code> variant in
that case. For such languages it can increase both the predictive
accuracy and convergence speed of classifiers trained using such
features while retaining the robustness with regards to misspellings and
word derivations.</p>
<p>While some local positioning information can be preserved by extracting
n-grams instead of individual words, bag of words and bag of n-grams
destroy most of the inner structure of the document and hence most of
the meaning carried by that internal structure.</p>
<p>In order to address the wider task of Natural Language Understanding,
the local structure of sentences and paragraphs should thus be taken
into account. Many such models will thus be casted as “Structured output”
problems which are currently outside of the scope of scikit-learn.</p>
</div>
<div class="section" id="hashing-vectorizer">
<span id="id14"></span><h3>4.2.3.8. 用散列技巧矢量化大型语料库<a class="headerlink" href="#hashing-vectorizer" title="Permalink to this headline">¶</a></h3>
<p>The above vectorization scheme is simple but the fact that it holds an <strong>in-
memory mapping from the string tokens to the integer feature indices</strong> (the
<code class="docutils literal notranslate"><span class="pre">vocabulary_</span></code> attribute) causes several <strong>problems when dealing with large
datasets</strong>:</p>
<ul class="simple">
<li>the larger the corpus, the larger the vocabulary will grow and hence the
memory use too,</li>
<li>fitting requires the allocation of intermediate data structures
of size proportional to that of the original dataset.</li>
<li>building the word-mapping requires a full pass over the dataset hence it is
not possible to fit text classifiers in a strictly online manner.</li>
<li>pickling and un-pickling vectorizers with a large <code class="docutils literal notranslate"><span class="pre">vocabulary_</span></code> can be very
slow (typically much slower than pickling / un-pickling flat data structures
such as a NumPy array of the same size),</li>
<li>it is not easily possible to split the vectorization work into concurrent sub
tasks as the <code class="docutils literal notranslate"><span class="pre">vocabulary_</span></code> attribute would have to be a shared state with a
fine grained synchronization barrier: the mapping from token string to
feature index is dependent on ordering of the first occurrence of each token
hence would have to be shared, potentially harming the concurrent workers’
performance to the point of making them slower than the sequential variant.</li>
</ul>
<p>It is possible to overcome those limitations by combining the “hashing trick”
(<a class="reference internal" href="#feature-hashing"><span class="std std-ref">特征哈希(散列)化</span></a>) implemented by the
<a class="reference internal" href="generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.feature_extraction.FeatureHasher</span></code></a> class and the text
preprocessing and tokenization features of the <a class="reference internal" href="generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">CountVectorizer</span></code></a>.</p>
<p>This combination is implementing in <a class="reference internal" href="generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">HashingVectorizer</span></code></a>,
a transformer class that is mostly API compatible with <a class="reference internal" href="generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">CountVectorizer</span></code></a>.
<a class="reference internal" href="generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">HashingVectorizer</span></code></a> is stateless,
meaning that you don’t have to call <code class="docutils literal notranslate"><span class="pre">fit</span></code> on it:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="k">import</span> <span class="n">HashingVectorizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hv</span> <span class="o">=</span> <span class="n">HashingVectorizer</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hv</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="gp">... </span>                               
<span class="go">&lt;4x10 sparse matrix of type &#39;&lt;... &#39;numpy.float64&#39;&gt;&#39;</span>
<span class="go">    with 16 stored elements in Compressed Sparse ... format&gt;</span>
</pre></div>
</div>
<p>You can see that 16 non-zero feature tokens were extracted in the vector
output: this is less than the 19 non-zeros extracted previously by the
<a class="reference internal" href="generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">CountVectorizer</span></code></a> on the same toy corpus. The discrepancy comes from
hash function collisions because of the low value of the <code class="docutils literal notranslate"><span class="pre">n_features</span></code> parameter.</p>
<p>In a real world setting, the <code class="docutils literal notranslate"><span class="pre">n_features</span></code> parameter can be left to its
default value of <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">**</span> <span class="pre">20</span></code> (roughly one million possible features). If memory
or downstream models size is an issue selecting a lower value such as <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">**</span>
<span class="pre">18</span></code> might help without introducing too many additional collisions on typical
text classification tasks.</p>
<p>Note that the dimensionality does not affect the CPU training time of
algorithms which operate on CSR matrices (<code class="docutils literal notranslate"><span class="pre">LinearSVC(dual=True)</span></code>,
<code class="docutils literal notranslate"><span class="pre">Perceptron</span></code>, <code class="docutils literal notranslate"><span class="pre">SGDClassifier</span></code>, <code class="docutils literal notranslate"><span class="pre">PassiveAggressive</span></code>) but it does for
algorithms that work with CSC matrices (<code class="docutils literal notranslate"><span class="pre">LinearSVC(dual=False)</span></code>, <code class="docutils literal notranslate"><span class="pre">Lasso()</span></code>,
etc).</p>
<p>Let’s try again with the default setting:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">hv</span> <span class="o">=</span> <span class="n">HashingVectorizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hv</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="gp">... </span>                              
<span class="go">&lt;4x1048576 sparse matrix of type &#39;&lt;... &#39;numpy.float64&#39;&gt;&#39;</span>
<span class="go">    with 19 stored elements in Compressed Sparse ... format&gt;</span>
</pre></div>
</div>
<p>We no longer get the collisions, but this comes at the expense of a much larger
dimensionality of the output space.
Of course, other terms than the 19 used here
might still collide with each other.</p>
<p>The <a class="reference internal" href="generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">HashingVectorizer</span></code></a> also comes with the following limitations:</p>
<ul class="simple">
<li>it is not possible to invert the model (no <code class="docutils literal notranslate"><span class="pre">inverse_transform</span></code> method),
nor to access the original string representation of the features,
because of the one-way nature of the hash function that performs the mapping.</li>
<li>it does not provide IDF weighting as that would introduce statefulness in the
model. A <a class="reference internal" href="generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer" title="sklearn.feature_extraction.text.TfidfTransformer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TfidfTransformer</span></code></a> can be appended to it in a pipeline if
required.</li>
</ul>
</div>
<div class="section" id="hashingvectorizer-scaling">
<h3>4.2.3.9. 使用 HashingVectorizer 执行核外scaling<a class="headerlink" href="#hashingvectorizer-scaling" title="Permalink to this headline">¶</a></h3>
<p>An interesting development of using a <a class="reference internal" href="generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">HashingVectorizer</span></code></a> is the ability
to perform <a class="reference external" href="https://en.wikipedia.org/wiki/Out-of-core_algorithm">out-of-core</a> scaling. This means that we can learn from data that
does not fit into the computer’s main memory.</p>
<p>A strategy to implement out-of-core scaling is to stream data to the estimator
in mini-batches. Each mini-batch is vectorized using <a class="reference internal" href="generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">HashingVectorizer</span></code></a>
so as to guarantee that the input space of the estimator has always the same
dimensionality. The amount of memory used at any time is thus bounded by the
size of a mini-batch. Although there is no limit to the amount of data that can
be ingested using such an approach, from a practical point of view the learning
time is often limited by the CPU time one wants to spend on the task.</p>
<p>For a full-fledged example of out-of-core scaling in a text classification
task see <a class="reference internal" href="../auto_examples/applications/plot_out_of_core_classification.html#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py"><span class="std std-ref">Out-of-core classification of text documents</span></a>.</p>
</div>
<div class="section" id="customizing-the-vectorizer-classes">
<h3>4.2.3.10. Customizing the vectorizer classes<a class="headerlink" href="#customizing-the-vectorizer-classes" title="Permalink to this headline">¶</a></h3>
<p>It is possible to customize the behavior by passing a callable
to the vectorizer constructor:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">my_tokenizer</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">my_tokenizer</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vectorizer</span><span class="o">.</span><span class="n">build_analyzer</span><span class="p">()(</span><span class="sa">u</span><span class="s2">&quot;Some... punctuation!&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="s1">&#39;some...&#39;</span><span class="p">,</span> <span class="s1">&#39;punctuation!&#39;</span><span class="p">])</span>
<span class="go">True</span>
</pre></div>
</div>
<p>In particular we name:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">preprocessor</span></code>: a callable that takes an entire document as input (as a
single string), and returns a possibly transformed version of the document,
still as an entire string. This can be used to remove HTML tags, lowercase
the entire document, etc.</li>
<li><code class="docutils literal notranslate"><span class="pre">tokenizer</span></code>: a callable that takes the output from the preprocessor
and splits it into tokens, then returns a list of these.</li>
<li><code class="docutils literal notranslate"><span class="pre">analyzer</span></code>: a callable that replaces the preprocessor and tokenizer.
The default analyzers all call the preprocessor and tokenizer, but custom
analyzers will skip this. N-gram extraction and stop word filtering take
place at the analyzer level, so a custom analyzer may have to reproduce
these steps.</li>
</ul>
</div></blockquote>
<p>(Lucene users might recognize these names, but be aware that scikit-learn
concepts may not map one-to-one onto Lucene concepts.)</p>
<p>To make the preprocessor, tokenizer and analyzers aware of the model
parameters it is possible to derive from the class and override the
<code class="docutils literal notranslate"><span class="pre">build_preprocessor</span></code>, <code class="docutils literal notranslate"><span class="pre">build_tokenizer</span></code> and <code class="docutils literal notranslate"><span class="pre">build_analyzer</span></code>
factory methods instead of passing custom functions.</p>
<p>一些经验和技巧:</p>
<blockquote>
<div><ul>
<li><p class="first">If documents are pre-tokenized by an external package, then store them in
files (or strings) with the tokens separated by whitespace and pass
<code class="docutils literal notranslate"><span class="pre">analyzer=str.split</span></code></p>
</li>
<li><p class="first">Fancy token-level analysis such as stemming, lemmatizing, compound
splitting, filtering based on part-of-speech, etc. are not included in the
scikit-learn codebase, but can be added by customizing either the
tokenizer or the analyzer.
Here’s a <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code> with a tokenizer and lemmatizer using
<a class="reference external" href="http://www.nltk.org">NLTK</a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk</span> <span class="k">import</span> <span class="n">word_tokenize</span>          
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="k">import</span> <span class="n">WordNetLemmatizer</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">LemmaTokenizer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">wnl</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">doc</span><span class="p">):</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">wnl</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">doc</span><span class="p">)]</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">LemmaTokenizer</span><span class="p">())</span>  
</pre></div>
</div>
<p>(Note that this will not filter out punctuation.)</p>
<p>The following example will, for instance, transform some British spelling
to American spelling:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">re</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">to_british</span><span class="p">(</span><span class="n">tokens</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">:</span>
<span class="gp">... </span>        <span class="n">t</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;(...)our$&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;\1or&quot;</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">t</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;([bt])re$&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;\1er&quot;</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">t</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;([iy])s(e$|ing|ation)&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;\1z\2&quot;</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">t</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;ogue$&quot;</span><span class="p">,</span> <span class="s2">&quot;og&quot;</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">yield</span> <span class="n">t</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">CustomVectorizer</span><span class="p">(</span><span class="n">CountVectorizer</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">build_tokenizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">tokenize</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">CustomVectorizer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">build_tokenizer</span><span class="p">()</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="k">lambda</span> <span class="n">doc</span><span class="p">:</span> <span class="nb">list</span><span class="p">(</span><span class="n">to_british</span><span class="p">(</span><span class="n">tokenize</span><span class="p">(</span><span class="n">doc</span><span class="p">)))</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">CustomVectorizer</span><span class="p">()</span><span class="o">.</span><span class="n">build_analyzer</span><span class="p">()(</span><span class="sa">u</span><span class="s2">&quot;color colour&quot;</span><span class="p">))</span> 
<span class="go">[...&#39;color&#39;, ...&#39;color&#39;]</span>
</pre></div>
</div>
<p>for other styles of preprocessing; examples include stemming, lemmatization,
or normalizing numerical tokens, with the latter illustrated in:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="../auto_examples/bicluster/plot_bicluster_newsgroups.html#sphx-glr-auto-examples-bicluster-plot-bicluster-newsgroups-py"><span class="std std-ref">Biclustering documents with the Spectral Co-clustering algorithm</span></a></li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
<p>Customizing the vectorizer can also be useful when handling Asian languages
that do not use an explicit word separator such as whitespace.</p>
</div>
</div>
<div class="section" id="image-feature-extraction">
<span id="id15"></span><h2>4.2.4. 图像特征提取<a class="headerlink" href="#image-feature-extraction" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id16">
<h3>4.2.4.1. 图像块提取<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h3>
<p>The <a class="reference internal" href="generated/sklearn.feature_extraction.image.extract_patches_2d.html#sklearn.feature_extraction.image.extract_patches_2d" title="sklearn.feature_extraction.image.extract_patches_2d"><code class="xref py py-func docutils literal notranslate"><span class="pre">extract_patches_2d</span></code></a> function extracts patches from an image stored
as a two-dimensional array, or three-dimensional with color information along
the third axis. For rebuilding an image from all its patches, use
<a class="reference internal" href="generated/sklearn.feature_extraction.image.reconstruct_from_patches_2d.html#sklearn.feature_extraction.image.reconstruct_from_patches_2d" title="sklearn.feature_extraction.image.reconstruct_from_patches_2d"><code class="xref py py-func docutils literal notranslate"><span class="pre">reconstruct_from_patches_2d</span></code></a>. For example let use generate a 4x4 pixel
picture with 3 color channels (e.g. in RGB format):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction</span> <span class="k">import</span> <span class="n">image</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">one_image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">one_image</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># R channel of a fake RGB picture</span>
<span class="go">array([[ 0,  3,  6,  9],</span>
<span class="go">       [12, 15, 18, 21],</span>
<span class="go">       [24, 27, 30, 33],</span>
<span class="go">       [36, 39, 42, 45]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">patches</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">extract_patches_2d</span><span class="p">(</span><span class="n">one_image</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">max_patches</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">patches</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(2, 2, 2, 3)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">patches</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="go">array([[[ 0,  3],</span>
<span class="go">        [12, 15]],</span>

<span class="go">       [[15, 18],</span>
<span class="go">        [27, 30]]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">patches</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">extract_patches_2d</span><span class="p">(</span><span class="n">one_image</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">patches</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(9, 2, 2, 3)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">patches</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="go">array([[15, 18],</span>
<span class="go">       [27, 30]])</span>
</pre></div>
</div>
<p>Let us now try to reconstruct the original image from the patches by averaging
on overlapping areas:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">reconstructed</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">reconstruct_from_patches_2d</span><span class="p">(</span><span class="n">patches</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_array_equal</span><span class="p">(</span><span class="n">one_image</span><span class="p">,</span> <span class="n">reconstructed</span><span class="p">)</span>
</pre></div>
</div>
<p>The <a class="reference internal" href="generated/sklearn.feature_extraction.image.PatchExtractor.html#sklearn.feature_extraction.image.PatchExtractor" title="sklearn.feature_extraction.image.PatchExtractor"><code class="xref py py-class docutils literal notranslate"><span class="pre">PatchExtractor</span></code></a> class works in the same way as
<a class="reference internal" href="generated/sklearn.feature_extraction.image.extract_patches_2d.html#sklearn.feature_extraction.image.extract_patches_2d" title="sklearn.feature_extraction.image.extract_patches_2d"><code class="xref py py-func docutils literal notranslate"><span class="pre">extract_patches_2d</span></code></a>, only it supports multiple images as input. It is
implemented as an estimator, so it can be used in pipelines. See:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">five_images</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">patches</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">PatchExtractor</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">five_images</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">patches</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(45, 2, 2, 3)</span>
</pre></div>
</div>
</div>
<div class="section" id="id17">
<h3>4.2.4.2. 图像的连接图<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h3>
<p>Several estimators in the scikit-learn can use connectivity information between
features or samples. For instance Ward clustering
(<a class="reference internal" href="clustering.html#hierarchical-clustering"><span class="std std-ref">层次聚类(Hierarchical clustering)</span></a>) can cluster together only neighboring pixels
of an image, thus forming contiguous patches:</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/cluster/plot_coin_ward_segmentation.html"><img alt="../images/sphx_glr_plot_coin_ward_segmentation_0011.png" src="../images/sphx_glr_plot_coin_ward_segmentation_0011.png" style="width: 200.0px; height: 200.0px;" /></a>
</div>
<p>For this purpose, the estimators use a ‘connectivity’ matrix, giving
which samples are connected.</p>
<p>The function <a class="reference internal" href="generated/sklearn.feature_extraction.image.img_to_graph.html#sklearn.feature_extraction.image.img_to_graph" title="sklearn.feature_extraction.image.img_to_graph"><code class="xref py py-func docutils literal notranslate"><span class="pre">img_to_graph</span></code></a> returns such a matrix from a 2D or 3D
image. Similarly, <a class="reference internal" href="generated/sklearn.feature_extraction.image.grid_to_graph.html#sklearn.feature_extraction.image.grid_to_graph" title="sklearn.feature_extraction.image.grid_to_graph"><code class="xref py py-func docutils literal notranslate"><span class="pre">grid_to_graph</span></code></a> build a connectivity matrix for
images given the shape of these image.</p>
<p>These matrices can be used to impose connectivity in estimators that use
connectivity information, such as Ward clustering
(<a class="reference internal" href="clustering.html#hierarchical-clustering"><span class="std std-ref">层次聚类(Hierarchical clustering)</span></a>), but also to build precomputed kernels,
or similarity matrices.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p><strong>案例</strong></p>
<ul class="last simple">
<li><a class="reference internal" href="../auto_examples/cluster/plot_coin_ward_segmentation.html#sphx-glr-auto-examples-cluster-plot-coin-ward-segmentation-py"><span class="std std-ref">A demo of structured Ward hierarchical clustering on an image of coins</span></a></li>
<li><a class="reference internal" href="../auto_examples/cluster/plot_segmentation_toy.html#sphx-glr-auto-examples-cluster-plot-segmentation-toy-py"><span class="std std-ref">Spectral clustering for image segmentation</span></a></li>
<li><a class="reference internal" href="../auto_examples/cluster/plot_feature_agglomeration_vs_univariate_selection.html#sphx-glr-auto-examples-cluster-plot-feature-agglomeration-vs-univariate-selection-py"><span class="std std-ref">Feature agglomeration vs. univariate selection</span></a></li>
</ul>
</div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer">
        &copy; 2007 - 2018, scikit-learn developers (BSD License).
      <a href="../_sources/modules/feature_extraction.rst.txt" rel="nofollow">Show this page source</a>
    </div>
     <div class="rel">
    
    <div class="buttonPrevious">
      <a href="compose.html">Previous
      </a>
    </div>
    <div class="buttonNext">
      <a href="preprocessing.html">Next
      </a>
    </div>
    
     </div>

    
    <script>
        window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
        ga('create', 'UA-22606712-2', 'auto');
        ga('set', 'anonymizeIp', true);
        ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    
    <script>
      (function() {
        var cx = '016639176250731907682:tjtqbvtvij0';
        var gcse = document.createElement('script'); gcse.type = 'text/javascript'; gcse.async = true;
        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s);
      })();
    </script>
  </body>
</html>