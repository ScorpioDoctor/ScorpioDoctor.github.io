

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

    <title>1.6. 最近邻方法(Nearest Neighbors) &#8212; scikit-learn 0.20.2 documentation</title>
<!-- htmltitle is before nature.css - we use this hack to load bootstrap first -->
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="../static/css/bootstrap.min.css" media="screen" />
<link rel="stylesheet" href="../static/css/bootstrap-responsive.css" />

    <link rel="stylesheet" href="../static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../static/gallery.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../static/documentation_options.js"></script>
    <script type="text/javascript" src="../static/jquery.js"></script>
    <script type="text/javascript" src="../static/underscore.js"></script>
    <script type="text/javascript" src="../static/doctools.js"></script>
    <script type="text/javascript" src="../static/js/copybutton.js"></script>
    <script type="text/javascript" src="../static/js/extra.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_SVG"></script>
    <link rel="shortcut icon" href="../static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1.7. 高斯过程(Gaussian Processes)" href="gaussian_process.html" />
    <link rel="prev" title="1.5. 随机梯度下降(Stochastic Gradient Descent)" href="sgd.html" />


<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<script src="../static/js/bootstrap.min.js" type="text/javascript"></script>
<script>
  VERSION_SUBDIR = (function (groups) {
    return groups ? groups[1] : null;
  })(location.href.match(/^https?:\/\/scikit-learn.org\/([^\/]+)/));
</script>
<link rel="canonical" href="http://scikit-learn.org/stable/modules/neighbors.html" />

<script type="text/javascript">
  $("div.buttonNext, div.buttonPrevious").hover(
    function () {
      $(this).css('background-color', '#FF9C34');
    },
    function () {
      $(this).css('background-color', '#A7D6E2');
    }
  );
  function showMenu() {
    var topNav = document.getElementById("scikit-navbar");
    if (topNav.className === "navbar") {
      topNav.className += " responsive";
    } else {
      topNav.className = "navbar";
    }
  };
</script>

<!-- 百度站长统计代码 -->
<script>
  var _hmt = _hmt || [];
  (function () {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?e7836e37a4cb7584127a787e9b44e3f1";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
  })();
</script>


  </head><body>

<div class="header-wrapper">
  <div class="header">
    <p class="logo"><a href="../index.html">
        <img src="../static/scikit-learn-logo-small.png" alt="Logo" />
      </a>
    </p><div class="navbar" id="scikit-navbar">
      <ul>
        <li><a href="../index.html">首页</a></li>
        <li><a href="../install.html">安装</a></li>
        <li class="btn-li">
          <div class="btn-group">
            <a href="../documentation.html">文档</a>
            <a class="btn dropdown-toggle" data-toggle="dropdown">
              <span class="caret"></span>
            </a>
            <ul class="dropdown-menu">
              <li class="link-title">Scikit-learn
                <script>document.write(DOCUMENTATION_OPTIONS.VERSION + (VERSION_SUBDIR ? " (" + VERSION_SUBDIR + ")" : ""));</script>
              </li>
              <li><a href="../tutorial/index.html">教程</a></li>
              <li><a href="../user_guide.html">用户指南</a></li>
              <li><a href="classes.html">API</a></li>
              <li><a href="../glossary.html">词汇表</a></li>
              <li><a href="../faq.html">FAQ</a></li>
              <li><a href="../developers/contributing.html">贡献</a></li>
              <li><a href="../roadmap.html">路线图</a></li>
              <li class="divider"></li>
              <script>if (VERSION_SUBDIR != "stable") document.write('<li><a href="https://www.studyai.cn">稳定版</a></li>')</script>
              <script>if (VERSION_SUBDIR != "dev") document.write('<li><a href="http://scikit-learn.org/dev/documentation.html" target="_blank">开发版</a></li>')</script>
              <li><a href="http://scikit-learn.org/dev/versions.html">所有可用版本</a></li>
              <li><a href="../downloads/scikit-learn-docs.pdf">PDF 文档</a></li>
            </ul>
          </div>
        </li>
        <li><a href="../auto_examples/index.html">案例</a></li>
      </ul>
      <a href="javascript:void(0);" onclick="showMenu()">
        <div class="nav-icon">
          <div class="hamburger-line"></div>
          <div class="hamburger-line"></div>
          <div class="hamburger-line"></div>
        </div>
      </a>
      <div class="search_form">
        <div class="gcse-search" id="cse" style="width: 100%;"></div>
      </div>
    </div> <!-- end navbar --></div>
</div>


<!-- GitHub "fork me" ribbon -->
<a href="https://github.com/scikit-learn/scikit-learn">
  <img class="fork-me" style="position: absolute; top: 0; right: 0; border: 0;" src="../static/img/forkme.png"
    alt="Fork me on GitHub" />
</a>

<div class="content-wrapper">
  <div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
      <div class="rel">
        
          <div class="rellink">
            <a href="sgd.html" accesskey="P">Previous
              <br />
              <span class="smallrellink">
                1.5. 随机梯度下降(S...
              </span>
              <span class="hiddenrellink">
                1.5. 随机梯度下降(Stochastic Gradient Descent)
              </span>
            </a>
          </div>
          <div class="spacer">
            &nbsp;
          </div>
          <div class="rellink">
            <a href="gaussian_process.html" accesskey="N">Next
              <br />
              <span class="smallrellink">
                1.7. 高斯过程(Gau...
              </span>
              <span class="hiddenrellink">
                1.7. 高斯过程(Gaussian Processes)
              </span>
            </a>
          </div>

          <!-- Ad a link to the 'up' page -->
          <div class="spacer">
            &nbsp;
          </div>
          <div class="rellink">
            <a href="../supervised_learning.html">
              Up
              <br />
              <span class="smallrellink">
                1. 监督学习(super...
              </span>
                <span class="hiddenrellink">
                  1. 监督学习(supervised learning)
                </span>
                
            </a>
          </div>
        </div>
        
        <p class="doc-version"><b>scikit-learn v0.20.2</b><br />
          <a href="http://scikit-learn.org/dev/versions.html">其他版本</a></p>
        <!-- <p class="citing">Please <b><a href="../about.html#citing-scikit-learn" style="font-size: 110%;">cite
              us </a></b>if you use the software.</p> -->
        <p class="citing">该中文文档由人工智能社区的<a href="http://www.studyai.com/antares" target="_blank">Antares</a>翻译!
        </p>
        <ul>
<li><a class="reference internal" href="#">1.6. 最近邻方法(Nearest Neighbors)</a><ul>
<li><a class="reference internal" href="#unsupervised-neighbors">1.6.1. 无监督最近邻</a><ul>
<li><a class="reference internal" href="#id2">1.6.1.1. 寻找最近的邻居</a></li>
<li><a class="reference internal" href="#kdtree-balltree">1.6.1.2. KDTree类 和 BallTree类</a></li>
</ul>
</li>
<li><a class="reference internal" href="#classification">1.6.2. 最近邻分类器</a></li>
<li><a class="reference internal" href="#regression">1.6.3. 最近邻回归器</a></li>
<li><a class="reference internal" href="#id5">1.6.4. 最近邻算法</a><ul>
<li><a class="reference internal" href="#brute-force">1.6.4.1. Brute Force</a></li>
<li><a class="reference internal" href="#k-d-tree">1.6.4.2. K-D Tree</a></li>
<li><a class="reference internal" href="#ball-tree">1.6.4.3. Ball Tree</a></li>
<li><a class="reference internal" href="#id10">1.6.4.4. 最近邻算法的选择</a></li>
<li><a class="reference internal" href="#leaf-size">1.6.4.5. <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code> 的效果</a></li>
</ul>
</li>
<li><a class="reference internal" href="#nearest-centroid-classifier">1.6.5. 最近质心分类器</a><ul>
<li><a class="reference internal" href="#nearest-shrunken-centroid">1.6.5.1. Nearest Shrunken Centroid</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        <br />
        <p>
          <a href="https://study.163.com/course/introduction/1209532843.htm?share=2&shareId=400000000535031" target="_blank">
            <img src="../static/img/advitise1.png" alt="座右铭" />
          </a>
        </p>
        <br />
        <p class="doc-version" style="font-size:10%">
          注意!本网站的网址是以 <em>https://</em> 开头的，而不是以 <em>http://</em> 开头的!!!
        </p>
      </div>
    </div>
    
    <input type="checkbox" id="nav-trigger" class="nav-trigger" checked />
    <label for="nav-trigger"></label>
    
    


    <div class="content">
      
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="nearest-neighbors">
<span id="neighbors"></span><h1>1.6. 最近邻方法(Nearest Neighbors)<a class="headerlink" href="#nearest-neighbors" title="Permalink to this headline">¶</a></h1>
<p><a class="reference internal" href="classes.html#module-sklearn.neighbors" title="sklearn.neighbors"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.neighbors</span></code></a> 模块提供了基于邻居的(neighbors-based)的无监督学习和监督学习的功能。
无监督的最近邻是许多其它学习方法的基础，尤其是 流行学习(manifold learning) 和 谱聚类(spectral clustering)。
基于邻居的监督学习分为两种： 分类( <a class="reference internal" href="#classification">classification</a> ):针对的是具有离散标签的数据，回归(<a class="reference internal" href="#regression">regression</a>):针对的是具有连续标签的数据。</p>
<p>最近邻方法背后的原理是从训练样本中找到与新样本点在距离上最近的预定数量的几个点，然后从这几个已知标签的样本点中预测新样本的标签。
这些点的数量可以是用户自定义的常量（K-最近邻学习）， 也可以根据不同的点的局部密度（基于半径的最近邻学习）。
距离通常可以通过任何测度(metric)来衡量： 标准欧式距离(standard Euclidean distance) 是最常见的选择。
Neighbors-based方法被称为 <em>non-generalizing</em> 的机器学习方法，
因为它们只是简单地”记住”了其所有的训练数据（可能转换为一个快速索引结构，如 <a class="reference internal" href="#ball-tree"><span class="std std-ref">Ball Tree</span></a> 或 <a class="reference internal" href="#kd-tree"><span class="std std-ref">KD Tree</span></a> ）。</p>
<p>尽管它简单，但最近邻算法已经成功地适用于很多的分类和回归问题，例如手写数字或卫星图像的场景。
作为一个 非参数化(non-parametric)方法，它经常成功地应用于决策边界非常不规则的分类情景下。</p>
<p><a class="reference internal" href="classes.html#module-sklearn.neighbors" title="sklearn.neighbors"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.neighbors</span></code></a> 模块可以处理 Numpy 数组或 <cite>scipy.sparse</cite> 矩阵作为其输入。
对于稠密矩阵，大多数可能的距离测度都是支持的。对于稀疏矩阵，支持任意的 Minkowski测度 用于搜索。</p>
<p>许多学习方法都依赖最近邻作为其核心。 一个例子是 核密度估计(<a class="reference internal" href="density.html#kernel-density"><span class="std std-ref">kernel density estimation</span></a>) ,
在 密度估计( <a class="reference internal" href="density.html#density-estimation"><span class="std std-ref">density estimation</span></a> ) 章节中有讨论。</p>
<div class="section" id="unsupervised-neighbors">
<span id="id1"></span><h2>1.6.1. 无监督最近邻<a class="headerlink" href="#unsupervised-neighbors" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors" title="sklearn.neighbors.NearestNeighbors"><code class="xref py py-class docutils literal notranslate"><span class="pre">NearestNeighbors</span></code></a> 类实现了 无监督的最近邻学习。 它为三种不同的最近邻算法提供统一的接口：<a class="reference internal" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a> , <a class="reference internal" href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a>,
还有基于 <a class="reference internal" href="classes.html#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise</span></code></a> 模块的 暴力搜索算法(brute-force search)。
近邻搜索算法的选择可通过关键字参数 <code class="docutils literal notranslate"><span class="pre">'algorithm'</span></code> 来控制， 取值必须是 <code class="docutils literal notranslate"><span class="pre">['auto',</span> <span class="pre">'ball_tree',</span> <span class="pre">'kd_tree',</span> <span class="pre">'brute']</span></code> 其中的一个。
当默认值设置为 <code class="docutils literal notranslate"><span class="pre">'auto'</span></code> 时，算法会尝试从训练数据中确定最佳方法。关于上述每个选项的优缺点的讨论，请看 <a href="#id12"><span class="problematic" id="id13">`Nearest Neighbor Algorithms`_</span></a>.。</p>
<blockquote>
<div><div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">关于最近邻算法，如果邻居 <span class="math notranslate nohighlight">\(k+1\)</span> 和邻居 <span class="math notranslate nohighlight">\(k\)</span> 具有相同的距离，但具有不同的标签， 结果将取决于训练数据的顺序。</p>
</div>
</div></blockquote>
<div class="section" id="id2">
<h3>1.6.1.1. 寻找最近的邻居<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>对于 找到两组数据集中最近邻点 的简单任务, 可以使用 <a class="reference internal" href="classes.html#module-sklearn.neighbors" title="sklearn.neighbors"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.neighbors</span></code></a> 模块中的无监督算法:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="k">import</span> <span class="n">NearestNeighbors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nbrs</span> <span class="o">=</span> <span class="n">NearestNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s1">&#39;ball_tree&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">distances</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">nbrs</span><span class="o">.</span><span class="n">kneighbors</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span>                                           <span class="c1"># doctest: +ELLIPSIS</span>
<span class="go">array([[0, 1],</span>
<span class="go">       [1, 0],</span>
<span class="go">       [2, 1],</span>
<span class="go">       [3, 4],</span>
<span class="go">       [4, 3],</span>
<span class="go">       [5, 4]]...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">distances</span>
<span class="go">array([[0.        , 1.        ],</span>
<span class="go">       [0.        , 1.        ],</span>
<span class="go">       [0.        , 1.41421356],</span>
<span class="go">       [0.        , 1.        ],</span>
<span class="go">       [0.        , 1.        ],</span>
<span class="go">       [0.        , 1.41421356]])</span>
</pre></div>
</div>
<p>因为查询集(query set)匹配训练集(training set)，每个点的最近邻点是其自身，距离为0。</p>
<p>还可以有效地生成一个稀疏图(sparse graph)来标识相连点之间的连接情况：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">nbrs</span><span class="o">.</span><span class="n">kneighbors_graph</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="go">array([[1., 1., 0., 0., 0., 0.],</span>
<span class="go">       [1., 1., 0., 0., 0., 0.],</span>
<span class="go">       [0., 1., 1., 0., 0., 0.],</span>
<span class="go">       [0., 0., 0., 1., 1., 0.],</span>
<span class="go">       [0., 0., 0., 1., 1., 0.],</span>
<span class="go">       [0., 0., 0., 0., 1., 1.]])</span>
</pre></div>
</div>
<p>由于数据集是结构化的，因此按索引顺序的相邻点就在参数空间也是相邻点，从而生成了近似 K-近邻 的块对角矩阵(block-diagonal matrix)。
这种稀疏图在各种利用样本点之间的空间关系进行无监督学习的情况下都很有用：特别地请看 <a class="reference internal" href="generated/sklearn.manifold.Isomap.html#sklearn.manifold.Isomap" title="sklearn.manifold.Isomap"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.manifold.Isomap</span></code></a>,
<a class="reference internal" href="generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding" title="sklearn.manifold.LocallyLinearEmbedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.manifold.LocallyLinearEmbedding</span></code></a>, 和 <a class="reference internal" href="generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering" title="sklearn.cluster.SpectralClustering"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.cluster.SpectralClustering</span></code></a>。</p>
</div>
<div class="section" id="kdtree-balltree">
<h3>1.6.1.2. KDTree类 和 BallTree类<a class="headerlink" href="#kdtree-balltree" title="Permalink to this headline">¶</a></h3>
<p>另外，我们可以直接使用 <a class="reference internal" href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a> 或 <a class="reference internal" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a> 来找最近邻。
这是上面提到过的 <a class="reference internal" href="generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors" title="sklearn.neighbors.NearestNeighbors"><code class="xref py py-class docutils literal notranslate"><span class="pre">NearestNeighbors</span></code></a> 类 所封装的功能。
KDTree 和 BallTree 具有相同的接口； 我们将在这里展示使用 KDTree 的例子:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="k">import</span> <span class="n">KDTree</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kdt</span> <span class="o">=</span> <span class="n">KDTree</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">leaf_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kdt</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_distance</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>          
<span class="go">array([[0, 1],</span>
<span class="go">       [1, 0],</span>
<span class="go">       [2, 1],</span>
<span class="go">       [3, 4],</span>
<span class="go">       [4, 3],</span>
<span class="go">       [5, 4]]...)</span>
</pre></div>
</div>
<p>对于 最近邻搜索 中参数选项的更多信息，包括各种距离度量的说明和策略的说明等，请参阅 <a class="reference internal" href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a> 和 <a class="reference internal" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a> 类文档。
关于可用测度(metrics)的列表，请参阅 <a class="reference internal" href="generated/sklearn.neighbors.DistanceMetric.html#sklearn.neighbors.DistanceMetric" title="sklearn.neighbors.DistanceMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistanceMetric</span></code></a>  类。</p>
</div>
</div>
<div class="section" id="classification">
<span id="id3"></span><h2>1.6.2. 最近邻分类器<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h2>
<p>基于邻居的分类是 基于实例的学习(<em>instance-based learning</em>) 或 非概括性学习(<em>non-generalizing learning</em>):
它并不试图构建一个一般的内部模型，而只是存储训练数据的实例。
分类是根据每个点的最近邻居的简单多数票计算的：一个查询点被分配给数据类，该类在该点最近的邻居中有最多的代表。</p>
<p>scikit-learn 实现了两个不同的最近邻分类器：<a class="reference internal" href="generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier" title="sklearn.neighbors.KNeighborsClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code></a> 类实现了 基于每个查询点的k个最近邻进行学习的方法，
其中 <span class="math notranslate nohighlight">\(k\)</span> 是用户指定的整数值。 <a class="reference internal" href="generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier" title="sklearn.neighbors.RadiusNeighborsClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">RadiusNeighborsClassifier</span></code></a> 类实现了 基于固定半径内的邻居进行学习的方法，其中
<span class="math notranslate nohighlight">\(r\)</span> 是用户指定的浮点值。</p>
<p><a class="reference internal" href="generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier" title="sklearn.neighbors.KNeighborsClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code></a> 类中的 <span class="math notranslate nohighlight">\(k\)</span>-neighbors classification  是最常用的技术。
<span class="math notranslate nohighlight">\(k\)</span> 值的最优选择是高度数据依赖的：一般来说  <span class="math notranslate nohighlight">\(k\)</span> 值越大就越能够压制噪声，但是会使得分类边界变得越不清晰。</p>
<p>如果数据是不均匀采样的，那么 <a class="reference internal" href="generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier" title="sklearn.neighbors.RadiusNeighborsClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">RadiusNeighborsClassifier</span></code></a> 类中的基于半径的近邻分类(radius-based neighbors classification)可能是更好的选择。</p>
<p>用户指定一个固定的半径 <span class="math notranslate nohighlight">\(r\)</span> , 以便在越稀疏的邻域内的点可以使用越少的最近邻点进行分类。对于高维参数空间，这个方法就会因为维数灾难而变得越来越没有效率。</p>
<p>基本的最近邻分类使用均匀的权重：分配给查询点的标签值是从其最近邻的简单多数投票中计算出来的。
在某些环境下，最好对邻居进行加权，使得离得越近的点对最终的预测做出越大的贡献。这可以通过 <code class="docutils literal notranslate"><span class="pre">weights</span></code> 关键字来实现。
该参数的默认值 <code class="docutils literal notranslate"><span class="pre">weights</span> <span class="pre">=</span> <span class="pre">'uniform'</span></code> 给每个邻居分配均匀的权重。而 <code class="docutils literal notranslate"><span class="pre">weights</span> <span class="pre">=</span> <span class="pre">'distance'</span></code> 给每个邻居分配的权重是每个邻居点到查询点的距离的倒数。
除此之外，用户还可以传递自定义的距离函数来计算权重：</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/neighbors/plot_classification.html"><img alt="classification_1" src="../images/sphx_glr_plot_classification_0011.png" style="width: 320.0px; height: 240.0px;" /></a> <a class="reference external" href="../auto_examples/neighbors/plot_classification.html"><img alt="classification_2" src="../images/sphx_glr_plot_classification_0021.png" style="width: 320.0px; height: 240.0px;" /></a></strong></p><div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py"><span class="std std-ref">Nearest Neighbors Classification</span></a>: an example of
classification using nearest neighbors.</li>
</ul>
</div>
</div>
<div class="section" id="regression">
<span id="id4"></span><h2>1.6.3. 最近邻回归器<a class="headerlink" href="#regression" title="Permalink to this headline">¶</a></h2>
<p>最近邻回归是用在数据标签为连续变量而不是离散变量的情况下。分配给查询点的标签是由它的最近邻标签的均值计算而来的。</p>
<p>scikit-learn 实现了两个不同的最近邻回归器：<a class="reference internal" href="generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor" title="sklearn.neighbors.KNeighborsRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">KNeighborsRegressor</span></code></a> 类实现了 基于每个查询点的k个最近邻进行学习的方法，
其中 <span class="math notranslate nohighlight">\(k\)</span> 是用户指定的整数值。 <a class="reference internal" href="generated/sklearn.neighbors.RadiusNeighborsRegressor.html#sklearn.neighbors.RadiusNeighborsRegressor" title="sklearn.neighbors.RadiusNeighborsRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">RadiusNeighborsRegressor</span></code></a> 类实现了 基于固定半径内的邻居进行学习的方法，其中
<span class="math notranslate nohighlight">\(r\)</span> 是用户指定的浮点值。</p>
<p>基本的最近邻回归使用均匀的权重：分配给查询点的标签值是从其最近邻的简单多数投票中计算出来的。
在某些环境下，最好对邻居进行加权，使得离得越近的点对最终的预测做出越大的贡献。这可以通过 <code class="docutils literal notranslate"><span class="pre">weights</span></code> 关键字来实现。
该参数的默认值 <code class="docutils literal notranslate"><span class="pre">weights</span> <span class="pre">=</span> <span class="pre">'uniform'</span></code> 给每个邻居分配均匀的权重。而 <code class="docutils literal notranslate"><span class="pre">weights</span> <span class="pre">=</span> <span class="pre">'distance'</span></code> 给每个邻居分配的权重是每个邻居点到查询点的距离的倒数。
除此之外，用户还可以传递自定义的距离函数来计算权重：</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/neighbors/plot_regression.html"><img alt="../images/sphx_glr_plot_regression_0011.png" src="../images/sphx_glr_plot_regression_0011.png" style="width: 480.0px; height: 360.0px;" /></a>
</div>
<p>多输出最近邻回归的使用案例可以看 <a class="reference internal" href="../auto_examples/plot_multioutput_face_completion.html#sphx-glr-auto-examples-plot-multioutput-face-completion-py"><span class="std std-ref">Face completion with a multi-output estimators</span></a>。 在这个案例中，输入 X 是一些人脸的上半部分，
输出 Y 是那些人脸的下半部分。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/plot_multioutput_face_completion.html"><img alt="../images/sphx_glr_plot_multioutput_face_completion_0011.png" src="../images/sphx_glr_plot_multioutput_face_completion_0011.png" style="width: 750.0px; height: 847.5px;" /></a>
</div>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/neighbors/plot_regression.html#sphx-glr-auto-examples-neighbors-plot-regression-py"><span class="std std-ref">Nearest Neighbors regression</span></a>: an example of regression
using nearest neighbors.</li>
<li><a class="reference internal" href="../auto_examples/plot_multioutput_face_completion.html#sphx-glr-auto-examples-plot-multioutput-face-completion-py"><span class="std std-ref">Face completion with a multi-output estimators</span></a>: an example of
multi-output regression using nearest neighbors.</li>
</ul>
</div>
</div>
<div class="section" id="id5">
<h2>1.6.4. 最近邻算法<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<div class="section" id="brute-force">
<span id="id6"></span><h3>1.6.4.1. Brute Force<a class="headerlink" href="#brute-force" title="Permalink to this headline">¶</a></h3>
<p>最近邻的快速计算是机器学习中一个活跃的研究领域。最简单的近邻搜索的实现涉及数据集中所有成对点之间距离的暴力计算(Brute Force computation)：
对于 <span class="math notranslate nohighlight">\(D\)</span> 维空间中的 <span class="math notranslate nohighlight">\(N\)</span> 个样本来说, 这个方法的复杂度是 <span class="math notranslate nohighlight">\(O[D N^2]\)</span> 。
对于少量的数据样本，高效的暴力近邻搜索是非常有竞争力的。 然而，随着样本数 <span class="math notranslate nohighlight">\(N\)</span> 的增长，暴力方法很快变得不切实际了。
在 <a class="reference internal" href="classes.html#module-sklearn.neighbors" title="sklearn.neighbors"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.neighbors</span></code></a> 模块中， 暴力近邻搜索通过关键字参数 <code class="docutils literal notranslate"><span class="pre">algorithm</span> <span class="pre">=</span> <span class="pre">'brute'</span></code> 来指定，并通过
<a class="reference internal" href="classes.html#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise</span></code></a> 中的例程(routines)来进行计算。</p>
</div>
<div class="section" id="k-d-tree">
<span id="kd-tree"></span><h3>1.6.4.2. K-D Tree<a class="headerlink" href="#k-d-tree" title="Permalink to this headline">¶</a></h3>
<p>为了解决效率低下的暴力计算方法，已经发明了大量的基于树的数据结构。总的来说，
这些数据结构试图通过有效地编码样本的 聚合距离信息(aggregate distance information) 来减少所需的距离计算量。
基本思想是，若 <span class="math notranslate nohighlight">\(A\)</span> 点距离 <span class="math notranslate nohighlight">\(B\)</span> 点非常远，<span class="math notranslate nohighlight">\(B\)</span> 点距离 <span class="math notranslate nohighlight">\(C\)</span> 点非常近， 可知 <span class="math notranslate nohighlight">\(A\)</span> 点与 <span class="math notranslate nohighlight">\(C\)</span> 点很遥远，
不需要明确计算它们的距离。 通过这样的方式，近邻搜索的计算成本可以降低为 <span class="math notranslate nohighlight">\(O[D N \log(N)]\)</span> 或更低。
这是对于暴力搜索在大样本数 <span class="math notranslate nohighlight">\(N\)</span> 中表现的显著改善。</p>
<p>利用这种聚合信息的早期方法是 <em>KD tree</em> 数据结构（* K-dimensional tree * 的简写）,
它将二维 <em>Quad-trees</em> 和三维 <em>Oct-trees</em> 推广到任意数量的维度. KD 树是一个二叉树结构，
它沿着数据轴递归地划分参数空间，将其划分为嵌入数据点的嵌套的各向异性区域。
KD 树的构造非常快：因为只需沿数据轴执行分区, 无需计算 D-dimensional 距离。
一旦构建完成, 查询点的最近邻距离计算复杂度仅为 <span class="math notranslate nohighlight">\(O[\log(N)]\)</span> 。 虽然 KD 树的方法对于低维度 (<span class="math notranslate nohighlight">\(D &lt; 20\)</span>) 近邻搜索非常快,
当 D 增长到很大时, 效率变低: 这就是所谓的 “维度灾难” 的一种体现。 在 scikit-learn 中, KD 树近邻搜索可以使用关键字 <code class="docutils literal notranslate"><span class="pre">algorithm</span> <span class="pre">=</span> <span class="pre">'kd_tree'</span></code> 来指定,
并且使用类 <a class="reference internal" href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a> 来计算。</p>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="http://dl.acm.org/citation.cfm?doid=361002.361007">“Multidimensional binary search trees used for associative searching”</a>,
Bentley, J.L., Communications of the ACM (1975)</li>
</ul>
</div>
</div>
<div class="section" id="ball-tree">
<span id="id7"></span><h3>1.6.4.3. Ball Tree<a class="headerlink" href="#ball-tree" title="Permalink to this headline">¶</a></h3>
<p>为了解决 KD 树在高维上效率低下的问题, <em>ball tree</em> 数据结构就被发明了。 其中 KD 树沿迪卡尔轴（即坐标轴）分割数据, 而
ball 树在沿着一系列的 nesting hyper-spheres 来分割数据。 通过这种方法构建的树要比 KD 树消耗更多的时间,
但是这种数据结构对于高结构化的数据是非常有效的, 即使在高维度上也是一样。</p>
<p>ball tree 将数据递归地划分到由质心 <span class="math notranslate nohighlight">\(C\)</span> 和 半径 <span class="math notranslate nohighlight">\(r\)</span> 定义的节点上，以使得 节点内的每个点都位于由 质心 <span class="math notranslate nohighlight">\(C\)</span> 和 半径 :math:<a href="#id8"><span class="problematic" id="id9">`</span></a>r`定义的
超球面(hyper-sphere)内。通过使用 三角不等式(<em>triangle inequality</em>) 减少近邻搜索的候选点数:</p>
<div class="math notranslate nohighlight">
\[|x+y| \leq |x| + |y|\]</div>
<p>通过这种设置, 测试点和质心之间的单一距离计算足以确定测试点到节点内所有点的距离的下限和上限。
由于 ball tree 节点的球形几何(spherical geometry), 它在高维度上的性能超出 <em>KD-tree</em> , 尽管实际的性能高度依赖于训练数据的结构。
在 scikit-learn 中, 基于 ball tree 的近邻搜索可以使用关键字 <code class="docutils literal notranslate"><span class="pre">algorithm</span> <span class="pre">=</span> <span class="pre">'ball_tree'</span></code> 来指定, 并且使用类 <a class="reference internal" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors.BallTree</span></code></a>
来计算, 或者, 用户可以直接使用 <a class="reference internal" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a> 类。</p>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.91.8209">“Five balltree construction algorithms”</a>,
Omohundro, S.M., International Computer Science Institute
Technical Report (1989)</li>
</ul>
</div>
</div>
<div class="section" id="id10">
<h3>1.6.4.4. 最近邻算法的选择<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>对一个给定的数据集,选择一个合适的最近邻算法是很复杂的，其依赖于很多个因素：</p>
<ul>
<li><p class="first">样本数量 <span class="math notranslate nohighlight">\(N\)</span> (i.e. <code class="docutils literal notranslate"><span class="pre">n_samples</span></code>) 和 特征维数 <span class="math notranslate nohighlight">\(D\)</span> (i.e. <code class="docutils literal notranslate"><span class="pre">n_features</span></code>).</p>
<ul class="simple">
<li><em>Brute force</em> 的查询时间增长复杂度是 <span class="math notranslate nohighlight">\(O[D N]\)</span></li>
<li><em>Ball tree</em> 的查询时间增长复杂度是 <span class="math notranslate nohighlight">\(O[D \log(N)]\)</span></li>
<li><em>KD tree</em> 的查询时间随着 <span class="math notranslate nohighlight">\(D\)</span> 变化，所以很难准确的量化。对于一个小的 <span class="math notranslate nohighlight">\(D\)</span> (&lt;=20)
代价近似是 <span class="math notranslate nohighlight">\(O[D\log(N)]\)</span>, 而 KD tree 的查询时间也非常有效率。对于较大的 <span class="math notranslate nohighlight">\(D\)</span> ，代价几乎增加到 <span class="math notranslate nohighlight">\(O[DN]\)</span>，
由于树结构引起的过载(overhead)导致查询比 brute force 还要慢。</li>
</ul>
<p>对比较小的数据集 (<span class="math notranslate nohighlight">\(N\)</span> &lt;= 30), <span class="math notranslate nohighlight">\(\log(N)\)</span> 与 <span class="math notranslate nohighlight">\(N\)</span> 类和 <a class="reference internal" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a> 类都强调了这一点：通过提供一个参数 <em>leaf size</em> 来控制样本的数量，
一旦小于这个数量则直接使用暴力搜索进行查询。这样的做法使得这两个算法类对于较小的 <span class="math notranslate nohighlight">\(N\)</span> 能够达到接近暴力搜索算法的效率。</p>
</li>
<li><p class="first">数据的结构: 数据的内在纬度(<em>intrinsic dimensionality</em>) 和/或 数据的稀疏性(<em>sparsity</em>)。
数据的内在纬度(Intrinsic dimensionality)指的是数据所在的流形(manifold)的纬度 <span class="math notranslate nohighlight">\(d \le D\)</span>, 其中 数据的流形可以是线性或非线性的嵌入到参数空间里的。
数据的稀疏性是指数据填充参数空间的度(这里数据稀疏性的概念区别于稀疏矩阵的稀疏概念，数据矩阵有可能一个0都没有，但是该矩阵的**结构**可能仍然是稀疏的。)</p>
<ul class="simple">
<li><em>Brute force</em> 的查询时间与数据的结构无关。</li>
<li><em>Ball tree</em> 和 <em>KD tree</em> 的查询时间可能会受数据的结构的很大影响。通常情况下，具有越小的内在纬度的越稀疏的数据会带来越快的查询时间。
因为 KD tree 的内部表示是对齐到参数坐标系轴上的，所以它不会在任意结构化的数据上与ball tree有同样的效率提升。</li>
</ul>
<p>机器学习中用到的数据集都是倾向于非常结构化的, 并且非常适合于基于树的查询。</p>
</li>
<li><p class="first">一个查询点需要的邻居的数量 <span class="math notranslate nohighlight">\(k\)</span></p>
<ul class="simple">
<li><em>Brute force</em> 的查询时间在很大程度上不受 <span class="math notranslate nohighlight">\(k\)</span> 值的影响。</li>
<li><em>Ball tree</em> 和 <em>KD tree</em> 的查询时间将会随着 <span class="math notranslate nohighlight">\(k\)</span> 的增加而越来越小。
这主要基于两方面的影响: 首先, 一个较大的 <span class="math notranslate nohighlight">\(k\)</span> 值会导致 搜索参数空间的一个较大的部分的必要性；第二，使用 <span class="math notranslate nohighlight">\(k &gt; 1\)</span>
需要在遍历树时对结果进行内部排队。</li>
</ul>
<p>随着 <span class="math notranslate nohighlight">\(k\)</span> 相较于 <span class="math notranslate nohighlight">\(N\)</span> 越来越大, 基于树的查询进行剪枝的能力就会越来越小。在这种情况下，暴力搜索查询会更有效率。</p>
</li>
<li><p class="first">查询点的数量。 ball tree 和 KD Tree 都需要一个构建阶段. 在许多查询中分摊时，这种结构的成本可以忽略不计。 如果只执行少量的查询,
可是构建成本却占总成本的很大一部分. 如果仅需查询很少的点, 暴力方法会比基于树的方法更好.</p>
</li>
</ul>
<p>Currently, <code class="docutils literal notranslate"><span class="pre">algorithm</span> <span class="pre">=</span> <span class="pre">'auto'</span></code> selects <code class="docutils literal notranslate"><span class="pre">'kd_tree'</span></code> if <span class="math notranslate nohighlight">\(k &lt; N/2\)</span>
and the <code class="docutils literal notranslate"><span class="pre">'effective_metric_'</span></code> is in the <code class="docutils literal notranslate"><span class="pre">'VALID_METRICS'</span></code> list of
<code class="docutils literal notranslate"><span class="pre">'kd_tree'</span></code>. It selects <code class="docutils literal notranslate"><span class="pre">'ball_tree'</span></code> if <span class="math notranslate nohighlight">\(k &lt; N/2\)</span> and the
<code class="docutils literal notranslate"><span class="pre">'effective_metric_'</span></code> is in the <code class="docutils literal notranslate"><span class="pre">'VALID_METRICS'</span></code> list of
<code class="docutils literal notranslate"><span class="pre">'ball_tree'</span></code>. It selects <code class="docutils literal notranslate"><span class="pre">'brute'</span></code> if <span class="math notranslate nohighlight">\(k &lt; N/2\)</span> and the
<code class="docutils literal notranslate"><span class="pre">'effective_metric_'</span></code> is not in the <code class="docutils literal notranslate"><span class="pre">'VALID_METRICS'</span></code> list of
<code class="docutils literal notranslate"><span class="pre">'kd_tree'</span></code> or <code class="docutils literal notranslate"><span class="pre">'ball_tree'</span></code>. It selects <code class="docutils literal notranslate"><span class="pre">'brute'</span></code> if <span class="math notranslate nohighlight">\(k &gt;= N/2\)</span>.
This choice is based on the assumption that the number of query points is at
least the same order as the number of training points, and that <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code>
is close to its default value of <code class="docutils literal notranslate"><span class="pre">30</span></code>.</p>
</div>
<div class="section" id="leaf-size">
<h3>1.6.4.5. <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code> 的效果<a class="headerlink" href="#leaf-size" title="Permalink to this headline">¶</a></h3>
<p>如上所述, 对于小样本暴力搜索是比基于数的搜索更有效的方法. 这一事实在 ball 树和
KD 树中被解释为在叶节点内部切换到蛮力搜索. 该开关的级别可以使用参数 <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code> 来指定.
这个参数选择有很多的效果:</p>
<dl class="docutils">
<dt>** 构建时间(construction time) **</dt>
<dd>更大的 <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code> 会导致更快的树构建时间, 因为需要创建的节点更少。
A larger <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code> leads to a faster tree construction time, because
fewer nodes need to be created</dd>
<dt>** 查询时间(query time)**</dt>
<dd>一个或大或小的 <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code> 可能会导致次优查询成本. 当 <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code> 接近 1 时, 遍历节点所涉及的开销大大减慢了查询时间.
当 <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code> 接近训练集的大小，查询变得本质上是暴力的. 这些之间的一个很好的妥协是 <code class="docutils literal notranslate"><span class="pre">leaf_size</span> <span class="pre">=</span> <span class="pre">30</span></code>, 这是该参数的默认值。</dd>
<dt>** 内存(memory) **</dt>
<dd>随着 <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code> 的增加，存储树结构所需的内存减少。 对于存储每个节点的:math:<cite>D`维质心的 ball tree，这点至关重要。
针对 :class:`BallTree</cite> 所需的存储空间近似于 <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">leaf_size</span></code> 乘以训练集的大小。</dd>
</dl>
<p><code class="docutils literal notranslate"><span class="pre">leaf_size</span></code> 在 brute force 查询中是没有用到的。</p>
</div>
</div>
<div class="section" id="nearest-centroid-classifier">
<span id="id11"></span><h2>1.6.5. 最近质心分类器<a class="headerlink" href="#nearest-centroid-classifier" title="Permalink to this headline">¶</a></h2>
<p>该 <a class="reference internal" href="generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid" title="sklearn.neighbors.NearestCentroid"><code class="xref py py-class docutils literal notranslate"><span class="pre">NearestCentroid</span></code></a> 分类器是一个简单的算法, 通过其成员的质心来表示每个类。
实际上, 这使得它类似于 <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.KMeans</span></code> 算法的标签更新阶段. 它也没有参数选择, 使其成为良好的基准分类器.
然而，在非凸类上，以及当类具有截然不同的方差时，它都会受到影响。所以这个分类器假设所有维度的方差都是相等的。
对于没有做出这个假设的更复杂的方法, 请参阅线性判别分析 (<a class="reference internal" href="generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.discriminant_analysis.LinearDiscriminantAnalysis</span></code></a>)
和二次判别分析 (<a class="reference internal" href="generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis" title="sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis</span></code></a>). 默认的 <a class="reference internal" href="generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid" title="sklearn.neighbors.NearestCentroid"><code class="xref py py-class docutils literal notranslate"><span class="pre">NearestCentroid</span></code></a> 用法示例如下:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors.nearest_centroid</span> <span class="k">import</span> <span class="n">NearestCentroid</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">NearestCentroid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">NearestCentroid(metric=&#39;euclidean&#39;, shrink_threshold=None)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<div class="section" id="nearest-shrunken-centroid">
<h3>1.6.5.1. Nearest Shrunken Centroid<a class="headerlink" href="#nearest-shrunken-centroid" title="Permalink to this headline">¶</a></h3>
<p>该 <a class="reference internal" href="generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid" title="sklearn.neighbors.NearestCentroid"><code class="xref py py-class docutils literal notranslate"><span class="pre">NearestCentroid</span></code></a> 分类器有一个 <code class="docutils literal notranslate"><span class="pre">shrink_threshold</span></code> 参数, 它实现了 nearest shrunken centroid 分类器.
实际上, 每个质心的每个特征的值除以该特征的类中的方差. 然后通过 <code class="docutils literal notranslate"><span class="pre">shrink_threshold</span></code> 来减小特征值.
最值得注意的是, 如果特定特征值过0, 则将其设置为0. 实际上，这个方法移除了影响分类器的特征。
这很有用, 例如, 去除噪声特征.</p>
<p>在以下例子中, 使用一个较小的 shrink 阀值将模型的准确度从 0.81 提高到 0.82。</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/neighbors/plot_nearest_centroid.html"><img alt="nearest_centroid_1" src="../images/sphx_glr_plot_nearest_centroid_0011.png" style="width: 320.0px; height: 240.0px;" /></a> <a class="reference external" href="../auto_examples/neighbors/plot_nearest_centroid.html"><img alt="nearest_centroid_2" src="../images/sphx_glr_plot_nearest_centroid_0021.png" style="width: 320.0px; height: 240.0px;" /></a></strong></p><div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/neighbors/plot_nearest_centroid.html#sphx-glr-auto-examples-neighbors-plot-nearest-centroid-py"><span class="std std-ref">Nearest Centroid Classification</span></a>: an example of
classification using nearest centroid with different shrink thresholds.</li>
</ul>
</div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
  </div>
  
  <div class="footer">
    &copy; 2007 - 2018, scikit-learn developers (BSD License).
    <!--
    <a href="../_sources/modules/neighbors.rst.txt" rel="nofollow">Show this page source</a> -->
  </div>
  <div class="rel">
    
      <div class="buttonPrevious">
        <a href="sgd.html">Previous
        </a>
      </div>
      <div class="buttonNext">
        <a href="gaussian_process.html">Next
        </a>
      </div>
      
    </div>

    
    <script>
      window.ga = window.ga || function () { (ga.q = ga.q || []).push(arguments) }; ga.l = +new Date;
      ga('create', 'UA-22606712-2', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    
    <script>
      (function () {
        var cx = '016639176250731907682:tjtqbvtvij0';
        var gcse = document.createElement('script'); gcse.type = 'text/javascript'; gcse.async = true;
        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s);
      })();
    </script>
  </body>
</html>