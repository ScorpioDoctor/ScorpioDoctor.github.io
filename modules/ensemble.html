

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

    <title>1.11. 集成学习方法(Ensemble methods) &#8212; scikit-learn 0.20.2 documentation</title>
<!-- htmltitle is before nature.css - we use this hack to load bootstrap first -->
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="../static/css/bootstrap.min.css" media="screen" />
<link rel="stylesheet" href="../static/css/bootstrap-responsive.css" />

    <link rel="stylesheet" href="../static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../static/gallery.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../static/documentation_options.js"></script>
    <script type="text/javascript" src="../static/jquery.js"></script>
    <script type="text/javascript" src="../static/underscore.js"></script>
    <script type="text/javascript" src="../static/doctools.js"></script>
    <script type="text/javascript" src="../static/js/copybutton.js"></script>
    <script type="text/javascript" src="../static/js/extra.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_SVG"></script>
    <link rel="shortcut icon" href="../static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1.12. 多类和多标签算法(Multiclass and multilabel algorithms)" href="multiclass.html" />
    <link rel="prev" title="1.10. 决策树(Decision Trees)" href="tree.html" />


<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<script src="../static/js/bootstrap.min.js" type="text/javascript"></script>
<script>
  VERSION_SUBDIR = (function (groups) {
    return groups ? groups[1] : null;
  })(location.href.match(/^https?:\/\/scikit-learn.org\/([^\/]+)/));
</script>
<link rel="canonical" href="http://scikit-learn.org/stable/modules/ensemble.html" />

<script type="text/javascript">
  $("div.buttonNext, div.buttonPrevious").hover(
    function () {
      $(this).css('background-color', '#FF9C34');
    },
    function () {
      $(this).css('background-color', '#A7D6E2');
    }
  );
  function showMenu() {
    var topNav = document.getElementById("scikit-navbar");
    if (topNav.className === "navbar") {
      topNav.className += " responsive";
    } else {
      topNav.className = "navbar";
    }
  };
</script>

<!-- 百度站长统计代码 -->
<script>
  var _hmt = _hmt || [];
  (function () {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?e7836e37a4cb7584127a787e9b44e3f1";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
  })();
</script>


  </head><body>

<div class="header-wrapper">
  <div class="header">
    <p class="logo"><a href="../index.html">
        <img src="../static/scikit-learn-logo-small.png" alt="Logo" />
      </a>
    </p><div class="navbar" id="scikit-navbar">
      <ul>
        <li><a href="../index.html">首页</a></li>
        <li><a href="../install.html">安装</a></li>
        <li class="btn-li">
          <div class="btn-group">
            <a href="../documentation.html">文档</a>
            <a class="btn dropdown-toggle" data-toggle="dropdown">
              <span class="caret"></span>
            </a>
            <ul class="dropdown-menu">
              <li class="link-title">Scikit-learn
                <script>document.write(DOCUMENTATION_OPTIONS.VERSION + (VERSION_SUBDIR ? " (" + VERSION_SUBDIR + ")" : ""));</script>
              </li>
              <li><a href="../tutorial/index.html">教程</a></li>
              <li><a href="../user_guide.html">用户指南</a></li>
              <li><a href="classes.html">API</a></li>
              <li><a href="../glossary.html">词汇表</a></li>
              <li><a href="../faq.html">FAQ</a></li>
              <li><a href="../developers/contributing.html">贡献</a></li>
              <li><a href="../roadmap.html">路线图</a></li>
              <li class="divider"></li>
              <script>if (VERSION_SUBDIR != "stable") document.write('<li><a href="https://www.studyai.cn">稳定版</a></li>')</script>
              <script>if (VERSION_SUBDIR != "dev") document.write('<li><a href="http://scikit-learn.org/dev/documentation.html" target="_blank">开发版</a></li>')</script>
              <li><a href="http://scikit-learn.org/dev/versions.html">所有可用版本</a></li>
              <li><a href="../downloads/scikit-learn-docs.pdf">PDF 文档</a></li>
            </ul>
          </div>
        </li>
        <li><a href="../auto_examples/index.html">案例</a></li>
      </ul>
      <a href="javascript:void(0);" onclick="showMenu()">
        <div class="nav-icon">
          <div class="hamburger-line"></div>
          <div class="hamburger-line"></div>
          <div class="hamburger-line"></div>
        </div>
      </a>
      <div class="search_form">
        <div class="gcse-search" id="cse" style="width: 100%;"></div>
      </div>
    </div> <!-- end navbar --></div>
</div>


<!-- GitHub "fork me" ribbon -->
<a href="https://github.com/scikit-learn/scikit-learn">
  <img class="fork-me" style="position: absolute; top: 0; right: 0; border: 0;" src="../static/img/forkme.png"
    alt="Fork me on GitHub" />
</a>

<div class="content-wrapper">
  <div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
      <div class="rel">
        
          <div class="rellink">
            <a href="tree.html" accesskey="P">Previous
              <br />
              <span class="smallrellink">
                1.10. 决策树(Dec...
              </span>
              <span class="hiddenrellink">
                1.10. 决策树(Decision Trees)
              </span>
            </a>
          </div>
          <div class="spacer">
            &nbsp;
          </div>
          <div class="rellink">
            <a href="multiclass.html" accesskey="N">Next
              <br />
              <span class="smallrellink">
                1.12. 多类和多标签算...
              </span>
              <span class="hiddenrellink">
                1.12. 多类和多标签算法(Multiclass and multilabel algorithms)
              </span>
            </a>
          </div>

          <!-- Ad a link to the 'up' page -->
          <div class="spacer">
            &nbsp;
          </div>
          <div class="rellink">
            <a href="../supervised_learning.html">
              Up
              <br />
              <span class="smallrellink">
                1. 监督学习(super...
              </span>
                <span class="hiddenrellink">
                  1. 监督学习(supervised learning)
                </span>
                
            </a>
          </div>
        </div>
        
        <p class="doc-version"><b>scikit-learn v0.20.2</b><br />
          <a href="http://scikit-learn.org/dev/versions.html">其他版本</a></p>
        <!-- <p class="citing">Please <b><a href="../about.html#citing-scikit-learn" style="font-size: 110%;">cite
              us </a></b>if you use the software.</p> -->
        <p class="citing">该中文文档由人工智能社区的<a href="http://www.studyai.com/antares" target="_blank">Antares</a>翻译!
        </p>
        <ul>
<li><a class="reference internal" href="#">1.11. 集成学习方法(Ensemble methods)</a><ul>
<li><a class="reference internal" href="#bagging-meta-estimator">1.11.1. Bagging meta-estimator</a></li>
<li><a class="reference internal" href="#forest">1.11.2. 由随机树组成的森林</a><ul>
<li><a class="reference internal" href="#id7">1.11.2.1. 随机森林</a></li>
<li><a class="reference internal" href="#id9">1.11.2.2. 极大随机树</a></li>
<li><a class="reference internal" href="#id10">1.11.2.3. 参数</a></li>
<li><a class="reference internal" href="#id11">1.11.2.4. 并行化</a></li>
<li><a class="reference internal" href="#random-forest-feature-importance">1.11.2.5. 特征重要性评估</a></li>
<li><a class="reference internal" href="#id14">1.11.2.6. 完全随机树嵌入</a></li>
</ul>
</li>
<li><a class="reference internal" href="#adaboost">1.11.3. 自适应推举算法(AdaBoost)</a><ul>
<li><a class="reference internal" href="#id19">1.11.3.1. 用法</a></li>
</ul>
</li>
<li><a class="reference internal" href="#gradient-tree-boosting">1.11.4. Gradient Tree Boosting</a><ul>
<li><a class="reference internal" href="#id21">1.11.4.1. 分类</a></li>
<li><a class="reference internal" href="#id22">1.11.4.2. 回归</a></li>
<li><a class="reference internal" href="#gradient-boosting-warm-start">1.11.4.3. 拟合附加的弱学习器</a></li>
<li><a class="reference internal" href="#treesize">1.11.4.4. 控制Tree的size</a></li>
<li><a class="reference internal" href="#id25">1.11.4.5. 数学表达式</a><ul>
<li><a class="reference internal" href="#gradient-boosting-loss">1.11.4.5.1. 损失函数</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id28">1.11.4.6. 正则化</a><ul>
<li><a class="reference internal" href="#shrinkage">1.11.4.6.1. Shrinkage</a></li>
<li><a class="reference internal" href="#subsampling">1.11.4.6.2. Subsampling</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id33">1.11.4.7. 对结果的解释</a><ul>
<li><a class="reference internal" href="#id34">1.11.4.7.1. 特征重要性</a></li>
<li><a class="reference internal" href="#partial-dependence">1.11.4.7.2. 部分依赖性</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#voting-classifier">1.11.5. 投票分类器(Voting Classifier)</a><ul>
<li><a class="reference internal" href="#majority-voting">1.11.5.1. 多数/硬投票(majority voting)</a><ul>
<li><a class="reference internal" href="#id41">1.11.5.1.1. 用法</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id42">1.11.5.2. 加权平均概率(软投票)</a></li>
<li><a class="reference internal" href="#id43">1.11.5.3. 投票分类器在网格搜索中应用</a><ul>
<li><a class="reference internal" href="#id44">1.11.5.3.1. 用法</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        <br />
        <p>
          <a href="http://www.studyai.com" target="_blank">
            <img src="../static/img/advitise1.png" alt="座右铭" />
          </a>
        </p>
        <br />
        <p class="doc-version" style="font-size:10%">
          注意!本网站的网址是以 <em>https://</em> 开头的，而不是以 <em>http://</em> 开头的!!!
        </p>
      </div>
    </div>
    
    <input type="checkbox" id="nav-trigger" class="nav-trigger" checked />
    <label for="nav-trigger"></label>
    
    


    <div class="content">
      
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="ensemble-methods">
<span id="ensemble"></span><h1>1.11. 集成学习方法(Ensemble methods)<a class="headerlink" href="#ensemble-methods" title="Permalink to this headline">¶</a></h1>
<p>集成方法(<strong>ensemble methods</strong>)的目标是把多个使用给定学习算法构建的基本估计器(base estimator)的预测结果结合起来，
从而获得比单个基本估计器更好的泛化能力/鲁棒性。</p>
<p>集成方法通常被分为两大家族:</p>
<ul>
<li><p class="first">在取平均的方法(<strong>averaging methods</strong>)中， 驱动原则是首先独立地构建若干个估计器，然后对它们的预测结果取平均。
在平均意义上，组合得到的估计器通常优于任意一个基本估计器，因为它的方差(variance)被减小了。</p>
<p><strong>代表算法:</strong> <a class="reference internal" href="#bagging"><span class="std std-ref">Bagging methods</span></a>, <a class="reference internal" href="#forest"><span class="std std-ref">Forests of randomized trees</span></a>, …</p>
</li>
<li><p class="first">作为对比, 在推举式方法(<strong>boosting methods</strong>)中, 基本估计器是被顺序的构建的,其中的每一个base estimator都致力于减小组合估计器的偏差(bias)。
这种方法的动机是通过组合若干个弱模型(weak model)来产生一个强大的集成模型。</p>
<p><strong>代表算法:</strong> <a class="reference internal" href="#adaboost"><span class="std std-ref">AdaBoost</span></a>, <a class="reference internal" href="#gradient-boosting"><span class="std std-ref">Gradient Tree Boosting</span></a>, …</p>
</li>
</ul>
<div class="section" id="bagging-meta-estimator">
<span id="bagging"></span><h2>1.11.1. Bagging meta-estimator<a class="headerlink" href="#bagging-meta-estimator" title="Permalink to this headline">¶</a></h2>
<p>(译者注：bagging 在此处属于名词动用，有 “装袋,打包” 的意思，后面就不翻译这个单词了)</p>
<p>在集成算法中，bagging 方法会在原始训练集的随机子集上构建一类黑盒估计器(black-box estimator)的多个实例，
然后把这些估计器的预测结果结合起来形成最终的预测结果。 该方法通过在构建模型的过程中引入随机性，
来减少基本估计器的方差(例如，减小决策树的方差)。 在多数情况下，bagging 方法提供了一种非常简单的方式来对单一模型进行改进，
而无需修改背后的算法。 因为 bagging 方法可以减小过拟合，所以通常在强分类器和复杂模型上使用时表现的很好
（例如，完全决策树，fully developed decision trees），相比之下 boosting 方法则在弱模型上表现更好
（例如，浅层决策树，shallow decision trees）。</p>
<p>bagging 方法有很多种，其主要区别在于随机抽取训练子集的方法不同：</p>
<blockquote>
<div><ul class="simple">
<li>如果抽取的数据集的随机子集是样本的随机子集，我们叫做粘贴 (Pasting) <a class="reference internal" href="#b1999" id="id1">[B1999]</a> 。</li>
<li>如果样本抽取是有放回的，我们称为 Bagging <a class="reference internal" href="#b1996" id="id2">[B1996]</a>.</li>
<li>如果抽取的数据集的随机子集是特征的随机子集，我们叫做随机子空间 (Random Subspaces) <a class="reference internal" href="#h1998" id="id3">[H1998]</a>。</li>
<li>最后，如果基本估计器构建在对于样本和特征抽取的子集之上时，我们叫做随机补丁(Random Patches) <a class="reference internal" href="#lg2012" id="id4">[LG2012]</a>.</li>
</ul>
</div></blockquote>
<p>在 scikit-learn 中，bagging 方法使用统一的 <a class="reference internal" href="generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier" title="sklearn.ensemble.BaggingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaggingClassifier</span></code></a> 元估计器（或者 <a class="reference internal" href="generated/sklearn.ensemble.BaggingRegressor.html#sklearn.ensemble.BaggingRegressor" title="sklearn.ensemble.BaggingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaggingRegressor</span></code></a> ），
基本估计器的种类及其参数和随机子集抽取策略都可以由用户指定。特别需要指出的是，<code class="docutils literal notranslate"><span class="pre">max_samples</span></code> 和 <code class="docutils literal notranslate"><span class="pre">max_features</span></code>
控制着子集的大小（对于样本和特征）， 而 <code class="docutils literal notranslate"><span class="pre">bootstrap</span></code> 和 <code class="docutils literal notranslate"><span class="pre">bootstrap_features</span></code> 控制着样本和特征的抽取是有放回还是无放回的。
当使用样本子集时，通过设置 <code class="docutils literal notranslate"><span class="pre">oob_score=True</span></code> 可以使用袋外(out-of-bag)样本来评估泛化精度。
下面的代码片段说明了如何构造一个 <code class="xref py py-class docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code> 估计器的 bagging 集成实例，
每一个base estimator都建立在 50% 的样本随机子集和 50% 的特征随机子集上。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">BaggingClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="k">import</span> <span class="n">KNeighborsClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bagging</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">KNeighborsClassifier</span><span class="p">(),</span>
<span class="gp">... </span>                            <span class="n">max_samples</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/ensemble/plot_bias_variance.html#sphx-glr-auto-examples-ensemble-plot-bias-variance-py"><span class="std std-ref">Single estimator versus bagging: bias-variance decomposition</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献</p>
<table class="docutils citation" frame="void" id="b1999" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id1">[B1999]</a></td><td>L. Breiman, “Pasting small votes for classification in large
databases and on-line”, Machine Learning, 36(1), 85-103, 1999.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="b1996" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[B1996]</a></td><td>L. Breiman, “Bagging predictors”, Machine Learning, 24(2),
123-140, 1996.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="h1998" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id3">[H1998]</a></td><td>T. Ho, “The random subspace method for constructing decision
forests”, Pattern Analysis and Machine Intelligence, 20(8), 832-844,
1998.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="lg2012" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id4">[LG2012]</a></td><td>G. Louppe and P. Geurts, “Ensembles on Random Patches”,
Machine Learning and Knowledge Discovery in Databases, 346-361, 2012.</td></tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="forest">
<span id="id5"></span><h2>1.11.2. 由随机树组成的森林<a class="headerlink" href="#forest" title="Permalink to this headline">¶</a></h2>
<p><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.ensemble`模块包含两个基于</span> <span class="pre">随机决策树</span> <span class="pre">(randomized</span> <span class="pre">:ref:`decision</span> <span class="pre">trees</span></code>) 的平均算法：
RandomForest 算法和 Extra-Trees 算法。这两种算法都是专门为树而设计的扰动和组合技术(perturb-and-combine techniques) <a class="reference internal" href="#b1998" id="id6">[B1998]</a> 。
这种技术通过在分类器构造过程中引入随机性来创建一组差异性很大的分类器。集成分类器的预测结果就是所有单个分类器预测结果的平均值。</p>
<p>就像其他分类器一样, 森林分类器(forest classifiers)必须要在两个数组上进行拟合：一个是用于持有训练样本的shape为 <code class="docutils literal notranslate"><span class="pre">[n_samples,</span> <span class="pre">n_features]</span></code>
的或稠密或稀疏的 X 数组，另一个是持有与训练数据对应的目标变量(如 类标签)的shape为 <code class="docutils literal notranslate"><span class="pre">[n_samples]</span></code> 的 Y 数组</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
<p>像 <a class="reference internal" href="tree.html#tree"><span class="std std-ref">decision trees</span></a> 一样, 由树组成的森林(forests of trees)也扩展到了可以支持多输出问题
<a class="reference internal" href="tree.html#tree-multioutput"><span class="std std-ref">multi-output problems</span></a>  (如果 Y 是一个 size 为 <code class="docutils literal notranslate"><span class="pre">[n_samples,</span> <span class="pre">n_outputs]</span></code> 的数组)。</p>
<div class="section" id="id7">
<h3>1.11.2.1. 随机森林<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>在随机森林(random forests)中（参见 <a class="reference internal" href="generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier" title="sklearn.ensemble.RandomForestClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">RandomForestClassifier</span></code></a> 和 <a class="reference internal" href="generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor" title="sklearn.ensemble.RandomForestRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">RandomForestRegressor</span></code></a> 类），
集成模型中的每棵树构建时的样本都是由训练集经过有放回抽样得来的（例如，自助采样法-bootstrap sample）。
另外，在构建树的过程中进行结点分割时，选择的分割点不再是所有特征中最佳分割点，而是特征的一个随机子集中的最佳分割点。
由于这种随机性，森林的偏差(bias)通常会有略微的增大（相对于单个非随机树的偏差），但是由于取了平均，其方差也会减小(variance)，
通常能够补偿偏差的增加，从而产生一个总体上更好的模型。</p>
<p>与原始文献 <a class="reference internal" href="#b2001" id="id8">[B2001]</a> 不同的是，scikit-learn 的实现是取每个分类器预测出的概率的平均，而不是让每个分类器对单个类别进行投票。</p>
</div>
<div class="section" id="id9">
<h3>1.11.2.2. 极大随机树<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p>(译者注：”Extremely” 有 “极致的,非常的” 意思，因此 “Extremely Randomized Trees” 就是指 “随机的不能再随机的树” 也就是 “极大随机树” 啦！)</p>
<p>在极大随机树方法(<a class="reference internal" href="generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier" title="sklearn.ensemble.ExtraTreesClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExtraTreesClassifier</span></code></a> 和 <a class="reference internal" href="generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor" title="sklearn.ensemble.ExtraTreesRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExtraTreesRegressor</span></code></a>)中，随机性更进一步的体现在划分的计算方式上。
极大随机树也和随机森林一样，使用了候选特征的随机子集，但是不同之处在于：随机森林为每个特征寻找最具分辨性的阈值(looking for the most discriminative thresholds)，
而在极大随机树里面 每个特征的阈值也是随机抽取的，并且这些随机生成的阈值里面最好的阈值会被用来分割节点。
这种更随机的做法通常能够使得模型的方差减小一点但是会使得模型的偏差稍微的增加一点:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">cross_val_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">make_blobs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">ExtraTreesClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="k">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>                               
<span class="go">0.98...</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>                               
<span class="go">0.999...</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">ExtraTreesClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.999</span>
<span class="go">True</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/ensemble/plot_forest_iris.html"><img alt="../images/sphx_glr_plot_forest_iris_0011.png" src="../images/sphx_glr_plot_forest_iris_0011.png" style="width: 480.0px; height: 360.0px;" /></a>
</div>
</div>
<div class="section" id="id10">
<h3>1.11.2.3. 参数<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>使用上述这些方法时要调整的参数主要是 <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> 和 <code class="docutils literal notranslate"><span class="pre">max_features</span></code> 。 <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> 是森林里树的数量，通常数量越大，效果越好，
但是计算时间也会随之增加。 此外要注意，当树的数量超过一个临界值之后，算法的效果并不会很显著地变好。
<code class="docutils literal notranslate"><span class="pre">max_features</span></code> 是分割节点时特征的随机子集的大小。 这个值越低，方差减小得越多，但是偏差的增加也越多。
根据经验，回归问题中使用 <code class="docutils literal notranslate"><span class="pre">max_features=n_features</span></code> ， 分类问题使用  <code class="docutils literal notranslate"><span class="pre">max_features=sqrt(n_features)</span></code> (<code class="docutils literal notranslate"><span class="pre">n_features</span></code> 是特征的个数)
是比较好的默认值。 <code class="docutils literal notranslate"><span class="pre">max_depth=None</span></code> 和 <code class="docutils literal notranslate"><span class="pre">min_samples_split=2</span></code> 的参数组合通常会有不错的效果（即生成完全的树）。
请记住，这些（默认）值通常不是最佳的，同时还可能消耗大量的内存，最佳参数值应由交叉验证获得。
另外，请注意，在随机森林中，默认使用自助采样法（<code class="docutils literal notranslate"><span class="pre">bootstrap=True</span></code>）， 然而 极大随机树(extra-trees) 的默认策略是使用整个数据集（<code class="docutils literal notranslate"><span class="pre">bootstrap=False</span></code>）。
当使用自助采样法方法抽样时，泛化精度是可以通过剩余的或者袋外的样本来估算的，设置 <code class="docutils literal notranslate"><span class="pre">oob_score=True</span></code> 即可实现。</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">默认参数下模型复杂度是：<span class="math notranslate nohighlight">\(O( M * N * log (N) )\)</span> ， 其中 <span class="math notranslate nohighlight">\(M\)</span>  是树的数目， <span class="math notranslate nohighlight">\(N\)</span> 是样本数。
可以通过设置以下参数来降低模型复杂度： <code class="docutils literal notranslate"><span class="pre">min_samples_split</span></code>, <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code>, <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> 和 <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> 。</p>
</div>
</div>
<div class="section" id="id11">
<h3>1.11.2.4. 并行化<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<p>最后，这个模块还支持树的并行构建和预测结果的并行计算，这可以通过 <code class="docutils literal notranslate"><span class="pre">n_jobs</span></code> 参数实现。
如果设置 <code class="docutils literal notranslate"><span class="pre">n_jobs=k</span></code> ，则计算被划分为 <code class="docutils literal notranslate"><span class="pre">k</span></code> 个作业，并运行在机器的 <code class="docutils literal notranslate"><span class="pre">k</span></code> 个核上。 如果设置 <code class="docutils literal notranslate"><span class="pre">n_jobs</span> <span class="pre">=</span> <span class="pre">-1</span></code> ，则使用机器的所有核。
注意由于进程间通信具有一定的开销，这里的提速并不是线性的（即，使用 <code class="docutils literal notranslate"><span class="pre">k</span></code> 个作业不会快 <code class="docutils literal notranslate"><span class="pre">k</span></code> 倍）。
当然，在建立大量的树，或者构建单个树需要相当长的时间（例如，在大数据集上）时，（通过并行化）仍然可以实现显著的加速。</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/ensemble/plot_forest_iris.html#sphx-glr-auto-examples-ensemble-plot-forest-iris-py"><span class="std std-ref">Plot the decision surfaces of ensembles of trees on the iris dataset</span></a></li>
<li><a class="reference internal" href="../auto_examples/ensemble/plot_forest_importances_faces.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-faces-py"><span class="std std-ref">Pixel importances with a parallel forest of trees</span></a></li>
<li><a class="reference internal" href="../auto_examples/plot_multioutput_face_completion.html#sphx-glr-auto-examples-plot-multioutput-face-completion-py"><span class="std std-ref">Face completion with a multi-output estimators</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献</p>
<table class="docutils citation" frame="void" id="b2001" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id8">[B2001]</a></td><td><ol class="first last upperalpha simple" start="12">
<li>Breiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001.</li>
</ol>
</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="b1998" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id6">[B1998]</a></td><td><ol class="first last upperalpha simple" start="12">
<li>Breiman, “Arcing Classifiers”, Annals of Statistics 1998.</li>
</ol>
</td></tr>
</tbody>
</table>
<ul class="simple">
<li>P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized
trees”, Machine Learning, 63(1), 3-42, 2006.</li>
</ul>
</div>
</div>
<div class="section" id="random-forest-feature-importance">
<span id="id12"></span><h3>1.11.2.5. 特征重要性评估<a class="headerlink" href="#random-forest-feature-importance" title="Permalink to this headline">¶</a></h3>
<p>特征对目标变量预测的相对重要性可以通过（树中的决策节点的）特征使用的相对顺序（即深度）来进行评估。
决策树顶部使用的特征对更大一部分输入样本的最终预测决策做出贡献；
因此，树顶部的那些特征所贡献的期望样本比例(<strong>expected fraction of the samples</strong>) 可以作为
特征的相对重要性(<strong>relative importance of the features</strong>) 的一个估计 。
在scikit-learn中，一个特征所贡献的样本的比例与分解它们所产生的不纯度的减少相结合，从而创建了对该特征的预测能力的归一化估计。</p>
<p>通过对多个随机树的预测能力的估计进行平均化(<strong>averaging</strong> )，可以**减小该估计的方差**，并将其用于特征选择。
这种方法被称为 不纯度平均减小(mean decrease in impurity), 或 MDI。
请参考 <a class="reference internal" href="#l2014" id="id13">[L2014]</a> 获得更多关于 MDI 和 用随机森林进行特征重要行评估 的信息。</p>
<p>下面的例子展示了一个面部识别任务中每个像素的相对重要性，其中重要性由颜色（的深浅）来表示，使用的模型是
<a class="reference internal" href="generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier" title="sklearn.ensemble.ExtraTreesClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExtraTreesClassifier</span></code></a> 模型.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/ensemble/plot_forest_importances_faces.html"><img alt="../images/sphx_glr_plot_forest_importances_faces_0011.png" src="../images/sphx_glr_plot_forest_importances_faces_0011.png" style="width: 360.0px; height: 360.0px;" /></a>
</div>
<p>实际上，对于训练完成的模型这些估计值存储在 <code class="docutils literal notranslate"><span class="pre">feature_importances_</span></code> 属性中。
这是一个大小为 <code class="docutils literal notranslate"><span class="pre">(n_features,)</span></code> 的数组，其每个元素值为正，并且总和为 1.0。
一个元素的值越高，其对应的特征对预测函数的贡献越大。</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/ensemble/plot_forest_importances_faces.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-faces-py"><span class="std std-ref">Pixel importances with a parallel forest of trees</span></a></li>
<li><a class="reference internal" href="../auto_examples/ensemble/plot_forest_importances.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-py"><span class="std std-ref">Feature importances with forests of trees</span></a></li>
</ul>
</div>
<div class="topic" id="random-trees-embedding">
<p class="topic-title first">参考文献</p>
<table class="docutils citation" frame="void" id="l2014" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id13">[L2014]</a></td><td>G. Louppe,
“Understanding Random Forests: From Theory to Practice”,
PhD Thesis, U. of Liege, 2014.</td></tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="id14">
<h3>1.11.2.6. 完全随机树嵌入<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.ensemble.RandomTreesEmbedding.html#sklearn.ensemble.RandomTreesEmbedding" title="sklearn.ensemble.RandomTreesEmbedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">RandomTreesEmbedding</span></code></a> implements an unsupervised transformation of the
data.  Using a forest of completely random trees, <a class="reference internal" href="generated/sklearn.ensemble.RandomTreesEmbedding.html#sklearn.ensemble.RandomTreesEmbedding" title="sklearn.ensemble.RandomTreesEmbedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">RandomTreesEmbedding</span></code></a>
encodes the data by the indices of the leaves a data point ends up in.  This
index is then encoded in a one-of-K manner, leading to a high dimensional,
sparse binary coding.
This coding can be computed very efficiently and can then be used as a basis
for other learning tasks.
The size and sparsity of the code can be influenced by choosing the number of
trees and the maximum depth per tree. For each tree in the ensemble, the coding
contains one entry of one. The size of the coding is at most <code class="docutils literal notranslate"><span class="pre">n_estimators</span> <span class="pre">*</span> <span class="pre">2</span>
<span class="pre">**</span> <span class="pre">max_depth</span></code>, the maximum number of leaves in the forest.</p>
<p>As neighboring data points are more likely to lie within the same leaf of a tree,
the transformation performs an implicit, non-parametric density estimation.</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/ensemble/plot_random_forest_embedding.html#sphx-glr-auto-examples-ensemble-plot-random-forest-embedding-py"><span class="std std-ref">Hashing feature transformation using Totally Random Trees</span></a></li>
<li><a class="reference internal" href="../auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py"><span class="std std-ref">Manifold learning on handwritten digits: Locally Linear Embedding, Isomap…</span></a> compares non-linear
dimensionality reduction techniques on handwritten digits.</li>
<li><a class="reference internal" href="../auto_examples/ensemble/plot_feature_transformation.html#sphx-glr-auto-examples-ensemble-plot-feature-transformation-py"><span class="std std-ref">Feature transformations with ensembles of trees</span></a> compares
supervised and unsupervised tree based feature transformations.</li>
</ul>
</div>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="manifold.html#manifold"><span class="std std-ref">流形学习(Manifold learning)</span></a> techniques can also be useful to derive non-linear
representations of feature space, also these approaches focus also on
dimensionality reduction.</p>
</div>
</div>
</div>
<div class="section" id="adaboost">
<span id="id15"></span><h2>1.11.3. 自适应推举算法(AdaBoost)<a class="headerlink" href="#adaboost" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="classes.html#module-sklearn.ensemble" title="sklearn.ensemble"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.ensemble</span></code></a> 模块 包括了一种推举算法(boosting algorithm)，叫 <strong>AdaBoost</strong>, 它是1995年由 Freund 和 Schapire <a class="reference internal" href="#fs1995" id="id16">[FS1995]</a> 引入的。</p>
<p>AdaBoost 的主要原则是 在被重复采样的数据上拟合一个弱学习器序列。(weak learner: 指的是那些比随机猜测稍好一点的模型，比如 小决策树)。
这些弱学习器的预测结果随后会通过加权投票的方式被组合起来产生最终的预测。
在每一次称之为 推举迭代(boosting iteration) 中，数据修改(data modifications)就是把权重 <span class="math notranslate nohighlight">\(w_1\)</span>, <span class="math notranslate nohighlight">\(w_2\)</span>, …, <span class="math notranslate nohighlight">\(w_N\)</span>
分配给每一个训练样本。在迭代开始的时候，所有的样本权重都被设置为 <span class="math notranslate nohighlight">\(w_i = 1/N\)</span>，这样第一步迭代就是在原始数据上简单的训练一个弱分类器 。
在后续的迭代步骤中，样本权重会被单独修改，然后学习算法被重新用到被重新加权的样本数据上。在某个给定的迭代步，那些没有被上一步得到的boosted model
正确预测的训练样本的权重就会增加，而那些已经被上一步得到的模型(boosted model)正确预测的样本的权重会被降低。
随着迭代过程的推进，那些比较难预测的样本会获得不断增加的权重。每一个后继的弱学习器就会被强制聚焦到这些被以前的学习器序列错分的权重较高的难分样本上[HTF]_。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/ensemble/plot_adaboost_hastie_10_2.html"><img alt="../images/sphx_glr_plot_adaboost_hastie_10_2_0011.png" src="../images/sphx_glr_plot_adaboost_hastie_10_2_0011.png" style="width: 480.0px; height: 360.0px;" /></a>
</div>
<p>AdaBoost 既可以用于分类问题也可以用于回归问题:</p>
<blockquote>
<div><ul class="simple">
<li>对于多类分类问题, <a class="reference internal" href="generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier" title="sklearn.ensemble.AdaBoostClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaBoostClassifier</span></code></a> 类实现了两个变体版本：AdaBoost-SAMME 和 AdaBoost-SAMME.R <a class="reference internal" href="#zzrh2009" id="id17">[ZZRH2009]</a>。</li>
<li>对于回归问题, <a class="reference internal" href="generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor" title="sklearn.ensemble.AdaBoostRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaBoostRegressor</span></code></a> 类实现了 AdaBoost.R2 <a class="reference internal" href="#d1997" id="id18">[D1997]</a>.</li>
</ul>
</div></blockquote>
<div class="section" id="id19">
<h3>1.11.3.1. 用法<a class="headerlink" href="#id19" title="Permalink to this headline">¶</a></h3>
<p>下面的例子展示了如何拟合一个用100个弱学习器构建的 AdaBoost 分类器:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">cross_val_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">AdaBoostClassifier</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>                             
<span class="go">0.9...</span>
</pre></div>
</div>
<p>弱学习器的数量由参数 <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> 控制。 参数 <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> 控制着在最终的集成学习器中每个弱学习器的贡献量。
默认情况下，弱学习器都是 决策树桩(decision stumps)。弱学习器可以通过参数 <code class="docutils literal notranslate"><span class="pre">base_estimator</span></code> 指定。为了获得好的结果需要调节的主要参数
有 <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> 和 基本学习器的复杂度参数(比如 树的最大深度 <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> 和 单个划分上需要的最小样本量 <code class="docutils literal notranslate"><span class="pre">min_samples_split</span></code>)。</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/ensemble/plot_adaboost_hastie_10_2.html#sphx-glr-auto-examples-ensemble-plot-adaboost-hastie-10-2-py"><span class="std std-ref">Discrete versus Real AdaBoost</span></a> compares the
classification error of a decision stump, decision tree, and a boosted
decision stump using AdaBoost-SAMME and AdaBoost-SAMME.R.</li>
<li><a class="reference internal" href="../auto_examples/ensemble/plot_adaboost_multiclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-multiclass-py"><span class="std std-ref">Multi-class AdaBoosted Decision Trees</span></a> shows the performance
of AdaBoost-SAMME and AdaBoost-SAMME.R on a multi-class problem.</li>
<li><a class="reference internal" href="../auto_examples/ensemble/plot_adaboost_twoclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-twoclass-py"><span class="std std-ref">Two-class AdaBoost</span></a> shows the decision boundary
and decision function values for a non-linearly separable two-class problem
using AdaBoost-SAMME.</li>
<li><a class="reference internal" href="../auto_examples/ensemble/plot_adaboost_regression.html#sphx-glr-auto-examples-ensemble-plot-adaboost-regression-py"><span class="std std-ref">Decision Tree Regression with AdaBoost</span></a> demonstrates regression
with the AdaBoost.R2 algorithm.</li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献</p>
<table class="docutils citation" frame="void" id="fs1995" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id16">[FS1995]</a></td><td>Y. Freund, and R. Schapire, “A Decision-Theoretic Generalization of
On-Line Learning and an Application to Boosting”, 1997.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="zzrh2009" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id17">[ZZRH2009]</a></td><td>J. Zhu, H. Zou, S. Rosset, T. Hastie. “Multi-class AdaBoost”,
2009.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="d1997" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id18">[D1997]</a></td><td><ol class="first last upperalpha simple" start="8">
<li>Drucker. “Improving Regressors using Boosting Techniques”, 1997.</li>
</ol>
</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="htf" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[HTF]</td><td>T. Hastie, R. Tibshirani and J. Friedman, “Elements of
Statistical Learning Ed. 2”, Springer, 2009.</td></tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="section" id="gradient-tree-boosting">
<span id="gradient-boosting"></span><h2>1.11.4. Gradient Tree Boosting<a class="headerlink" href="#gradient-tree-boosting" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Gradient_boosting">Gradient Tree Boosting</a>
或者叫 Gradient Boosted Regression Trees (GBRT) 是 boosting 在任意可微损失函数上的推广。
GBRT 是一个准确高效的现成的程序(procedure)，可被用于回归和分类问题。
Gradient Tree Boosting 模型被广泛用于各个领域包括网站搜索排序和生态学。</p>
<p>GBRT 的优点是:</p>
<blockquote>
<div><ul class="simple">
<li>混合数据类型的自然操作 (= 各种各样的异质特征 heterogeneous features)</li>
<li>预测能力(Predictive power)</li>
<li>对输出空间中离群点的鲁棒性 (通过鲁棒的损失函数做到这一点)</li>
</ul>
</div></blockquote>
<p>GBRT的缺点:</p>
<blockquote>
<div><ul class="simple">
<li>可伸缩性(Scalability), 由于 boosting 方法的序列化特性， 很难被并行化。</li>
</ul>
</div></blockquote>
<p><a class="reference internal" href="classes.html#module-sklearn.ensemble" title="sklearn.ensemble"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.ensemble</span></code></a> 模块 提供了使用 gradient boosted regression trees 的 分类和回归算法。</p>
<div class="section" id="id21">
<h3>1.11.4.1. 分类<a class="headerlink" href="#id21" title="Permalink to this headline">¶</a></h3>
<p>类 <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier" title="sklearn.ensemble.GradientBoostingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code></a> 支持 二分类和多分类任务。</p>
<p>下面的例子展示了如何拟合一个 带有100个决策树桩(decision stumps)作为弱学习器的 gradient boosting classifier</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">make_hastie_10_2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">GradientBoostingClassifier</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_hastie_10_2</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">2000</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">2000</span><span class="p">:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="mi">2000</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">2000</span><span class="p">:]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>                 
<span class="go">0.913...</span>
</pre></div>
</div>
<p>弱学习器的数量 (i.e. regression trees) 通过参数 <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> 设置;
<a class="reference internal" href="#gradient-boosting-tree-size"><span class="std std-ref">The size of each tree</span></a> 可以通过设置树深度 <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> 或者
通过 <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> 设置叶节点的数量。 学习率(<code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>)是一个介于(0.0, 1.0]之间的超参数，
它通过 <a class="reference internal" href="#gradient-boosting-shrinkage"><span class="std std-ref">shrinkage</span></a> 控制着过拟合 .</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">超过两类的分类问题在每一次迭代时需要归纳 <code class="docutils literal notranslate"><span class="pre">n_classes</span></code> 个回归树。因此，所有的需要归纳的树数量等于 <code class="docutils literal notranslate"><span class="pre">n_classes</span> <span class="pre">*</span> <span class="pre">n_estimators</span></code> 。
对于拥有大量类别的数据集我们强烈推荐使用 <a class="reference internal" href="generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier" title="sklearn.ensemble.RandomForestClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">RandomForestClassifier</span></code></a> 来代替 <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier" title="sklearn.ensemble.GradientBoostingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code></a>  。</p>
</div>
</div>
<div class="section" id="id22">
<h3>1.11.4.2. 回归<a class="headerlink" href="#id22" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor" title="sklearn.ensemble.GradientBoostingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code></a> 支持一系列 ref:<cite>不同的损失函数 &lt;gradient_boosting_loss&gt;</cite>
用于回归，可以用参数 <code class="docutils literal notranslate"><span class="pre">loss</span></code> 来设置; 用于回归的默认损失函数是： least squares (<code class="docutils literal notranslate"><span class="pre">'ls'</span></code>).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">mean_squared_error</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">make_friedman1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">GradientBoostingRegressor</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_friedman1</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1200</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">200</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">200</span><span class="p">:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="mi">200</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">200</span><span class="p">:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">est</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;ls&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>    
<span class="go">5.00...</span>
</pre></div>
</div>
<p>下图展示了应用损失函数为最小二乘损失，基学习器个数为 500 的 <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor" title="sklearn.ensemble.GradientBoostingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code></a> 来处理
波士顿房价数据集 (<a class="reference internal" href="generated/sklearn.datasets.load_boston.html#sklearn.datasets.load_boston" title="sklearn.datasets.load_boston"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.datasets.load_boston</span></code></a>) 的结果。左图表示每一次迭代的训练误差和测试误差。
每一次迭代的训练误差保存在 gradient boosting model 的 <code class="xref py py-attr docutils literal notranslate"><span class="pre">train_score_</span></code> 属性中，
每一次迭代的测试误差能够通过 <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor.staged_predict" title="sklearn.ensemble.GradientBoostingRegressor.staged_predict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">staged_predict</span></code></a> 方法获取，该方法返回一个生成器，
用来产生每一步迭代的预测结果。类似下面这样的图表， 可以用于决定最优的树的数量，从而进行提前停止。右图表示每个特征的重要性，
它 可以通过 <code class="docutils literal notranslate"><span class="pre">feature_importances_</span></code> 属性来获取.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/ensemble/plot_gradient_boosting_regression.html"><img alt="../images/sphx_glr_plot_gradient_boosting_regression_0011.png" src="../images/sphx_glr_plot_gradient_boosting_regression_0011.png" style="width: 900.0px; height: 450.0px;" /></a>
</div>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py"><span class="std std-ref">Gradient Boosting regression</span></a></li>
<li><a class="reference internal" href="../auto_examples/ensemble/plot_gradient_boosting_oob.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-oob-py"><span class="std std-ref">Gradient Boosting Out-of-Bag estimates</span></a></li>
</ul>
</div>
</div>
<div class="section" id="gradient-boosting-warm-start">
<span id="id23"></span><h3>1.11.4.3. 拟合附加的弱学习器<a class="headerlink" href="#gradient-boosting-warm-start" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor" title="sklearn.ensemble.GradientBoostingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code></a> 和 <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier" title="sklearn.ensemble.GradientBoostingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code></a> 都支持 <code class="docutils literal notranslate"><span class="pre">warm_start=True</span></code> ，
它允许你添加更多的估计器到一个已经训练好的模型上。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># set warm_start and new nr of trees</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="c1"># fit additional 100 trees to est</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>    
<span class="go">3.84...</span>
</pre></div>
</div>
</div>
<div class="section" id="treesize">
<span id="gradient-boosting-tree-size"></span><h3>1.11.4.4. 控制Tree的size<a class="headerlink" href="#treesize" title="Permalink to this headline">¶</a></h3>
<p>回归树基学习器的大小定义了能够被  gradient boosting model 捕捉的变量（即特征）相互作用（即多个特征共同对预测产生影响）的程度。
总的来讲, 一棵深度为 <code class="docutils literal notranslate"><span class="pre">h</span></code> 的树能够捕捉 <code class="docutils literal notranslate"><span class="pre">h</span></code> 阶的相互作用(interactions)。有两种方法用来控制单个回归树的大小：</p>
<p>如果你指定 <code class="docutils literal notranslate"><span class="pre">max_depth=h</span></code> ，那么将会产生一个深度为 <code class="docutils literal notranslate"><span class="pre">h</span></code> 的完全二叉树。这棵树将会有（至多） <code class="docutils literal notranslate"><span class="pre">2**h</span></code> 个叶子节点(leaf nodes)和
<code class="docutils literal notranslate"><span class="pre">2**h</span> <span class="pre">-</span> <span class="pre">1</span></code> 个切分节点(split nodes)。</p>
<p>另外，你能通过参数 <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> 指定叶子节点的数量来控制树的大小。在这种情况下，
树将会使用最优优先搜索(best-first search)来生成，这种搜索方式是通过每次选取对不纯度提升最大的节点来展开。
一棵 <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes=k</span></code> 的树拥有 <code class="docutils literal notranslate"><span class="pre">k</span> <span class="pre">-</span> <span class="pre">1</span></code> 个切分节点，因此可以建模阶数最高达到 <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span> <span class="pre">-</span> <span class="pre">1</span></code> 阶
的相互作用（即 max_leaf_nodes - 1 个特征共同决定预测值）。</p>
<p>我们发现 <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes=k</span></code> 可以给出与  <code class="docutils literal notranslate"><span class="pre">max_depth=k-1</span></code> 品质相当的结果，但是其训练速度明显更快，
同时也会以多一点的训练误差作为代价。
参数 <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> 对应于文章 <a class="reference internal" href="#f2001" id="id24">[F2001]</a> 中gradient boosting章节中的变量 <code class="docutils literal notranslate"><span class="pre">J</span></code> ，同时与 R 语言的 gbm 包的参数
<code class="docutils literal notranslate"><span class="pre">interaction.depth</span></code> 相关， 两者间的关系是 <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span> <span class="pre">==</span> <span class="pre">interaction.depth</span> <span class="pre">+</span> <span class="pre">1</span></code> 。</p>
</div>
<div class="section" id="id25">
<h3>1.11.4.5. 数学表达式<a class="headerlink" href="#id25" title="Permalink to this headline">¶</a></h3>
<p>GBRT 是一种具有以下形式的加性模型(additive models):</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[F(x) = \sum_{m=1}^{M} \gamma_m h_m(x)\]</div>
</div></blockquote>
<p>其中 <span class="math notranslate nohighlight">\(h_m(x)\)</span> 是基本函数，在 Boosting 算法家族中通常被称为 弱学习器 (<em>weak learners</em> )。
Gradient Tree Boosting 使用固定大小的 <a class="reference internal" href="tree.html#tree"><span class="std std-ref">decision trees</span></a> 作为弱学习器。
决策树具有许多有价值的提升能力，即处理混合类型数据的能力和建立复杂函数模型的能力。</p>
<p>与其他的boosting算法类似, GBRT以贪心(greedy)的方式构建加性模型:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[F_m(x) = F_{m-1}(x) + \gamma_m h_m(x),\]</div>
</div></blockquote>
<p>当给定上一步产生的集成模型(previous ensemble) <span class="math notranslate nohighlight">\(F_{m-1}\)</span> 时， 新添加的树 <span class="math notranslate nohighlight">\(h_m\)</span> 尝试最小化损失 <span class="math notranslate nohighlight">\(L\)</span> :</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[h_m =  \arg\min_{h} \sum_{i=1}^{n} L(y_i,
F_{m-1}(x_i) + h(x_i)).\]</div>
</div></blockquote>
<p>初始模型 <span class="math notranslate nohighlight">\(F_{0}\)</span> 是问题相关(problem specific)的，对于最小二乘回归，通常选择目标值的均值。</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">初始化模型 <span class="math notranslate nohighlight">\(F_{0}\)</span> 也可以通过参数 <code class="docutils literal notranslate"><span class="pre">init</span></code> 来指定。
传入的对象必须实现 <code class="docutils literal notranslate"><span class="pre">fit</span></code> 和 <code class="docutils literal notranslate"><span class="pre">predict</span></code> 方法。</p>
</div>
<p>Gradient Boosting 尝试通过最陡下降(steepest descent)方法来数值化地求解上述最小化问题:
最陡下降方向是在当前模型 <span class="math notranslate nohighlight">\(F_{m-1}\)</span> 上计算出的损失函数的负梯度方向，这可以为任何可微损失函数计算:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[F_m(x) = F_{m-1}(x) - \gamma_m \sum_{i=1}^{n} \nabla_F L(y_i,
F_{m-1}(x_i))\]</div>
</div></blockquote>
<p>其中 步长 <span class="math notranslate nohighlight">\(\gamma_m\)</span> 是使用线性搜索(line search)选择的:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\gamma_m = \arg\min_{\gamma} \sum_{i=1}^{n} L(y_i, F_{m-1}(x_i)
- \gamma \frac{\partial L(y_i, F_{m-1}(x_i))}{\partial F_{m-1}(x_i)})\]</div>
</div></blockquote>
<p>用于回归的GBRT算法和用于分类的GBRT算法的区别仅在于具体的损失函数的使用上。</p>
<div class="section" id="gradient-boosting-loss">
<span id="id26"></span><h4>1.11.4.5.1. 损失函数<a class="headerlink" href="#gradient-boosting-loss" title="Permalink to this headline">¶</a></h4>
<p>GBRT支持以下损失函数，可以通过参数 <code class="docutils literal notranslate"><span class="pre">loss</span></code> 指定 :</p>
<blockquote>
<div><ul class="simple">
<li>用于回归的损失函数<ul>
<li>Least squares (<code class="docutils literal notranslate"><span class="pre">'ls'</span></code>): 由于其优越的计算性能，是回归问题的自然选择。初始模型 <span class="math notranslate nohighlight">\(F_{0}\)</span> 由 目标变量的平均值 给出。</li>
<li>Least absolute deviation (<code class="docutils literal notranslate"><span class="pre">'lad'</span></code>): 一个用于回归的鲁棒的损失函数。初始模型 <span class="math notranslate nohighlight">\(F_{0}\)</span> 由 目标变量的中值 给出。</li>
<li>Huber (<code class="docutils literal notranslate"><span class="pre">'huber'</span></code>): 另一个用于回归的鲁棒的损失函数，它组合了least squares 和 least absolute deviation;
使用 <code class="docutils literal notranslate"><span class="pre">alpha</span></code> 参数来控制损失函数对离群点(outliers)的敏感度(sensitivity)，(请看 <a class="reference internal" href="#f2001" id="id27">[F2001]</a> 获得更多详情)。</li>
<li>Quantile (<code class="docutils literal notranslate"><span class="pre">'quantile'</span></code>): 一个用于分位数回归(quantile regression)的损失函数。
使用 <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">alpha</span> <span class="pre">&lt;</span> <span class="pre">1</span></code> 来指定分位数。 这个损失函数可以用来创造预测区间(prediction intervals)
(请看案例 <a class="reference internal" href="../auto_examples/ensemble/plot_gradient_boosting_quantile.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-quantile-py"><span class="std std-ref">Prediction Intervals for Gradient Boosting Regression</span></a>)。</li>
</ul>
</li>
<li>用于分类的损失函数<ul>
<li>Binomial deviance (<code class="docutils literal notranslate"><span class="pre">'deviance'</span></code>): 用于二元分类的负二项式对数似然损失函数(提供概率估计(probability estimates))。初始模型 <span class="math notranslate nohighlight">\(F_{0}\)</span> 由
log odds-ratio 给出。</li>
<li>Multinomial deviance (<code class="docutils literal notranslate"><span class="pre">'deviance'</span></code>): 用于多元分类(有 <code class="docutils literal notranslate"><span class="pre">n_classes</span></code> 个互斥类)的负多项式对数似然损失函数(也提供概率估计(probability estimates))。
初始模型 <span class="math notranslate nohighlight">\(F_{0}\)</span> 由每个类的先验概率给出。在每一次迭代中必须构建 <code class="docutils literal notranslate"><span class="pre">n_classes</span></code> 棵回归树使得 GBRT 在具有大量类的数据集上相当低效。</li>
<li>Exponential loss (<code class="docutils literal notranslate"><span class="pre">'exponential'</span></code>): 与 <a class="reference internal" href="generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier" title="sklearn.ensemble.AdaBoostClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaBoostClassifier</span></code></a> 的损失函数一样。 与 <code class="docutils literal notranslate"><span class="pre">'deviance'</span></code> 相比，对于误标记的样本的鲁棒性较差。
只能用于二元分类问题。</li>
</ul>
</li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="id28">
<h3>1.11.4.6. 正则化<a class="headerlink" href="#id28" title="Permalink to this headline">¶</a></h3>
<div class="section" id="shrinkage">
<span id="gradient-boosting-shrinkage"></span><h4>1.11.4.6.1. Shrinkage<a class="headerlink" href="#shrinkage" title="Permalink to this headline">¶</a></h4>
<p><a class="reference internal" href="#f2001" id="id29">[F2001]</a> 提出了一种简单的正则化策略，通过因子 <span class="math notranslate nohighlight">\(\nu\)</span> 来衡量(scale)每个弱学习器的贡献：</p>
<div class="math notranslate nohighlight">
\[F_m(x) = F_{m-1}(x) + \nu \gamma_m h_m(x)\]</div>
<p>参数 <span class="math notranslate nohighlight">\(\nu\)</span> 被称之为 <strong>learning rate</strong>， 因为它可以控制梯度下降的步长, 并且可以通过 <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> 参数来设置。</p>
<p>参数 <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> 和参数 <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> 之间有很强的制约关系。
较小的 <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> 需要大量的弱分类器才能维持训练误差的稳定。经验表明数值较小的 <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> 将会得到更好的测试误差。
<a class="reference internal" href="#htf2009" id="id30">[HTF2009]</a> 推荐把 <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> 设置为一个较小的常数 (例如: <code class="docutils literal notranslate"><span class="pre">learning_rate</span> <span class="pre">&lt;=</span> <span class="pre">0.1</span></code> )同时通过提前停止策略来选择合适的 <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>。
有关 <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> 和 <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> 更详细的讨论可以参考 <a class="reference internal" href="#r2007" id="id31">[R2007]</a>.</p>
</div>
<div class="section" id="subsampling">
<h4>1.11.4.6.2. Subsampling<a class="headerlink" href="#subsampling" title="Permalink to this headline">¶</a></h4>
<p><a class="reference internal" href="#f1999" id="id32">[F1999]</a> 提出了一种将梯度增强(gradient boosting)和自举平均(bootstrap averaging (bagging))相结合的随机梯度增强方法。
在每次迭代中，基分类器在可用训练数据的一部分子样本 <code class="docutils literal notranslate"><span class="pre">subsample</span></code> 上被训练。子采样(subsample)是通过无放回采样获得的(drawn without replacement)。
<code class="docutils literal notranslate"><span class="pre">subsample</span></code> 的典型值为0.5。</p>
<p>下图展示了收缩和子采样对于模型拟合好坏的影响。我们可以明显看到指定收缩率比没有收缩拥有更好的表现。
而将子采样和收缩率相结合能进一步的提高模型的准确率。相反，使用子采样而不使用收缩的结果十分糟糕。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/ensemble/plot_gradient_boosting_regularization.html"><img alt="../images/sphx_glr_plot_gradient_boosting_regularization_0011.png" src="../images/sphx_glr_plot_gradient_boosting_regularization_0011.png" style="width: 480.0px; height: 360.0px;" /></a>
</div>
<p>另一个减少方差的策略是特征子采样,这种方法类似于 <a class="reference internal" href="generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier" title="sklearn.ensemble.RandomForestClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">RandomForestClassifier</span></code></a> 中的随机分割。子采样的特征数量可以通过参数 <code class="docutils literal notranslate"><span class="pre">max_features</span></code> 来控制。</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">使用一个小的 <code class="docutils literal notranslate"><span class="pre">max_features</span></code> 值能够显著降低计算时间。</p>
</div>
<p>随机梯度提升(Stochastic gradient boosting)允许计算测试偏差的袋外估计值（Out-of-bag），方法是计算那些不在自助采样之内的样本偏差的改进
(i.e. the out-of-bag examples)。
这个改进保存在属性 <code class="xref py py-attr docutils literal notranslate"><span class="pre">oob_improvement_</span></code> 中。
<code class="docutils literal notranslate"><span class="pre">oob_improvement_[i]</span></code> holds the improvement in terms of the loss on the OOB samples if you add the i-th stage
to the current predictions.
袋外估计可以用于模型选择中，例如决定最优迭代次数。 OOB 估计通常都很悲观(pessimistic), 因此我们推荐使用交叉验证来代替它，
而当交叉验证太耗时时我们就只能使用 OOB 了。</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/ensemble/plot_gradient_boosting_regularization.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regularization-py"><span class="std std-ref">Gradient Boosting regularization</span></a></li>
<li><a class="reference internal" href="../auto_examples/ensemble/plot_gradient_boosting_oob.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-oob-py"><span class="std std-ref">Gradient Boosting Out-of-Bag estimates</span></a></li>
<li><a class="reference internal" href="../auto_examples/ensemble/plot_ensemble_oob.html#sphx-glr-auto-examples-ensemble-plot-ensemble-oob-py"><span class="std std-ref">OOB Errors for Random Forests</span></a></li>
</ul>
</div>
</div>
</div>
<div class="section" id="id33">
<h3>1.11.4.7. 对结果的解释<a class="headerlink" href="#id33" title="Permalink to this headline">¶</a></h3>
<p>通过简单地可视化树结构可以很容易地解释单个决策树,然而对于梯度提升模型来说,一般拥有数百棵/种回归树，
将每一棵树都可视化来解释整个模型是很困难的。
幸运的是，有很多关于总结和解释梯度提升模型的技术。</p>
<div class="section" id="id34">
<h4>1.11.4.7.1. 特征重要性<a class="headerlink" href="#id34" title="Permalink to this headline">¶</a></h4>
<p>通常，各个特征分量对目标响应的预测所做的贡献是不一样的。在很多情形中，大多数特征分量之间是不相关的。
当我们要解释一个模型的时候, 遇到的第一个问题通常是：最重要的特征是哪些？它们对目标响应的预测做了怎样的贡献？</p>
<p>单个决策树本质上就是通过选择适当的分割点来进行特征选择的一种模型。这些信息可以用来度量每个特征的重要性，
其基本的思想是：如果一个特征在树的分割节点中用的越频繁，则这个特征的重要性就越高。 这种特征重要性的概念可以
通过简单的平均一下每棵树上的特征重要性扩展到决策树集合，(请看 <a class="reference internal" href="#random-forest-feature-importance"><span class="std std-ref">特征重要性评估</span></a> 获得更多详情)。</p>
<p>一个经过拟合得的gradient boosting model的特征重要性得分可以通过参数 <code class="docutils literal notranslate"><span class="pre">feature_importances_</span></code> 获得:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">make_hastie_10_2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">GradientBoostingClassifier</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_hastie_10_2</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">feature_importances_</span>  
<span class="go">array([0.10..., 0.10..., 0.11..., ...</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py"><span class="std std-ref">Gradient Boosting regression</span></a></li>
</ul>
</div>
</div>
<div class="section" id="partial-dependence">
<span id="id35"></span><h4>1.11.4.7.2. 部分依赖性<a class="headerlink" href="#partial-dependence" title="Permalink to this headline">¶</a></h4>
<p>部分依赖图（Partial dependence plots (PDP)）展示了目标响应和一系列目标特征的依赖关系，
同时边缘化了其他所有特征的取值（候选补充特征）。
直觉上，我们可以将 部分依赖 解释为作为目标特征函数 <a class="footnote-reference" href="#id39" id="id36">[2]</a> 的预期目标响应 <a class="footnote-reference" href="#id38" id="id37">[1]</a>  。</p>
<p>由于人类感知能力的限制，目标特征的设置必须小一点(通常是1到2)，因此目标特征通常在最重要的特征中选择。</p>
<p>下图展示了加州住房数据集的四个单向和一个双向c部分依赖图(four one-way PDP 和 one two-way PDP):</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/ensemble/plot_partial_dependence.html"><img alt="../images/sphx_glr_plot_partial_dependence_0011.png" src="../images/sphx_glr_plot_partial_dependence_0011.png" style="width: 448.0px; height: 336.0px;" /></a>
</div>
<p>单向PDPs (One-way PDPs) 告诉我们目标响应和目标特征的相互影响(例如：线性或者非线性)。
上面的左上图展示了一个地区的中等收入对中等房价的影响。我们可以清楚的看到两者之间是线性相关的。</p>
<p>具有两个目标特征的 PDPs 显示这两个特征之间的相互影响。例如：上图中两个变量的 PDP 展示了房价中位数与房屋年龄和
每户平均入住人数之间的依赖关系。我们能清楚的看到这两个特征之间的影响：对于每户入住均值而言,当其值大于 2 时，
房价与房屋年龄几乎是相对独立的，而其值小于 2 的时，房价对房屋年龄的依赖性就会很强。</p>
<p>模块 <code class="xref py py-mod docutils literal notranslate"><span class="pre">partial_dependence</span></code> 提供了一个方便的函数 <a class="reference internal" href="generated/sklearn.ensemble.partial_dependence.plot_partial_dependence.html#sklearn.ensemble.partial_dependence.plot_partial_dependence" title="sklearn.ensemble.partial_dependence.plot_partial_dependence"><code class="xref py py-func docutils literal notranslate"><span class="pre">plot_partial_dependence</span></code></a>
创建 one-way and two-way partial dependence plots。 在下面的例子中，我们展示了如何创建一个PDPs网格 :
two one-way PDPs for the features <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">1</span></code> 以及 a two-way PDP between the two features:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">make_hastie_10_2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">GradientBoostingClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble.partial_dependence</span> <span class="k">import</span> <span class="n">plot_partial_dependence</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_hastie_10_2</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plot_partial_dependence</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">features</span><span class="p">)</span> 
</pre></div>
</div>
<p>对于多类别的模型，你需要通过 <code class="docutils literal notranslate"><span class="pre">label</span></code> 参数设置类别标签来创建 PDPs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mc_clf</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plot_partial_dependence</span><span class="p">(</span><span class="n">mc_clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> 
</pre></div>
</div>
<p>如果你需要部分依赖函数的原始值而不是图，你可以调用 <a class="reference internal" href="generated/sklearn.ensemble.partial_dependence.partial_dependence.html#sklearn.ensemble.partial_dependence.partial_dependence" title="sklearn.ensemble.partial_dependence.partial_dependence"><code class="xref py py-func docutils literal notranslate"><span class="pre">partial_dependence</span></code></a> 函数</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble.partial_dependence</span> <span class="k">import</span> <span class="n">partial_dependence</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">pdp</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">partial_dependence</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pdp</span>  
<span class="go">array([[ 2.46643157,  2.46643157, ...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">axes</span>  
<span class="go">[array([-1.62497054, -1.59201391, ...</span>
</pre></div>
</div>
<p>该函数需要 通过 <code class="docutils literal notranslate"><span class="pre">grid</span></code> 参数指定应该被评估的部分依赖函数的目标特征值 或 可以
十分便利地通过设置 <code class="docutils literal notranslate"><span class="pre">X</span></code> 参数从而在训练数据中自动创建 <code class="docutils literal notranslate"><span class="pre">grid</span></code> 。
如果 <code class="docutils literal notranslate"><span class="pre">X</span></code> 被给出，函数返回的 <code class="docutils literal notranslate"><span class="pre">axes</span></code> 为每个目标特征提供轴(axis)。</p>
<p>对于 <code class="docutils literal notranslate"><span class="pre">grid</span></code> 中的每一个 ‘目标’ 特征值，部分依赖函数需要边缘化一棵树中所有候选特征(‘complement’ features)的可能值的预测。
在决策树中，这个函数可以在不参考训练数据的情况下被高效的评估，对于每一网格点执行加权遍历:
如果切分点包含 ‘目标’ 特征，遍历其相关的左分支或相关的右分支,否则就遍历两个分支。
每一个分支将被通过进入该分支的训练样本的占比加权， 最后，部分依赖通过所有访问的叶节点的权重的平均值给出。
组合树（tree ensembles）的整体结果，需要对每棵树的结果再次平均得到。</p>
<p class="rubric">Footnotes</p>
<table class="docutils footnote" frame="void" id="id38" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id37">[1]</a></td><td>For classification with <code class="docutils literal notranslate"><span class="pre">loss='deviance'</span></code>  the target
response is logit(p).</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id39" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id36">[2]</a></td><td>More precisely its the expectation of the target response after
accounting for the initial model; partial dependence plots
do not include the <code class="docutils literal notranslate"><span class="pre">init</span></code> model.</td></tr>
</tbody>
</table>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/ensemble/plot_partial_dependence.html#sphx-glr-auto-examples-ensemble-plot-partial-dependence-py"><span class="std std-ref">Partial Dependence Plots</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献</p>
<table class="docutils citation" frame="void" id="f2001" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[F2001]</td><td><em>(<a class="fn-backref" href="#id24">1</a>, <a class="fn-backref" href="#id27">2</a>, <a class="fn-backref" href="#id29">3</a>)</em> J. Friedman, “Greedy Function Approximation: A Gradient Boosting Machine”,
The Annals of Statistics, Vol. 29, No. 5, 2001.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="f1999" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id32">[F1999]</a></td><td><ol class="first last upperalpha simple" start="10">
<li>Friedman, “Stochastic Gradient Boosting”, 1999</li>
</ol>
</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="htf2009" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id30">[HTF2009]</a></td><td><ol class="first last upperalpha simple" start="20">
<li>Hastie, R. Tibshirani and J. Friedman, “Elements of Statistical Learning Ed. 2”, Springer, 2009.</li>
</ol>
</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="r2007" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id31">[R2007]</a></td><td><ol class="first last upperalpha simple" start="7">
<li>Ridgeway, “Generalized Boosted Models: A guide to the gbm package”, 2007</li>
</ol>
</td></tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<div class="section" id="voting-classifier">
<span id="id40"></span><h2>1.11.5. 投票分类器(Voting Classifier)<a class="headerlink" href="#voting-classifier" title="Permalink to this headline">¶</a></h2>
<p>投票分类器(<code class="xref py py-class docutils literal notranslate"><span class="pre">VotingClassifier</span></code>)背后的思想是组合概念上不同的分类器然后使用多数表决法(majority vote)
或 使用对预测概率取平均的方法 来预测类标签。 这种投票分类器在组合一些性能不相上下的模型的时候比较有用，因为这样可以平衡每个单独分类器的弱点。</p>
<div class="section" id="majority-voting">
<h3>1.11.5.1. 多数/硬投票(majority voting)<a class="headerlink" href="#majority-voting" title="Permalink to this headline">¶</a></h3>
<p>在majority voting中，对一个特定样本的预测类标签是所有由每个单独分类器预测出的类标签中出现次数最多的那个类标签。
(译者注：其实就是 票多者胜出 的意思，所以叫 “硬投票” )</p>
<p>举个栗子,对一个给定的样本，有三个分类器都给出他们的预测结果：</p>
<ul class="simple">
<li>classifier 1 -&gt; class 1</li>
<li>classifier 2 -&gt; class 1</li>
<li>classifier 3 -&gt; class 2</li>
</ul>
<p>VotingClassifier (with <code class="docutils literal notranslate"><span class="pre">voting='hard'</span></code>) 将会把这个样本分为 “class 1” 类，因为”class 1” 类得了两票而”class 2” 类只有一票。</p>
<p>在得票数势均力敌的情况下，<cite>VotingClassifier</cite> 将会基于升序来选择类标签。
举个栗子, 在下面的场景中，</p>
<ul class="simple">
<li>classifier 1 -&gt; class 2</li>
<li>classifier 2 -&gt; class 1</li>
</ul>
<p>class label 1 将会被分配给样本。</p>
<div class="section" id="id41">
<h4>1.11.5.1.1. 用法<a class="headerlink" href="#id41" title="Permalink to this headline">¶</a></h4>
<p>下面的例子展示了怎样去拟合硬投票规则的分类器</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">cross_val_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LogisticRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="k">import</span> <span class="n">GaussianNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">VotingClassifier</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">],</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">clf1</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;multinomial&#39;</span><span class="p">,</span>
<span class="gp">... </span>                          <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf2</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf3</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span> <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;hard&#39;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">clf</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">clf1</span><span class="p">,</span> <span class="n">clf2</span><span class="p">,</span> <span class="n">clf3</span><span class="p">,</span> <span class="n">eclf</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;Logistic Regression&#39;</span><span class="p">,</span> <span class="s1">&#39;Random Forest&#39;</span><span class="p">,</span> <span class="s1">&#39;naive Bayes&#39;</span><span class="p">,</span> <span class="s1">&#39;Ensemble&#39;</span><span class="p">]):</span>
<span class="gp">... </span>    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">%0.2f</span><span class="s2"> (+/- </span><span class="si">%0.2f</span><span class="s2">) [</span><span class="si">%s</span><span class="s2">]&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">(),</span> <span class="n">label</span><span class="p">))</span>
<span class="go">Accuracy: 0.95 (+/- 0.04) [Logistic Regression]</span>
<span class="go">Accuracy: 0.94 (+/- 0.04) [Random Forest]</span>
<span class="go">Accuracy: 0.91 (+/- 0.04) [naive Bayes]</span>
<span class="go">Accuracy: 0.95 (+/- 0.04) [Ensemble]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id42">
<h3>1.11.5.2. 加权平均概率(软投票)<a class="headerlink" href="#id42" title="Permalink to this headline">¶</a></h3>
<p>相比于多数(硬)投票法(hard voting), 软投票法(soft voting) 返回预测概率的和的最大值所对应的那个标签作为最终的预测类标签。</p>
<p>可以通过参数 <code class="docutils literal notranslate"><span class="pre">weights</span></code> 给每个分类器分配特定的权重。如果提供了权重，每个分类器预测出的类概率分布都会收集起来并乘以对应的权重然后再取平均。
最终的类标签就是取平均以后的概率分布中最大概率值所对应的那个类标签。</p>
<p>为了用一个简单的例子说明其用法, 假定我们有3个分类器和一个3个类的分类问题，我们给每个分类器分配相同的权重: w1=1, w2=1, w3=1.</p>
<p>对一个给定样本， 其加权平均概率的计算如下所示：</p>
<table border="1" class="docutils">
<colgroup>
<col width="35%" />
<col width="22%" />
<col width="22%" />
<col width="22%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">classifier</th>
<th class="head">class 1</th>
<th class="head">class 2</th>
<th class="head">class 3</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>classifier 1</td>
<td>w1 * 0.2</td>
<td>w1 * 0.5</td>
<td>w1 * 0.3</td>
</tr>
<tr class="row-odd"><td>classifier 2</td>
<td>w2 * 0.6</td>
<td>w2 * 0.3</td>
<td>w2 * 0.1</td>
</tr>
<tr class="row-even"><td>classifier 3</td>
<td>w3 * 0.3</td>
<td>w3 * 0.4</td>
<td>w3 * 0.3</td>
</tr>
<tr class="row-odd"><td>weighted average</td>
<td>0.37</td>
<td>0.4</td>
<td>0.23</td>
</tr>
</tbody>
</table>
<p>在这里, 预测出的类标签是 2, 因为它有最高的平均概率值。</p>
<p>下面的这个例子展示的是用线性SVM, 决策树, 和 K-最近邻 作为基本分类器构建一个soft <cite>VotingClassifier</cite>。
我们要观察当使用这样一个 soft <cite>VotingClassifier</cite> 时决策区域将会如何变化:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="k">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="k">import</span> <span class="n">KNeighborsClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="k">import</span> <span class="n">SVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">itertools</span> <span class="k">import</span> <span class="n">product</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">VotingClassifier</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading some example data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Training classifiers</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf1</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf2</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf3</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">probability</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;dt&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;knn&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;svc&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span>
<span class="gp">... </span>                        <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">clf1</span> <span class="o">=</span> <span class="n">clf1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf2</span> <span class="o">=</span> <span class="n">clf2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf3</span> <span class="o">=</span> <span class="n">clf3</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf</span> <span class="o">=</span> <span class="n">eclf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/ensemble/plot_voting_decision_regions.html"><img alt="../images/sphx_glr_plot_voting_decision_regions_0011.png" src="../images/sphx_glr_plot_voting_decision_regions_0011.png" style="width: 750.0px; height: 600.0px;" /></a>
</div>
</div>
<div class="section" id="id43">
<h3>1.11.5.3. 投票分类器在网格搜索中应用<a class="headerlink" href="#id43" title="Permalink to this headline">¶</a></h3>
<p>投票分类器 (<cite>VotingClassifier</cite>)也可以与网格搜索(<cite>GridSearch</cite>)共同使用来调节每个单独的estimator的超参数</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">GridSearchCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf1</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;multinomial&#39;</span><span class="p">,</span>
<span class="gp">... </span>                          <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf2</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf3</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span> <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;lr__C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">],</span> <span class="s1">&#39;rf__n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">200</span><span class="p">]}</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">eclf</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grid</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="id44">
<h4>1.11.5.3.1. 用法<a class="headerlink" href="#id44" title="Permalink to this headline">¶</a></h4>
<p>为了基于预测出的类概率分布预测类标签(VotingClassifier 中的 estimator必须支持 <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> 方法)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">eclf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span> <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>可选的, 权重可以被提供给每个单独的分类器</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">eclf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span>
<span class="gp">... </span>                        <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
  </div>
  
  <div class="footer">
    &copy; 2007 - 2018, scikit-learn developers (BSD License).
    <!--
    <a href="../_sources/modules/ensemble.rst.txt" rel="nofollow">Show this page source</a> -->
  </div>
  <div class="rel">
    
      <div class="buttonPrevious">
        <a href="tree.html">Previous
        </a>
      </div>
      <div class="buttonNext">
        <a href="multiclass.html">Next
        </a>
      </div>
      
    </div>

    
    <script>
      window.ga = window.ga || function () { (ga.q = ga.q || []).push(arguments) }; ga.l = +new Date;
      ga('create', 'UA-22606712-2', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    
    <script>
      (function () {
        var cx = '016639176250731907682:tjtqbvtvij0';
        var gcse = document.createElement('script'); gcse.type = 'text/javascript'; gcse.async = true;
        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s);
      })();
    </script>
  </body>
</html>