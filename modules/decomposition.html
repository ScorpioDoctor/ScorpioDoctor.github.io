

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

    <title>2.5. 信号分量分解(矩阵因子分解问题) &#8212; scikit-learn 0.20.2 documentation</title>
<!-- htmltitle is before nature.css - we use this hack to load bootstrap first -->
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="../static/css/bootstrap.min.css" media="screen" />
<link rel="stylesheet" href="../static/css/bootstrap-responsive.css" />

    <link rel="stylesheet" href="../static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../static/gallery.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../static/documentation_options.js"></script>
    <script type="text/javascript" src="../static/jquery.js"></script>
    <script type="text/javascript" src="../static/underscore.js"></script>
    <script type="text/javascript" src="../static/doctools.js"></script>
    <script type="text/javascript" src="../static/js/copybutton.js"></script>
    <script type="text/javascript" src="../static/js/extra.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_SVG"></script>
    <link rel="shortcut icon" href="../static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2.6. 协方差估计(Covariance estimation)" href="covariance.html" />
    <link rel="prev" title="2.4. Biclustering" href="biclustering.html" />


<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<script src="../static/js/bootstrap.min.js" type="text/javascript"></script>
<script>
  VERSION_SUBDIR = (function (groups) {
    return groups ? groups[1] : null;
  })(location.href.match(/^https?:\/\/scikit-learn.org\/([^\/]+)/));
</script>
<link rel="canonical" href="http://scikit-learn.org/stable/modules/decomposition.html" />

<script type="text/javascript">
  $("div.buttonNext, div.buttonPrevious").hover(
    function () {
      $(this).css('background-color', '#FF9C34');
    },
    function () {
      $(this).css('background-color', '#A7D6E2');
    }
  );
  function showMenu() {
    var topNav = document.getElementById("scikit-navbar");
    if (topNav.className === "navbar") {
      topNav.className += " responsive";
    } else {
      topNav.className = "navbar";
    }
  };
</script>

<!-- 百度站长统计代码 -->
<script>
  var _hmt = _hmt || [];
  (function () {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?e7836e37a4cb7584127a787e9b44e3f1";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
  })();
</script>


  </head><body>

<div class="header-wrapper">
  <div class="header">
    <p class="logo"><a href="../index.html">
        <img src="../static/scikit-learn-logo-small.png" alt="Logo" />
      </a>
    </p><div class="navbar" id="scikit-navbar">
      <ul>
        <li><a href="../index.html">首页</a></li>
        <li><a href="../install.html">安装</a></li>
        <li class="btn-li">
          <div class="btn-group">
            <a href="../documentation.html">文档</a>
            <a class="btn dropdown-toggle" data-toggle="dropdown">
              <span class="caret"></span>
            </a>
            <ul class="dropdown-menu">
              <li class="link-title">Scikit-learn
                <script>document.write(DOCUMENTATION_OPTIONS.VERSION + (VERSION_SUBDIR ? " (" + VERSION_SUBDIR + ")" : ""));</script>
              </li>
              <li><a href="../tutorial/index.html">教程</a></li>
              <li><a href="../user_guide.html">用户指南</a></li>
              <li><a href="classes.html">API</a></li>
              <li><a href="../glossary.html">词汇表</a></li>
              <li><a href="../faq.html">FAQ</a></li>
              <li><a href="../developers/contributing.html">贡献</a></li>
              <li><a href="../roadmap.html">路线图</a></li>
              <li class="divider"></li>
              <script>if (VERSION_SUBDIR != "stable") document.write('<li><a href="https://www.studyai.cn">稳定版</a></li>')</script>
              <script>if (VERSION_SUBDIR != "dev") document.write('<li><a href="http://scikit-learn.org/dev/documentation.html" target="_blank">开发版</a></li>')</script>
              <li><a href="http://scikit-learn.org/dev/versions.html">所有可用版本</a></li>
              <li><a href="../downloads/scikit-learn-docs.pdf">PDF 文档</a></li>
            </ul>
          </div>
        </li>
        <li><a href="../auto_examples/index.html">案例</a></li>
      </ul>
      <a href="javascript:void(0);" onclick="showMenu()">
        <div class="nav-icon">
          <div class="hamburger-line"></div>
          <div class="hamburger-line"></div>
          <div class="hamburger-line"></div>
        </div>
      </a>
      <div class="search_form">
        <div class="gcse-search" id="cse" style="width: 100%;"></div>
      </div>
    </div> <!-- end navbar --></div>
</div>


<!-- GitHub "fork me" ribbon -->
<a href="https://github.com/scikit-learn/scikit-learn">
  <img class="fork-me" style="position: absolute; top: 0; right: 0; border: 0;" src="../static/img/forkme.png"
    alt="Fork me on GitHub" />
</a>

<div class="content-wrapper">
  <div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
      <div class="rel">
        
          <div class="rellink">
            <a href="biclustering.html" accesskey="P">Previous
              <br />
              <span class="smallrellink">
                2.4. Biclustering
              </span>
              <span class="hiddenrellink">
                2.4. Biclustering
              </span>
            </a>
          </div>
          <div class="spacer">
            &nbsp;
          </div>
          <div class="rellink">
            <a href="covariance.html" accesskey="N">Next
              <br />
              <span class="smallrellink">
                2.6. 协方差估计(Co...
              </span>
              <span class="hiddenrellink">
                2.6. 协方差估计(Covariance estimation)
              </span>
            </a>
          </div>

          <!-- Ad a link to the 'up' page -->
          <div class="spacer">
            &nbsp;
          </div>
          <div class="rellink">
            <a href="../unsupervised_learning.html">
              Up
              <br />
              <span class="smallrellink">
                2. 无监督学习(unsu...
              </span>
                <span class="hiddenrellink">
                  2. 无监督学习(unsupervised learning)
                </span>
                
            </a>
          </div>
        </div>
        
        <p class="doc-version"><b>scikit-learn v0.20.2</b><br />
          <a href="http://scikit-learn.org/dev/versions.html">其他版本</a></p>
        <!-- <p class="citing">Please <b><a href="../about.html#citing-scikit-learn" style="font-size: 110%;">cite
              us </a></b>if you use the software.</p> -->
        <p class="citing">该中文文档由人工智能社区的<a href="http://www.studyai.com/antares" target="_blank">Antares</a>翻译!
        </p>
        <ul>
<li><a class="reference internal" href="#">2.5. 信号分量分解(矩阵因子分解问题)</a><ul>
<li><a class="reference internal" href="#pca">2.5.1. 主成分分析 (PCA)</a><ul>
<li><a class="reference internal" href="#id3">2.5.1.1. 准确的PCA及其概率性解释</a></li>
<li><a class="reference internal" href="#incrementalpca">2.5.1.2. 增量主成分分析</a></li>
<li><a class="reference internal" href="#svdpca">2.5.1.3. 使用随机SVD的PCA</a></li>
<li><a class="reference internal" href="#kernel-pca">2.5.1.4. 核 PCA</a></li>
<li><a class="reference internal" href="#sparsepca-minibatchsparsepca">2.5.1.5. 稀疏主成分分析 (SparsePCA 和 MiniBatchSparsePCA)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#lsa">2.5.2. 截断奇异值分解和隐语义分析</a></li>
<li><a class="reference internal" href="#dictionary-learning">2.5.3. 词典学习(Dictionary Learning)</a><ul>
<li><a class="reference internal" href="#sparsecoder">2.5.3.1. 用预先计算好的字典进行稀疏编码</a></li>
<li><a class="reference internal" href="#id10">2.5.3.2. 通用词典学习</a></li>
<li><a class="reference internal" href="#minibatchdictionarylearning">2.5.3.3. 小批量词典学习</a></li>
</ul>
</li>
<li><a class="reference internal" href="#factor-analysis">2.5.4. 因子分析(Factor Analysis)</a></li>
<li><a class="reference internal" href="#ica">2.5.5. 独立分量分析(ICA)</a></li>
<li><a class="reference internal" href="#nmf-or-nnmf">2.5.6. 非负矩阵分解(NMF or NNMF)</a><ul>
<li><a class="reference internal" href="#frobeniusnmf">2.5.6.1. 带有Frobenius范数的NMF</a></li>
<li><a class="reference internal" href="#beta-divergencenmf">2.5.6.2. 带有beta-divergence的NMF</a></li>
</ul>
</li>
<li><a class="reference internal" href="#dirichlet-lda">2.5.7. 隐Dirichlet分配 (LDA)</a></li>
</ul>
</li>
</ul>

        <br />
        <p>
          <a href="http://world.people.com.cn/n1/2019/1128/c1002-31479407.html" target="_blank">
            <img src="../static/img/advitise1.png" alt="座右铭" />
          </a>
        </p>
        <br />
        <p class="doc-version" style="font-size:10%">
          注意!本网站的网址是以 <em>https://</em> 开头的，而不是以 <em>http://</em> 开头的!!!
        </p>
      </div>
    </div>
    
    <input type="checkbox" id="nav-trigger" class="nav-trigger" checked />
    <label for="nav-trigger"></label>
    
    


    <div class="content">
      
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="decompositions">
<span id="id1"></span><h1>2.5. 信号分量分解(矩阵因子分解问题)<a class="headerlink" href="#decompositions" title="Permalink to this headline">¶</a></h1>
<div class="section" id="pca">
<span id="id2"></span><h2>2.5.1. 主成分分析 (PCA)<a class="headerlink" href="#pca" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id3">
<h3>2.5.1.1. 准确的PCA及其概率性解释<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>PCA 被用来把多元数据集分解成一组连续正交分量的表示，这些正交分量可以用来解释方差的最大量。
在 scikit-learn 中, <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 类被实现为一个变换器对象(<em>transformer</em> object), 可以
在它的 <code class="docutils literal notranslate"><span class="pre">fit</span></code> 方法中学习 <span class="math notranslate nohighlight">\(n\)</span> 个分量(components)，并且可以把新的数据投影到学习到的这些分量上。</p>
<p>可选参数 <code class="docutils literal notranslate"><span class="pre">whiten=True</span></code> 使得它可以将数据投影到奇异空间(singular space),同时将每一个分量缩放到单位方差。
如果pipeline中下游模型对信号的各向同性(isotropy)作了强有力的假设(例如 带有RBF核的支持向量机和K-means聚类算法就做了这个假定)，
这样的变换通常是有用的。</p>
<p>下面是iris dataset上的例子，鸢尾花数据集有四个特征分量组成，将其投影到方差最大的2个纬度上:</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_pca_vs_lda.html"><img alt="../images/sphx_glr_plot_pca_vs_lda_0011.png" src="../images/sphx_glr_plot_pca_vs_lda_0011.png" style="width: 480.0px; height: 360.0px;" /></a>
</div>
<p><a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 类的对象也提供PCA的概率性解释(probabilistic interpretation)，
可以基于它所解释的方差的量给出数据的似然性(likelihood)。
它也实现了一个评分函数: <cite>score</cite> ，还可以被用于交叉验证:</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_pca_vs_fa_model_selection.html"><img alt="../images/sphx_glr_plot_pca_vs_fa_model_selection_0011.png" src="../images/sphx_glr_plot_pca_vs_fa_model_selection_0011.png" style="width: 480.0px; height: 360.0px;" /></a>
</div>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/decomposition/plot_pca_vs_lda.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-lda-py"><span class="std std-ref">Comparison of LDA and PCA 2D projection of Iris dataset</span></a></li>
<li><a class="reference internal" href="../auto_examples/decomposition/plot_pca_vs_fa_model_selection.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-fa-model-selection-py"><span class="std std-ref">Model selection with Probabilistic PCA and Factor Analysis (FA)</span></a></li>
</ul>
</div>
</div>
<div class="section" id="incrementalpca">
<span id="id4"></span><h3>2.5.1.2. 增量主成分分析<a class="headerlink" href="#incrementalpca" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 类非常有用,但是在大数据集上会有某些限制。最大的限制是 <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 类仅支持批量处理，
这意味着所有要被处理的数据都必须在主内存中进行拟合。 <a class="reference internal" href="generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA" title="sklearn.decomposition.IncrementalPCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">IncrementalPCA</span></code></a> 类使用不同的处理形式并且
允许进行部分数据计算(minibatch computation)而且其结果几乎与 <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 的结果一样。
<a class="reference internal" href="generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA" title="sklearn.decomposition.IncrementalPCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">IncrementalPCA</span></code></a> 类使得实现核外(out-of-core)主分量分析成为可能：(通过以下方式)</p>
<blockquote>
<div><ul class="simple">
<li>使用其 <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code> 方法在从硬盘或网络上顺序取来的数据块上拟合</li>
<li>调用它的 fit 方法在使用``numpy.memmap``的内存映射文件上</li>
</ul>
</div></blockquote>
<p><a class="reference internal" href="generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA" title="sklearn.decomposition.IncrementalPCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">IncrementalPCA</span></code></a> 类为了增量式的更新 <code class="docutils literal notranslate"><span class="pre">explained_variance_ratio_</span></code> ，仅需要存储估计出的分量和噪声方差。
这就是为啥内存使用量依赖于每个批次的样本数量，而不是数据集中需要处理的样本总量。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_incremental_pca.html"><img alt="../images/sphx_glr_plot_incremental_pca_0011.png" src="../images/sphx_glr_plot_incremental_pca_0011.png" style="width: 600.0px; height: 600.0px;" /></a>
</div>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_incremental_pca.html"><img alt="../images/sphx_glr_plot_incremental_pca_0021.png" src="../images/sphx_glr_plot_incremental_pca_0021.png" style="width: 600.0px; height: 600.0px;" /></a>
</div>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/decomposition/plot_incremental_pca.html#sphx-glr-auto-examples-decomposition-plot-incremental-pca-py"><span class="std std-ref">Incremental PCA</span></a></li>
</ul>
</div>
</div>
<div class="section" id="svdpca">
<span id="randomizedpca"></span><h3>2.5.1.3. 使用随机SVD的PCA<a class="headerlink" href="#svdpca" title="Permalink to this headline">¶</a></h3>
<p>通过丢弃具有较低奇异值的奇异向量成分，将数据投影到低维空间并保留大部分方差是非常有趣的。</p>
<p>例如，如果我们使用64x64像素的灰度级图像进行人脸识别，数据的维数为4096，
在这样大的数据上训练含RBF内核的支持向量机是很慢的。 此外我们知道数据本质上的维度远低于4096，
因为人脸的所有照片都看起来有点相似。 样本位于许多的很低维度（例如约200维）。
PCA算法可以用于线性变换数据，同时降低维数并同时保留大部分解释方差(explained variance)。</p>
<p>使用可选参数 <code class="docutils literal notranslate"><span class="pre">svd_solver='randomized'</span></code> 的 <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 是非常有用的, 在这种情况下:
因为我们将要丢弃大部分奇异向量(singular vectors)，所以对我们将保留并实际执行变换的奇异向量进行近似估计的有限的计算更有效。</p>
<p>例如：以下显示了来自 Olivetti 数据集的 16 个样本肖像（以 0.0 为中心）。 右侧是把前16个奇异向量重新改变shape使其成为肖像。
因为我们只需要使用大小为 <span class="math notranslate nohighlight">\(n_{samples} = 400\)</span> 和 <span class="math notranslate nohighlight">\(n_{features} = 64 \times 64 = 4096\)</span> 的数据集的前 16 个奇异向量,
使得计算时间小于 1 秒。</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="orig_img" src="../images/sphx_glr_plot_faces_decomposition_0011.png" style="width: 360.0px; height: 270.59999999999997px;" /></a> <a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="pca_img" src="../images/sphx_glr_plot_faces_decomposition_0021.png" style="width: 360.0px; height: 270.59999999999997px;" /></a></strong></p><p>注意：使用可选参数 <code class="docutils literal notranslate"><span class="pre">svd_solver='randomized'</span></code> ，我们还需要在 <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 中给出低维空间的大小 <code class="docutils literal notranslate"><span class="pre">n_components</span></code> 作为强制输入参数。</p>
<p>如果我们记： <span class="math notranslate nohighlight">\(n_{\max} = \max(n_{\mathrm{samples}}, n_{\mathrm{features}})\)</span>
和 <span class="math notranslate nohighlight">\(n_{\min} = \min(n_{\mathrm{samples}}, n_{\mathrm{features}})\)</span> , 那么 randomized <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 的时间复杂度为
<span class="math notranslate nohighlight">\(O(n_{\max}^2 \cdot n_{\mathrm{components}})\)</span> ,而不是在 <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 类中实现的精确方法的时间复杂度: <span class="math notranslate nohighlight">\(O(n_{\max}^2 \cdot n_{\min})\)</span>。</p>
<p>randomized <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 的内存足迹也是与 <span class="math notranslate nohighlight">\(2 \cdot n_{\max} \cdot n_{\mathrm{components}}\)</span> 成比例的，
而不是 精确实现中的 <span class="math notranslate nohighlight">\(n_{\max}\cdot n_{\min}\)</span> 。</p>
<p>注意: 参数为 <code class="docutils literal notranslate"><span class="pre">svd_solver='randomized'</span></code> 的 <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> ，其 <code class="docutils literal notranslate"><span class="pre">inverse_transform</span></code> 方法的实现
并不是 <code class="docutils literal notranslate"><span class="pre">transform</span></code> 的确切的逆变换，即使当 <code class="docutils literal notranslate"><span class="pre">whiten=False</span></code> (default)。</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py"><span class="std std-ref">Faces recognition example using eigenfaces and SVMs</span></a></li>
<li><a class="reference internal" href="../auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py"><span class="std std-ref">Faces dataset decompositions</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="https://arxiv.org/abs/0909.4061">“Finding structure with randomness: Stochastic algorithms for
constructing approximate matrix decompositions”</a>
Halko, et al., 2009</li>
</ul>
</div>
</div>
<div class="section" id="kernel-pca">
<span id="id5"></span><h3>2.5.1.4. 核 PCA<a class="headerlink" href="#kernel-pca" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA" title="sklearn.decomposition.KernelPCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">KernelPCA</span></code></a> 类是PCA的扩展，通过使用 kernel (请看 <a class="reference internal" href="metrics.html#metrics"><span class="std std-ref">成对测度, 相似性 和 核</span></a>) 来达到非线性降维 的目的。
它有很多应用，包括 去噪声(denoising),压缩和结构化预测(kernel dependency estimation)。
<a class="reference internal" href="generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA" title="sklearn.decomposition.KernelPCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">KernelPCA</span></code></a> 支持 <code class="docutils literal notranslate"><span class="pre">transform</span></code> 和 <code class="docutils literal notranslate"><span class="pre">inverse_transform</span></code>。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_kernel_pca.html"><img alt="../images/sphx_glr_plot_kernel_pca_0011.png" src="../images/sphx_glr_plot_kernel_pca_0011.png" style="width: 480.0px; height: 360.0px;" /></a>
</div>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/decomposition/plot_kernel_pca.html#sphx-glr-auto-examples-decomposition-plot-kernel-pca-py"><span class="std std-ref">Kernel PCA</span></a></li>
</ul>
</div>
</div>
<div class="section" id="sparsepca-minibatchsparsepca">
<span id="sparsepca"></span><h3>2.5.1.5. 稀疏主成分分析 (SparsePCA 和 MiniBatchSparsePCA)<a class="headerlink" href="#sparsepca-minibatchsparsepca" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.decomposition.SparsePCA.html#sklearn.decomposition.SparsePCA" title="sklearn.decomposition.SparsePCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparsePCA</span></code></a> 是PCA的一个变体, 目的是抽取一组能够最好的重构数据的稀疏分量。</p>
<p>小批量稀疏PCA (<a class="reference internal" href="generated/sklearn.decomposition.MiniBatchSparsePCA.html#sklearn.decomposition.MiniBatchSparsePCA" title="sklearn.decomposition.MiniBatchSparsePCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">MiniBatchSparsePCA</span></code></a>) 是 <a class="reference internal" href="generated/sklearn.decomposition.SparsePCA.html#sklearn.decomposition.SparsePCA" title="sklearn.decomposition.SparsePCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparsePCA</span></code></a> 的一个更快速但准确率稍差的变体。
计算速度的提升主要是通过在 特征集的一小部分 上不断迭代直到达到给定的迭代次数。</p>
<p>主成分分析(Principal component analysis,PCA)的缺点在于：通过该方法提取的成分具有唯一的稠密表达式，
即当表示为原始变量的线性组合时，它们具有非零系数，这使得对结果的解释性变的困难。
在许多情况下，真正的底层分量可以更自然地想象为稀疏向量; 例如在面部识别中，每个分量可能自然地映射到面部的某个部分。</p>
<p>稀疏的主成分产生更简洁、可解释的表达，明确强调了样本之间的差异性来自哪些原始特征。</p>
<p>以下示例展示了使用 sparse PCA 提取 Olivetti 人脸数据集中的 16 个components。可以看出正则化项是如何产生许多零的。
此外，数据的自然结构导致了非零系数的垂直相邻(vertically adjacent)。
该模型并没有在数学上强制执行这一点: 每个分量(component)都是一个向量 <span class="math notranslate nohighlight">\(h \in \mathbf{R}^{4096}\)</span>, 没有垂直相邻性(vertical adjacency)的概念
除非人性化地的可视化为 64x64 像素的图像。
下面显示的components看起来局部化(appear local)是数据的内在结构的影响，这种局部模式使重建误差最小化。
存在几种将邻接性和不同结构类型都考虑进去的稀疏诱导范数(sparsity-inducing norms),参见 <a class="reference internal" href="#jen09" id="id6">[Jen09]</a> 对这种方法进行了解。
有关如何使用 Sparse PCA 的更多详细信息，请参阅下面的示例部分。</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="pca_img" src="../images/sphx_glr_plot_faces_decomposition_0021.png" style="width: 360.0px; height: 270.59999999999997px;" /></a> <a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="spca_img" src="../images/sphx_glr_plot_faces_decomposition_0051.png" style="width: 360.0px; height: 270.59999999999997px;" /></a></strong></p><p>注意到 Sparse PCA 问题有很多种不同的表述形式。 这里的实现基于 <a class="reference internal" href="#mrl09" id="id7">[Mrl09]</a>  。 待求解的优化问题是一个
带有对components的 <span class="math notranslate nohighlight">\(\ell_1\)</span> 惩罚项的PCA问题(字典学习,dictionary learning):</p>
<div class="math notranslate nohighlight">
\[\begin{split}(U^*, V^*) = \underset{U, V}{\operatorname{arg\,min\,}} &amp; \frac{1}{2}
             ||X-UV||_2^2+\alpha||V||_1 \\
             \text{subject to } &amp; ||U_k||_2 = 1 \text{ for all }
             0 \leq k &lt; n_{components}\end{split}\]</div>
<p>当训练样本比较少的时候，稀疏诱导的 <span class="math notranslate nohighlight">\(\ell_1\)</span> 范数还可以防止从噪声中学习components。
惩罚的程度(以及由此导致的稀疏性)可以通过参数 <code class="docutils literal notranslate"><span class="pre">alpha</span></code> 进行调节。
较小的值会导致轻微的正则化因子分解，而较大的值会将多个系数收缩为零。</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">根据在线算法的精神，<a class="reference internal" href="generated/sklearn.decomposition.MiniBatchSparsePCA.html#sklearn.decomposition.MiniBatchSparsePCA" title="sklearn.decomposition.MiniBatchSparsePCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">MiniBatchSparsePCA</span></code></a> 类不实现 <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code> ，因为该算法是沿着特征方向在线，而不是样本方向。</p>
</div>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py"><span class="std std-ref">Faces dataset decompositions</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<table class="docutils citation" frame="void" id="mrl09" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id7">[Mrl09]</a></td><td><a class="reference external" href="http://www.di.ens.fr/sierra/pdfs/icml09.pdf">“Online Dictionary Learning for Sparse Coding”</a>
J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="jen09" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id6">[Jen09]</a></td><td><a class="reference external" href="www.di.ens.fr/~fbach/sspca_AISTATS2010.pdf">“Structured Sparse Principal Component Analysis”</a>
R. Jenatton, G. Obozinski, F. Bach, 2009</td></tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="section" id="lsa">
<span id="id8"></span><h2>2.5.2. 截断奇异值分解和隐语义分析<a class="headerlink" href="#lsa" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code class="xref py py-class docutils literal notranslate"><span class="pre">TruncatedSVD</span></code></a> 实现了一个奇异值分解（singular value decomposition,SVD）的变体，
它只计算 <span class="math notranslate nohighlight">\(k\)</span> 个最大的奇异值，其中 <span class="math notranslate nohighlight">\(k\)</span> 是用户指定的参数。</p>
<p>当截断SVD被应用于term-document矩阵（由 <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code> 或 <code class="docutils literal notranslate"><span class="pre">TfidfVectorizer</span></code> 返回）时，
这种变换被称为隐语义分析(<a class="reference external" href="http://nlp.stanford.edu/IR-book/pdf/18lsi.pdf">latent semantic analysis</a> , LSA),
因为它将term-document矩阵变换为低纬度的 “语义空间(semantic space)” 。
特别地是 LSA 能够抵抗同义词(synonymy)和多义词(polysemy)的影响（两者大致意味着每个单词有多重含义），
这导致term-document矩阵过度稀疏，并且在诸如余弦相似度的度量下表现出较差的相似性。</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">LSA 也被称为隐语义索引(latent semantic indexing,LSI)，尽管严格地说它是指在持久索引(persistent indexes)中
用于信息检索(information retrieval)的目的。</p>
</div>
<p>从数学上讲, 应用在训练集 <span class="math notranslate nohighlight">\(X\)</span> 上的 truncated SVD 产生一个低秩近似(low-rank approximation) <span class="math notranslate nohighlight">\(X\)</span>:</p>
<div class="math notranslate nohighlight">
\[X \approx X_k = U_k \Sigma_k V_k^\top\]</div>
<p>经过这个操作以后, <span class="math notranslate nohighlight">\(U_k \Sigma_k^\top\)</span> 是具有 <span class="math notranslate nohighlight">\(k\)</span> (在API中称之为 <code class="docutils literal notranslate"><span class="pre">n_components</span></code>)个特征的变换训练集。.</p>
<p>为了再对测试集 <span class="math notranslate nohighlight">\(X\)</span> 也进行变换, 我们用 <span class="math notranslate nohighlight">\(V_k\)</span> 乘以它:</p>
<div class="math notranslate nohighlight">
\[X' = X V_k\]</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">自然语言处理(NLP)和信息检索(IR)文献中的 LSA 的大多数处理方式是交换 矩阵 <span class="math notranslate nohighlight">\(X\)</span> 的坐标轴,
使其具有 <code class="docutils literal notranslate"><span class="pre">n_features</span></code> × <code class="docutils literal notranslate"><span class="pre">n_samples</span></code> 的shape。 我们以不同方式呈现LSA,使其与scikit-learn API
相匹配，但是找到的奇异值(singular values)是相同的。</p>
</div>
<p><a class="reference internal" href="generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code class="xref py py-class docutils literal notranslate"><span class="pre">TruncatedSVD</span></code></a> 非常类似于 <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a>, 但不同之处在于它工作在样本矩阵 <span class="math notranslate nohighlight">\(X\)</span> 上而不是它们的协方差矩阵。
当从特征数值中减去 <span class="math notranslate nohighlight">\(X\)</span> 的每列（每一列对应一个特征）的均值时，在得到的矩阵上应用 truncated SVD 等价于 PCA 。
实际上，这意味着 <a class="reference internal" href="generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code class="xref py py-class docutils literal notranslate"><span class="pre">TruncatedSVD</span></code></a> 变换器（transformer）接受 <code class="docutils literal notranslate"><span class="pre">scipy.sparse</span></code> 矩阵，而不需要把它们变稠密(density)，
因为即使密集化(densifying)中型大小文档的集合也可能填满内存。</p>
<p>虽然 <a class="reference internal" href="generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code class="xref py py-class docutils literal notranslate"><span class="pre">TruncatedSVD</span></code></a> 变换器可以在任何(稀疏的)特征矩阵上工作，
但在LSA/document 处理的原始频率计数任务中 还是建议把它用在tf–idf矩阵上。
特别地，应该打开 子线性缩放(sublinear scaling)和逆文档频率(inverse document frequency),即 (<code class="docutils literal notranslate"><span class="pre">sublinear_tf=True,</span> <span class="pre">use_idf=True</span></code>)
以使特征数值更接近于高斯分布，补偿 LSA 对文本数据的错误假设。</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py"><span class="std std-ref">Clustering text documents using k-means</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li>Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze (2008),
<em>Introduction to Information Retrieval</em>, Cambridge University Press,
chapter 18: <a class="reference external" href="http://nlp.stanford.edu/IR-book/pdf/18lsi.pdf">Matrix decompositions &amp; latent semantic indexing</a></li>
</ul>
</div>
</div>
<div class="section" id="dictionary-learning">
<span id="dictionarylearning"></span><h2>2.5.3. 词典学习(Dictionary Learning)<a class="headerlink" href="#dictionary-learning" title="Permalink to this headline">¶</a></h2>
<div class="section" id="sparsecoder">
<span id="id9"></span><h3>2.5.3.1. 用预先计算好的字典进行稀疏编码<a class="headerlink" href="#sparsecoder" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.decomposition.SparseCoder.html#sklearn.decomposition.SparseCoder" title="sklearn.decomposition.SparseCoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparseCoder</span></code></a> 对象是一个估计器(estimator)，可以用来将信号变换成很多原子(atoms)的稀疏线性组合，
如离散小波基。而这些原子(atoms)来自于一个固定的预先计算好的字典(dictionary)中。
因此，该对象不实现 <code class="docutils literal notranslate"><span class="pre">fit</span></code> 方法。该变换相当于一个稀疏编码问题: 将数据表示为尽可能少的词典原子(dictionary atoms)的线性组合。
词典学习的所有变体实现以下变换方法，可以通过 <code class="docutils literal notranslate"><span class="pre">transform_method</span></code> 初始化参数进行控制:</p>
<ul class="simple">
<li>Orthogonal matching pursuit (<a class="reference internal" href="linear_model.html#omp"><span class="std std-ref">正交匹配追踪法 (OMP)</span></a>)</li>
<li>Least-angle regression (<a class="reference internal" href="linear_model.html#least-angle-regression"><span class="std std-ref">最小角回归</span></a>)</li>
<li>Lasso computed by least-angle regression</li>
<li>Lasso using coordinate descent (<a class="reference internal" href="linear_model.html#lasso"><span class="std std-ref">Lasso</span></a>)</li>
<li>Thresholding</li>
</ul>
<p>阈值方法速度非常快，但是不能产生精确的重建。 它们在分类任务的文献中已被证明是有用的。
对于图像重建任务，正交匹配追踪(orthogonal matching pursuit)可以产生最精确、无偏的重建。</p>
<p>词典学习对象通过 <code class="docutils literal notranslate"><span class="pre">split_code</span></code> 参数提供将稀疏编码结果中的正值和负值分离的可能性。
当使用词典学习来提取用于监督学习的特征时，这是有用的，
因为它允许学习算法将不同的权重从正加载(positive loading)分配给相应的负加载(negative loadings)的特定原子。</p>
<p>单个样本的分割编码具有长度 <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">*</span> <span class="pre">n_components</span></code> ，并使用以下规则构造: 首先，计算长度为 <code class="docutils literal notranslate"><span class="pre">n_components</span></code> 的常规编码。
然后，<code class="docutils literal notranslate"><span class="pre">split_code</span></code> 的第一个``n_components``条目将用正常编码向量的正部分填充。
分割编码的第二部分用编码向量的负部分填充，只有一个正号。
因此， split_code 是非负的。</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/decomposition/plot_sparse_coding.html#sphx-glr-auto-examples-decomposition-plot-sparse-coding-py"><span class="std std-ref">Sparse coding with a precomputed dictionary</span></a></li>
</ul>
</div>
</div>
<div class="section" id="id10">
<h3>2.5.3.2. 通用词典学习<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>词典学习(<a class="reference internal" href="generated/sklearn.decomposition.DictionaryLearning.html#sklearn.decomposition.DictionaryLearning" title="sklearn.decomposition.DictionaryLearning"><code class="xref py py-class docutils literal notranslate"><span class="pre">DictionaryLearning</span></code></a>)是一个矩阵因子分解(matrix factorization)问题，
相当于找到一个在拟合过的数据的稀疏编码中表现良好的（通常是过完备的）词典。</p>
<p>将数据表示为 来自过完备词典的原子的稀疏组合 被认为是哺乳动物初级视觉皮层的工作方式。
因此，应用于图像块的词典学习已被证明在诸如图像完成、修复和去噪，以及有监督识别图像处理任务中表现出狠好的结果。</p>
<p>词典学习是一个优化问题，通过交替更新稀疏编码来求解。作为解决多个 Lasso 问题的一个解决方案，
考虑到字典的固定性，然后更新字典以最好地适合于稀疏编码。</p>
<div class="math notranslate nohighlight">
\[\begin{split}(U^*, V^*) = \underset{U, V}{\operatorname{arg\,min\,}} &amp; \frac{1}{2}
             ||X-UV||_2^2+\alpha||U||_1 \\
             \text{subject to } &amp; ||V_k||_2 = 1 \text{ for all }
             0 \leq k &lt; n_{\mathrm{atoms}}\end{split}\]</div>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="pca_img2" src="../images/sphx_glr_plot_faces_decomposition_0021.png" style="width: 360.0px; height: 270.59999999999997px;" /></a> <a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="dict_img2" src="../images/sphx_glr_plot_faces_decomposition_0061.png" style="width: 360.0px; height: 270.59999999999997px;" /></a></strong></p><p>在使用这样一个过程来拟合词典之后，变换只是一个稀疏编码的步骤，
与所有的词典学习对象共享相同的实现。(参见 <a class="reference internal" href="#sparsecoder"><span class="std std-ref">用预先计算好的字典进行稀疏编码</span></a>)。</p>
<p>还可以将 字典 和/或 编码 约束为正，以匹配数据中可能存在的约束。
下面是应用不同正性约束(positivity constraints)的人脸。红色表示负值，蓝色表示正值，白色表示零。</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/decomposition/plot_image_denoising.html"><img alt="dict_img_pos1" src="../images/sphx_glr_plot_faces_decomposition_0111.png" style="width: 360.0px; height: 270.59999999999997px;" /></a> <a class="reference external" href="../auto_examples/decomposition/plot_image_denoising.html"><img alt="dict_img_pos2" src="../images/sphx_glr_plot_faces_decomposition_0121.png" style="width: 360.0px; height: 270.59999999999997px;" /></a></strong></p><p class="centered">
<strong><a class="reference external" href="../auto_examples/decomposition/plot_image_denoising.html"><img alt="dict_img_pos3" src="../images/sphx_glr_plot_faces_decomposition_0131.png" style="width: 360.0px; height: 270.59999999999997px;" /></a> <a class="reference external" href="../auto_examples/decomposition/plot_image_denoising.html"><img alt="dict_img_pos4" src="../images/sphx_glr_plot_faces_decomposition_0141.png" style="width: 360.0px; height: 270.59999999999997px;" /></a></strong></p><p>以下图像显示了 从浣熊脸部的部分图像中提取的4x4像素图像块中进行字典学习得到的字典看起来长啥样儿。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_image_denoising.html"><img alt="../images/sphx_glr_plot_image_denoising_0011.png" src="../images/sphx_glr_plot_image_denoising_0011.png" style="width: 210.0px; height: 200.0px;" /></a>
</div>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/decomposition/plot_image_denoising.html#sphx-glr-auto-examples-decomposition-plot-image-denoising-py"><span class="std std-ref">Image denoising using dictionary learning</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.di.ens.fr/sierra/pdfs/icml09.pdf">“Online dictionary learning for sparse coding”</a>
J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009</li>
</ul>
</div>
</div>
<div class="section" id="minibatchdictionarylearning">
<span id="id12"></span><h3>2.5.3.3. 小批量词典学习<a class="headerlink" href="#minibatchdictionarylearning" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.decomposition.MiniBatchDictionaryLearning.html#sklearn.decomposition.MiniBatchDictionaryLearning" title="sklearn.decomposition.MiniBatchDictionaryLearning"><code class="xref py py-class docutils literal notranslate"><span class="pre">MiniBatchDictionaryLearning</span></code></a> 类实现了一个更快、但不太准确的字典学习算法版本，该算法更适合于大型数据集。</p>
<p>默认情况下，<a class="reference internal" href="generated/sklearn.decomposition.MiniBatchDictionaryLearning.html#sklearn.decomposition.MiniBatchDictionaryLearning" title="sklearn.decomposition.MiniBatchDictionaryLearning"><code class="xref py py-class docutils literal notranslate"><span class="pre">MiniBatchDictionaryLearning</span></code></a> 将数据划分为小批量(mini-batches)，并以在线方式通过指定次数的迭代循环对数据进行优化。
但是，目前该类还没有实现停止条件。</p>
<p>这个估计器也实现了 <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code>, 只通过在一个mini-batch上迭代一次来更新字典。
当数据从一开始就不容易获得时，或者当数据不适合全部放在内存时，这可以用于在线学习。</p>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_dict_face_patches.html"><img alt="../images/sphx_glr_plot_dict_face_patches_0011.png" class="align-right" src="../images/sphx_glr_plot_dict_face_patches_0011.png" style="width: 210.0px; height: 200.0px;" /></a>
<div class="topic">
<p class="topic-title first"><strong>用于字典学习的聚类</strong></p>
<p>注意，当使用字典学习来提取表示（例如，用于稀疏编码）时，聚类可以是学习字典的良好中间方法。
例如，<a class="reference internal" href="generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans" title="sklearn.cluster.MiniBatchKMeans"><code class="xref py py-class docutils literal notranslate"><span class="pre">MiniBatchKMeans</span></code></a> 估计器能高效计算并使用 <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code> 方法实现在线学习。</p>
<blockquote>
<div>案例: <a class="reference internal" href="../auto_examples/cluster/plot_dict_face_patches.html#sphx-glr-auto-examples-cluster-plot-dict-face-patches-py"><span class="std std-ref">Online learning of a dictionary of parts of faces</span></a></div></blockquote>
</div>
</div>
</div>
<div class="section" id="factor-analysis">
<span id="fa"></span><h2>2.5.4. 因子分析(Factor Analysis)<a class="headerlink" href="#factor-analysis" title="Permalink to this headline">¶</a></h2>
<p>在非监督学习中，我们只需要一个数据集 <span class="math notranslate nohighlight">\(X = \{x_1, x_2, \dots, x_n \}\)</span>。
这个数据集如何用数学的方式来描述呢？ 一个用于描述 <span class="math notranslate nohighlight">\(X\)</span> 的非常简单的 <cite>连续隐变量(continuous latent variable)</cite> 模型 如下所示：</p>
<div class="math notranslate nohighlight">
\[x_i = W h_i + \mu + \epsilon\]</div>
<p>向量 <span class="math notranslate nohighlight">\(h_i\)</span> 被称为 “隐变量(latent variable)” 是因为它无法被观测。<span class="math notranslate nohighlight">\(\epsilon\)</span> 被看做是一个服从 0 均值协方差为 <span class="math notranslate nohighlight">\(\Psi\)</span> 的高斯分布的噪声项
，即 <span class="math notranslate nohighlight">\(\epsilon \sim \mathcal{N}(0, \Psi)\)</span> 。 <span class="math notranslate nohighlight">\(\mu\)</span> 是一个任意的实数偏移向量。
这样的模型被称之为 “生成式(generative)” 模型, 因为它描述了 <span class="math notranslate nohighlight">\(x_i\)</span> 是如何从 <span class="math notranslate nohighlight">\(h_i\)</span> 产生的。
如果我们使用所有的 <span class="math notranslate nohighlight">\(x_i\)</span> 作为列来形成一个矩阵  <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 并且 所有的 <span class="math notranslate nohighlight">\(h_i\)</span> 作为列来形成一个矩阵 <span class="math notranslate nohighlight">\(\mathbf{H}\)</span> ，
那么我们可以得到下面的式子(with suitably defined <span class="math notranslate nohighlight">\(\mathbf{M}\)</span> 和 <span class="math notranslate nohighlight">\(\mathbf{E}\)</span>):</p>
<div class="math notranslate nohighlight">
\[\mathbf{X} = W \mathbf{H} + \mathbf{M} + \mathbf{E}\]</div>
<p>换而言之, 我们分解(<em>decomposed</em>)了矩阵 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 。</p>
<p>如果给定了 <span class="math notranslate nohighlight">\(h_i\)</span>, 则上面的等式自动的隐含了下面的概率性解释(probabilistic interpretation):</p>
<div class="math notranslate nohighlight">
\[p(x_i|h_i) = \mathcal{N}(Wh_i + \mu, \Psi)\]</div>
<p>对于一个完备的概率模型，我们还需要一个关于隐变量 <span class="math notranslate nohighlight">\(h\)</span> 的先验分布，最简单直接的假设(基于高斯分布的优良性质)是 <span class="math notranslate nohighlight">\(h \sim \mathcal{N}(0,\mathbf{I})\)</span>。
这就产生了一个高斯分布，将其作为 <span class="math notranslate nohighlight">\(x\)</span> 的边缘分布 :</p>
<div class="math notranslate nohighlight">
\[p(x) = \mathcal{N}(\mu, WW^T + \Psi)\]</div>
<p>现在，没有任何进一步的假设，用一个隐变量 <span class="math notranslate nohighlight">\(h\)</span> 的想法是多余的— <span class="math notranslate nohighlight">\(x\)</span> 完全可以用均值和协方差来建模。
我们需要强加一些更特殊的结构(more specific structure)在这两个参数中的其中一个上。
一个关于 误差协方差 <span class="math notranslate nohighlight">\(\Psi\)</span> 的结构的简单附加假设 如下:</p>
<ul class="simple">
<li><span class="math notranslate nohighlight">\(\Psi = \sigma^2 \mathbf{I}\)</span>: 这个假设导致了一个 <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 的概率性模型(probabilistic model)。</li>
<li><span class="math notranslate nohighlight">\(\Psi = \mathrm{diag}(\psi_1, \psi_2, \dots, \psi_n)\)</span>: 此模型被称为 <a class="reference internal" href="generated/sklearn.decomposition.FactorAnalysis.html#sklearn.decomposition.FactorAnalysis" title="sklearn.decomposition.FactorAnalysis"><code class="xref py py-class docutils literal notranslate"><span class="pre">FactorAnalysis</span></code></a>, 一个经典的统计模型。
矩阵 W 有时被称为 “factor loading matrix”。</li>
</ul>
<p>这两种模型本质上都是用低秩协方差矩阵估计一个高斯分布。因为这两个模型都是概率性的，所以它们可以集成到更复杂的模型中，
e.g. 因子分析的混合模型(Mixture of Factor Analysers)。 如果在隐变量上做了非高斯先验的假定，你会得到非常不一样的模型(e.g. <a class="reference internal" href="generated/sklearn.decomposition.FastICA.html#sklearn.decomposition.FastICA" title="sklearn.decomposition.FastICA"><code class="xref py py-class docutils literal notranslate"><span class="pre">FastICA</span></code></a>)。</p>
<p>因子分析(Factor analysis)可以产生与 <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 类似的components(the columns of its loading
matrix)。然而，你不能做出任何关于这些components的general statements(e.g. 它们是否正交):</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="pca_img3" src="../images/sphx_glr_plot_faces_decomposition_0021.png" style="width: 360.0px; height: 270.59999999999997px;" /></a> <a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="fa_img3" src="../images/sphx_glr_plot_faces_decomposition_0091.png" style="width: 360.0px; height: 270.59999999999997px;" /></a></strong></p><p>因子分析相对于 <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 的主要优势是 它可以在输入空间的每一个方向上独立的对方差进行建模
(heteroscedastic noise):</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="../images/sphx_glr_plot_faces_decomposition_0081.png" src="../images/sphx_glr_plot_faces_decomposition_0081.png" style="width: 150.0px; height: 168.75px;" /></a>
</div>
<p>在有heteroscedastic noise出现的情况下，它比 probabilistic PCA 允许有更好的模型选择 :</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_pca_vs_fa_model_selection.html"><img alt="../images/sphx_glr_plot_pca_vs_fa_model_selection_0021.png" src="../images/sphx_glr_plot_pca_vs_fa_model_selection_0021.png" style="width: 480.0px; height: 360.0px;" /></a>
</div>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/decomposition/plot_pca_vs_fa_model_selection.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-fa-model-selection-py"><span class="std std-ref">Model selection with Probabilistic PCA and Factor Analysis (FA)</span></a></li>
</ul>
</div>
</div>
<div class="section" id="ica">
<span id="id13"></span><h2>2.5.5. 独立分量分析(ICA)<a class="headerlink" href="#ica" title="Permalink to this headline">¶</a></h2>
<p>独立分量分析(Independent component analysis)将多变量信号分解为独立性最强的加性子分量。
在 scikit-learn 中实现了ICA，使用 <a class="reference internal" href="generated/sklearn.decomposition.FastICA.html#sklearn.decomposition.FastICA" title="sklearn.decomposition.FastICA"><code class="xref py py-class docutils literal notranslate"><span class="pre">Fast</span> <span class="pre">ICA</span></code></a> 算法。
ICA 通常不用于降低维度，而是用于分离叠加信号(superimposed signals)。
由于 ICA 模型不包括噪声项，因此要使模型正确，必须使用白化(whitening)。
这可以在内部调节白化参数或手动使用 PCA 的一种变体。</p>
<p>它通常用于分离混合信号(一个称为盲源分离(<em>blind source separation</em>)的问题)，如下例所示：</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_ica_blind_source_separation.html"><img alt="../images/sphx_glr_plot_ica_blind_source_separation_0011.png" src="../images/sphx_glr_plot_ica_blind_source_separation_0011.png" style="width: 384.0px; height: 288.0px;" /></a>
</div>
<p>ICA 还可以用于另一个非线性分解：寻找稀疏分量 :</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="pca_img4" src="../images/sphx_glr_plot_faces_decomposition_0021.png" style="width: 360.0px; height: 270.59999999999997px;" /></a> <a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="ica_img4" src="../images/sphx_glr_plot_faces_decomposition_0041.png" style="width: 360.0px; height: 270.59999999999997px;" /></a></strong></p><div class="topic">
<p class="topic-title first">案列:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/decomposition/plot_ica_blind_source_separation.html#sphx-glr-auto-examples-decomposition-plot-ica-blind-source-separation-py"><span class="std std-ref">Blind source separation using FastICA</span></a></li>
<li><a class="reference internal" href="../auto_examples/decomposition/plot_ica_vs_pca.html#sphx-glr-auto-examples-decomposition-plot-ica-vs-pca-py"><span class="std std-ref">FastICA on 2D point clouds</span></a></li>
<li><a class="reference internal" href="../auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py"><span class="std std-ref">Faces dataset decompositions</span></a></li>
</ul>
</div>
</div>
<div class="section" id="nmf-or-nnmf">
<span id="nmf"></span><h2>2.5.6. 非负矩阵分解(NMF or NNMF)<a class="headerlink" href="#nmf-or-nnmf" title="Permalink to this headline">¶</a></h2>
<div class="section" id="frobeniusnmf">
<h3>2.5.6.1. 带有Frobenius范数的NMF<a class="headerlink" href="#frobeniusnmf" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> <a class="footnote-reference" href="#id20" id="id14">[1]</a> 是另一种分解(decomposition)方法,它要求数据和分量都是非负的(non-negative)。
在数据矩阵不包含负值的情况下， <a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> 可以直接插入对 <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 或其变体进行替换。
通过优化 <span class="math notranslate nohighlight">\(X\)</span> 与矩阵乘积 <span class="math notranslate nohighlight">\(WH\)</span> 之间的距离 <span class="math notranslate nohighlight">\(d\)</span> ，
可以将样本 <span class="math notranslate nohighlight">\(X\)</span> 分解为两个非负矩阵 <span class="math notranslate nohighlight">\(W\)</span> 和 <span class="math notranslate nohighlight">\(H\)</span> 。
最广泛使用的距离函数是 squared Frobenius norm，它是欧几里德范数到矩阵的推广:</p>
<div class="math notranslate nohighlight">
\[d_{\mathrm{Fro}}(X, Y) = \frac{1}{2} ||X - Y||_{\mathrm{Fro}}^2 = \frac{1}{2} \sum_{i,j} (X_{ij} - {Y}_{ij})^2\]</div>
<p>与 <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> 不一样, 向量的表示(representation of a vector)是以加性方式(an additive fashion)获得的, 通过叠加分量，不做减法。
这样的加性模型(additive models) 用于表达图像和文本是很高效的。</p>
<p>在 [Hoyer, 2004] <a class="footnote-reference" href="#id21" id="id15">[2]</a> 中已经观察到， 当被小心约束时，<a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> 可以产生数据集的基于部分的表式(parts-based representation of the dataset),
能够产生可解释的模型(interpretable models)。
下面的例子展示了 <a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> 从Olivetti面部数据集中找到的16个稀疏分量，拿它与PCA eigenfaces对比一下下。</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="pca_img5" src="../images/sphx_glr_plot_faces_decomposition_0021.png" style="width: 360.0px; height: 270.59999999999997px;" /></a> <a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="nmf_img5" src="../images/sphx_glr_plot_faces_decomposition_0031.png" style="width: 360.0px; height: 270.59999999999997px;" /></a></strong></p><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">init</span></code> 属性确定了应用的初始化方法，这对方法的性能有很大的影响。 <a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> 实现了非负双奇异值分解方法(Nonnegative Double Singular Value Decomposition)。
NNDSVD <a class="footnote-reference" href="#id22" id="id16">[4]</a> 基于两个 SVD 过程，一个逼近数据矩阵， 另一个逼近 由此产生的部分SVD因子的正部分。而部分SVD因子(partial SVD factors)的正部分是使用单位秩矩阵的代数性质得到的。
基本的 NNDSVD 算法更适合稀疏因子分解。其变体 NNDSVDa（全部零值替换为所有元素的平均值）和 NNDSVDar（零值替换为比数据平均值除以100小的随机扰动）
在稠密情况时推荐使用。</p>
<p>请注意，乘法更新 (‘mu’) 求解器无法更新在初始化中出现的零，因此当与引入大量零的基本 NNDSVD 算法联合使用时， 会导致较差的结果;
在这种情况下，应优先使用 NNDSVDa 或 NNDSVDar。</p>
<p>也可以通过设置属性 <code class="xref py py-attr docutils literal notranslate"><span class="pre">init=&quot;random&quot;</span></code> ，使用正确缩放的随机非负矩阵初始化 <a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> 。
整数种子或 <code class="docutils literal notranslate"><span class="pre">RandomState</span></code> 也可以传递给属性 <code class="xref py py-attr docutils literal notranslate"><span class="pre">random_state</span></code>  以控制可重现性(reproducibility)。</p>
<p>在 <a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> 中，L1 和 L2 先验可以被添加到损失函数中以正规化模型。 L2先验使用 Frobenius 范数，而L1先验使用按元素的(elementwise) L1范数。
与 <code class="xref py py-class docutils literal notranslate"><span class="pre">ElasticNet</span></code> 一样，我们通过 <code class="xref py py-attr docutils literal notranslate"><span class="pre">l1_ratio</span></code> (<span class="math notranslate nohighlight">\(\rho\)</span>) 参数和正则化强度参数 <code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code> (<span class="math notranslate nohighlight">\(\alpha\)</span>) 来控制L1和L2的组合。
那么先验项(priors terms)是:</p>
<div class="math notranslate nohighlight">
\[\alpha \rho ||W||_1 + \alpha \rho ||H||_1
+ \frac{\alpha(1-\rho)}{2} ||W||_{\mathrm{Fro}} ^ 2
+ \frac{\alpha(1-\rho)}{2} ||H||_{\mathrm{Fro}} ^ 2\]</div>
<p>被正则化的目标函数是:</p>
<div class="math notranslate nohighlight">
\[d_{\mathrm{Fro}}(X, WH)
+ \alpha \rho ||W||_1 + \alpha \rho ||H||_1
+ \frac{\alpha(1-\rho)}{2} ||W||_{\mathrm{Fro}} ^ 2
+ \frac{\alpha(1-\rho)}{2} ||H||_{\mathrm{Fro}} ^ 2\]</div>
<p><a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> 类对 W 和 H 都进行正则化。 该类的公开函数(public function)
<code class="xref py py-func docutils literal notranslate"><span class="pre">non_negative_factorization</span></code> 允许通过属性 <code class="xref py py-attr docutils literal notranslate"><span class="pre">regularization</span></code> 进行更精细的控制，可以选择只对W,或只对H,或W,H一起 进行正则化。</p>
</div>
<div class="section" id="beta-divergencenmf">
<h3>2.5.6.2. 带有beta-divergence的NMF<a class="headerlink" href="#beta-divergencenmf" title="Permalink to this headline">¶</a></h3>
<p>如前所述，最广泛使用的距离函数是 squared Frobenius 范数，这是欧几里得范数到矩阵的推广:</p>
<div class="math notranslate nohighlight">
\[d_{\mathrm{Fro}}(X, Y) = \frac{1}{2} ||X - Y||_{Fro}^2 = \frac{1}{2} \sum_{i,j} (X_{ij} - {Y}_{ij})^2\]</div>
<p>其他距离函数可用于 NMF，例如（广义） Kullback-Leibler(KL)散度，也称为 I-divergence:</p>
<div class="math notranslate nohighlight">
\[d_{KL}(X, Y) = \sum_{i,j} (X_{ij} \log(\frac{X_{ij}}{Y_{ij}}) - X_{ij} + Y_{ij})\]</div>
<p>或者, the Itakura-Saito (IS) divergence:</p>
<div class="math notranslate nohighlight">
\[d_{IS}(X, Y) = \sum_{i,j} (\frac{X_{ij}}{Y_{ij}} - \log(\frac{X_{ij}}{Y_{ij}}) - 1)\]</div>
<p>这三个距离函数是 beta-divergence 函数族的特殊情况，其参数分别为 <span class="math notranslate nohighlight">\(\beta = 2, 1, 0\)</span> <a class="footnote-reference" href="#id24" id="id17">[6]</a>。 The beta-divergence 定义如下:</p>
<div class="math notranslate nohighlight">
\[d_{\beta}(X, Y) = \sum_{i,j} \frac{1}{\beta(\beta - 1)}(X_{ij}^\beta + (\beta-1)Y_{ij}^\beta - \beta X_{ij} Y_{ij}^{\beta - 1})\]</div>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_beta_divergence.html"><img alt="../images/sphx_glr_plot_beta_divergence_0011.png" src="../images/sphx_glr_plot_beta_divergence_0011.png" style="width: 480.0px; height: 360.0px;" /></a>
</div>
<p>请注意，如果 <span class="math notranslate nohighlight">\(\beta \in (0; 1)\)</span> ，此定义无效，仅仅在 <span class="math notranslate nohighlight">\(d_{KL}\)</span> 和 <span class="math notranslate nohighlight">\(d_{IS}\)</span> 上可以分别连续扩展。</p>
<p><a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> 类使用坐标下降法(Coordinate Descent (‘cd’)) <a class="footnote-reference" href="#id23" id="id18">[5]</a> 和乘法更新(Multiplicative Update (‘mu’)) <a class="footnote-reference" href="#id24" id="id19">[6]</a> 来实现两个求解器(solvers)。
‘mu’ 求解器可以优化每个 beta-divergence，包括 Frobenius 范数(<span class="math notranslate nohighlight">\(\beta=2\)</span>) ，(广义)KL散度(<span class="math notranslate nohighlight">\(\beta=1\)</span>)和Itakura-Saito 散度 (<span class="math notranslate nohighlight">\(\beta=0\)</span>)。
请注意，对于 <span class="math notranslate nohighlight">\(\beta \in (1; 2)\)</span>, ‘mu’求解器明显快于 <span class="math notranslate nohighlight">\(\beta\)</span> 的其他值。
还要注意，使用小于0的(或0，即 ‘itakura-saito’ ) <span class="math notranslate nohighlight">\(\beta\)</span> ，输入矩阵不能包含零值。</p>
<p>‘cd’ solver 只能优化 Frobenius 范数。由于 NMF 的潜在非凸性，即使优化相同的距离函数， 不同的求解器也可能会收敛到不同的最小值。</p>
<p>NMF最适用于 <code class="docutils literal notranslate"><span class="pre">fit_transform</span></code> 方法，该方法返回矩阵 W。矩阵H被存储到拟合后的模型的 <code class="docutils literal notranslate"><span class="pre">components_</span></code> 属性中中;
方法 <code class="docutils literal notranslate"><span class="pre">transform</span></code> 将基于这些存储的components去分解新的矩阵 X_new:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="k">import</span> <span class="n">NMF</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">NMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">W</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">H</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">components_</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">6.1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">W_new</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py"><span class="std std-ref">Faces dataset decompositions</span></a></li>
<li><a class="reference internal" href="../auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py"><span class="std std-ref">Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation</span></a></li>
<li><a class="reference internal" href="../auto_examples/decomposition/plot_beta_divergence.html#sphx-glr-auto-examples-decomposition-plot-beta-divergence-py"><span class="std std-ref">Beta-divergence loss functions</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<table class="docutils footnote" frame="void" id="id20" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id14">[1]</a></td><td><a class="reference external" href="http://www.columbia.edu/~jwp2128/Teaching/E4903/papers/nmf_nature.pdf">“Learning the parts of objects by non-negative matrix factorization”</a>
D. Lee, S. Seung, 1999</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id21" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id15">[2]</a></td><td><a class="reference external" href="http://www.jmlr.org/papers/volume5/hoyer04a/hoyer04a.pdf">“Non-negative Matrix Factorization with Sparseness Constraints”</a>
P. Hoyer, 2004</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id22" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id16">[4]</a></td><td><a class="reference external" href="http://scgroup.hpclab.ceid.upatras.gr/faculty/stratis/Papers/HPCLAB020107.pdf">“SVD based initialization: A head start for nonnegative
matrix factorization”</a>
C. Boutsidis, E. Gallopoulos, 2008</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id23" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id18">[5]</a></td><td><a class="reference external" href="http://www.bsp.brain.riken.jp/publications/2009/Cichocki-Phan-IEICE_col.pdf">“Fast local algorithms for large scale nonnegative matrix and tensor
factorizations.”</a>
A. Cichocki, A. Phan, 2009</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id24" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[6]</td><td><em>(<a class="fn-backref" href="#id17">1</a>, <a class="fn-backref" href="#id19">2</a>)</em> <a class="reference external" href="https://arxiv.org/pdf/1010.1763.pdf">“Algorithms for nonnegative matrix factorization with the beta-divergence”</a>
C. Fevotte, J. Idier, 2011</td></tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="section" id="dirichlet-lda">
<span id="latentdirichletallocation"></span><h2>2.5.7. 隐Dirichlet分配 (LDA)<a class="headerlink" href="#dirichlet-lda" title="Permalink to this headline">¶</a></h2>
<p>隐 Dirichlet 分配是用于离散数据集（如文本语料库）的集合的生成概率模型(generative probabilistic model)。
它也是一个主题模型(topic model)，用于从文档集合中发现抽象主题(abstract topics)。</p>
<p>LDA的图形模型(graphical model)是一个三级生成模型( three-level generative model):</p>
<img alt="../images/lda_model_graph.png" class="align-center" src="../images/lda_model_graph.png" />
<p>关于上述图形模型中的符号的注意事项，可在Hoffman等人中找到(Hoffman et al. (2013)):</p>
<blockquote>
<div><ul class="simple">
<li>语料库(corpus)是 <span class="math notranslate nohighlight">\(D\)</span> 个文档(documents)的集合。</li>
<li>一个文档(document)是 <span class="math notranslate nohighlight">\(N\)</span> 个单词(words)的序列。</li>
<li>一个语料库有 <span class="math notranslate nohighlight">\(K\)</span> 个主题(topics)。</li>
<li>图中的boxes代表了重复采样。</li>
</ul>
</div></blockquote>
<p>在上述图形模型中, 每个节点是个随机变量并且在生成过程里面有个角色。
阴影节点表示观察到的变量(observed variable)，无阴影节点表示隐藏(潜在)变量(hidden (latent) variable)。
在这种情况下，语料库中的单词是我们观察到的唯一数据。潜在变量决定了语料库中主题的随机混合和文档中单词的分布。
LDA的目的是利用观察到的词来推断隐藏的主题结构(topic structure)。</p>
<p>当建模文本语料库时，对具有 <span class="math notranslate nohighlight">\(D\)</span> 个文档和 <span class="math notranslate nohighlight">\(K\)</span> 个主题的语料库，该模型假设了以下生成过程。
其中，<span class="math notranslate nohighlight">\(K\)</span> 对应于API中的 <code class="xref py py-attr docutils literal notranslate"><span class="pre">n_components</span></code> :</p>
<blockquote>
<div><ol class="arabic simple">
<li>对每个主题 <span class="math notranslate nohighlight">\(k \in K\)</span>, draw <span class="math notranslate nohighlight">\(\beta_k \sim \mathrm{Dirichlet}(\eta)\)</span>。 这可以为单词提供一个分布,
i.e. 某个单词出现在主题 <span class="math notranslate nohighlight">\(k\)</span> 中的概率。
<span class="math notranslate nohighlight">\(\eta\)</span> 对应于 <code class="xref py py-attr docutils literal notranslate"><span class="pre">topic_word_prior</span></code> 。</li>
<li>对每一个文档 <span class="math notranslate nohighlight">\(d \in D\)</span>, draw the topic proportions <span class="math notranslate nohighlight">\(\theta_d \sim \mathrm{Dirichlet}(\alpha)\)</span> 。</li>
</ol>
<blockquote>
<div><span class="math notranslate nohighlight">\(\alpha\)</span> 对应于 <code class="xref py py-attr docutils literal notranslate"><span class="pre">doc_topic_prior</span></code> 。</div></blockquote>
<ol class="arabic simple" start="3">
<li>对文档 <span class="math notranslate nohighlight">\(d\)</span> 中的每一个单词 <span class="math notranslate nohighlight">\(i\)</span> :</li>
</ol>
<blockquote>
<div><ol class="loweralpha simple">
<li>Draw the topic assignment <span class="math notranslate nohighlight">\(z_{di} \sim \mathrm{Multinomial}(\theta_d)\)</span></li>
<li>Draw the observed word <span class="math notranslate nohighlight">\(w_{ij} \sim \mathrm{Multinomial}(\beta_{z_{di}})\)</span></li>
</ol>
</div></blockquote>
</div></blockquote>
<p>对于参数估计, 后验分布如下:</p>
<div class="math notranslate nohighlight">
\[p(z, \theta, \beta |w, \alpha, \eta) =
  \frac{p(z, \theta, \beta|\alpha, \eta)}{p(w|\alpha, \eta)}\]</div>
<p>由于后验是难以处理的，所以变分贝叶斯方法使用了一种更简单的分布 <span class="math notranslate nohighlight">\(q(z,\theta,\beta | \lambda, \phi, \gamma)\)</span>
来近似它, 然后优化这些变分参数 <span class="math notranslate nohighlight">\(\lambda\)</span>, <span class="math notranslate nohighlight">\(\phi\)</span>, <span class="math notranslate nohighlight">\(\gamma\)</span> 使得 Evidence Lower Bound (ELBO) 达到最大化:</p>
<div class="math notranslate nohighlight">
\[\log\: P(w | \alpha, \eta) \geq L(w,\phi,\gamma,\lambda) \overset{\triangle}{=}
  E_{q}[\log\:p(w,z,\theta,\beta|\alpha,\eta)] - E_{q}[\log\:q(z, \theta, \beta)]\]</div>
<p>最大化 ELBO 等价于最小化 <span class="math notranslate nohighlight">\(q(z,\theta,\beta)\)</span> 和 真实的后验分布 <span class="math notranslate nohighlight">\(p(z, \theta, \beta |w, \alpha, \eta)\)</span> 之间的 KL散度。</p>
<p><a class="reference internal" href="generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation" title="sklearn.decomposition.LatentDirichletAllocation"><code class="xref py py-class docutils literal notranslate"><span class="pre">LatentDirichletAllocation</span></code></a> 类实现了在线变分贝叶斯算法并且支持在线及批量更新方法。
批处理方法在每次完全遍历数据后更新变分变量(variational variables)，而，在线方法则处理小批量数据(mini-batch data)后更新变分变量。</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">虽然在线方法可以保证收敛到局部最优点，但最优点的质量和收敛速度可能取决于小批量的大小和与学习速率设置有关的属性。</p>
</div>
<p>当 <a class="reference internal" href="generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation" title="sklearn.decomposition.LatentDirichletAllocation"><code class="xref py py-class docutils literal notranslate"><span class="pre">LatentDirichletAllocation</span></code></a> 应用于”document-term”矩阵时，该矩阵将被分解为”topic-term”矩阵和”document-topic”矩阵。
当”topic-term”矩阵作为 <code class="xref py py-attr docutils literal notranslate"><span class="pre">components_</span></code> 存储在模型中时，可以通过 <code class="docutils literal notranslate"><span class="pre">transform</span></code> 方法计算”document-topic”矩阵。</p>
<p><a class="reference internal" href="generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation" title="sklearn.decomposition.LatentDirichletAllocation"><code class="xref py py-class docutils literal notranslate"><span class="pre">LatentDirichletAllocation</span></code></a> 类也实现了 <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code> 方法。 该方法是在数据可以按顺序获取时使用的。</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py"><span class="std std-ref">Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation</span></a></li>
</ul>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">“Latent Dirichlet Allocation”</a>
D. Blei, A. Ng, M. Jordan, 2003</li>
<li><a class="reference external" href="https://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation.pdf">“Online Learning for Latent Dirichlet Allocation”</a>
M. Hoffman, D. Blei, F. Bach, 2010</li>
<li><a class="reference external" href="http://www.columbia.edu/~jwp2128/Papers/HoffmanBleiWangPaisley2013.pdf">“Stochastic Variational Inference”</a>
M. Hoffman, D. Blei, C. Wang, J. Paisley, 2013</li>
</ul>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
  </div>
  
  <div class="footer">
    &copy; 2007 - 2018, scikit-learn developers (BSD License).
    <!--
    <a href="../_sources/modules/decomposition.rst.txt" rel="nofollow">Show this page source</a> -->
  </div>
  <div class="rel">
    
      <div class="buttonPrevious">
        <a href="biclustering.html">Previous
        </a>
      </div>
      <div class="buttonNext">
        <a href="covariance.html">Next
        </a>
      </div>
      
    </div>

    
    <script>
      window.ga = window.ga || function () { (ga.q = ga.q || []).push(arguments) }; ga.l = +new Date;
      ga('create', 'UA-22606712-2', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    
    <script>
      (function () {
        var cx = '016639176250731907682:tjtqbvtvij0';
        var gcse = document.createElement('script'); gcse.type = 'text/javascript'; gcse.async = true;
        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s);
      })();
    </script>
  </body>
</html>