

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

    <title>1.17. 监督神经网络模型(Supervised Neural network models) &#8212; scikit-learn 0.20.2 documentation</title>
<!-- htmltitle is before nature.css - we use this hack to load bootstrap first -->
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="../static/css/bootstrap.min.css" media="screen" />
<link rel="stylesheet" href="../static/css/bootstrap-responsive.css" />

    <link rel="stylesheet" href="../static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../static/gallery.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../static/documentation_options.js"></script>
    <script type="text/javascript" src="../static/jquery.js"></script>
    <script type="text/javascript" src="../static/underscore.js"></script>
    <script type="text/javascript" src="../static/doctools.js"></script>
    <script type="text/javascript" src="../static/js/copybutton.js"></script>
    <script type="text/javascript" src="../static/js/extra.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_SVG"></script>
    <link rel="shortcut icon" href="../static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2. 无监督学习(unsupervised learning)" href="../unsupervised_learning.html" />
    <link rel="prev" title="1.16. 概率校准(Probability calibration)" href="calibration.html" />


<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<script src="../static/js/bootstrap.min.js" type="text/javascript"></script>
<script>
  VERSION_SUBDIR = (function (groups) {
    return groups ? groups[1] : null;
  })(location.href.match(/^https?:\/\/scikit-learn.org\/([^\/]+)/));
</script>
<link rel="canonical" href="http://scikit-learn.org/stable/modules/neural_networks_supervised.html" />

<script type="text/javascript">
  $("div.buttonNext, div.buttonPrevious").hover(
    function () {
      $(this).css('background-color', '#FF9C34');
    },
    function () {
      $(this).css('background-color', '#A7D6E2');
    }
  );
  function showMenu() {
    var topNav = document.getElementById("scikit-navbar");
    if (topNav.className === "navbar") {
      topNav.className += " responsive";
    } else {
      topNav.className = "navbar";
    }
  };
</script>

<!-- 百度站长统计代码 -->
<script>
  var _hmt = _hmt || [];
  (function () {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?e7836e37a4cb7584127a787e9b44e3f1";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
  })();
</script>


  </head><body>

<div class="header-wrapper">
  <div class="header">
    <p class="logo"><a href="../index.html">
        <img src="../static/scikit-learn-logo-small.png" alt="Logo" />
      </a>
    </p><div class="navbar" id="scikit-navbar">
      <ul>
        <li><a href="../index.html">首页</a></li>
        <li><a href="../install.html">安装</a></li>
        <li class="btn-li">
          <div class="btn-group">
            <a href="../documentation.html">文档</a>
            <a class="btn dropdown-toggle" data-toggle="dropdown">
              <span class="caret"></span>
            </a>
            <ul class="dropdown-menu">
              <li class="link-title">Scikit-learn
                <script>document.write(DOCUMENTATION_OPTIONS.VERSION + (VERSION_SUBDIR ? " (" + VERSION_SUBDIR + ")" : ""));</script>
              </li>
              <li><a href="../tutorial/index.html">教程</a></li>
              <li><a href="../user_guide.html">用户指南</a></li>
              <li><a href="classes.html">API</a></li>
              <li><a href="../glossary.html">词汇表</a></li>
              <li><a href="../faq.html">FAQ</a></li>
              <li><a href="../developers/contributing.html">贡献</a></li>
              <li><a href="../roadmap.html">路线图</a></li>
              <li class="divider"></li>
              <script>if (VERSION_SUBDIR != "stable") document.write('<li><a href="https://www.studyai.cn">稳定版</a></li>')</script>
              <script>if (VERSION_SUBDIR != "dev") document.write('<li><a href="http://scikit-learn.org/dev/documentation.html" target="_blank">开发版</a></li>')</script>
              <li><a href="http://scikit-learn.org/dev/versions.html">所有可用版本</a></li>
              <li><a href="../downloads/scikit-learn-docs.pdf">PDF 文档</a></li>
            </ul>
          </div>
        </li>
        <li><a href="../auto_examples/index.html">案例</a></li>
      </ul>
      <a href="javascript:void(0);" onclick="showMenu()">
        <div class="nav-icon">
          <div class="hamburger-line"></div>
          <div class="hamburger-line"></div>
          <div class="hamburger-line"></div>
        </div>
      </a>
      <div class="search_form">
        <div class="gcse-search" id="cse" style="width: 100%;"></div>
      </div>
    </div> <!-- end navbar --></div>
</div>


<!-- GitHub "fork me" ribbon -->
<a href="https://github.com/scikit-learn/scikit-learn">
  <img class="fork-me" style="position: absolute; top: 0; right: 0; border: 0;" src="../static/img/forkme.png"
    alt="Fork me on GitHub" />
</a>

<div class="content-wrapper">
  <div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
      <div class="rel">
        
          <div class="rellink">
            <a href="calibration.html" accesskey="P">Previous
              <br />
              <span class="smallrellink">
                1.16. 概率校准(Pr...
              </span>
              <span class="hiddenrellink">
                1.16. 概率校准(Probability calibration)
              </span>
            </a>
          </div>
          <div class="spacer">
            &nbsp;
          </div>
          <div class="rellink">
            <a href="../unsupervised_learning.html" accesskey="N">Next
              <br />
              <span class="smallrellink">
                2. 无监督学习(unsu...
              </span>
              <span class="hiddenrellink">
                2. 无监督学习(unsupervised learning)
              </span>
            </a>
          </div>

          <!-- Ad a link to the 'up' page -->
          <div class="spacer">
            &nbsp;
          </div>
          <div class="rellink">
            <a href="../supervised_learning.html">
              Up
              <br />
              <span class="smallrellink">
                1. 监督学习(super...
              </span>
                <span class="hiddenrellink">
                  1. 监督学习(supervised learning)
                </span>
                
            </a>
          </div>
        </div>
        
        <p class="doc-version"><b>scikit-learn v0.20.2</b><br />
          <a href="http://scikit-learn.org/dev/versions.html">其他版本</a></p>
        <!-- <p class="citing">Please <b><a href="../about.html#citing-scikit-learn" style="font-size: 110%;">cite
              us </a></b>if you use the software.</p> -->
        <p class="citing">该中文文档由人工智能社区的<a href="http://www.studyai.com/antares" target="_blank">Antares</a>翻译!
        </p>
        <ul>
<li><a class="reference internal" href="#">1.17. 监督神经网络模型(Supervised Neural network models)</a><ul>
<li><a class="reference internal" href="#multilayer-perceptron">1.17.1. 多层感知器</a></li>
<li><a class="reference internal" href="#id2">1.17.2. 分类</a></li>
<li><a class="reference internal" href="#id3">1.17.3. 回归</a></li>
<li><a class="reference internal" href="#id4">1.17.4. 正则化</a></li>
<li><a class="reference internal" href="#id5">1.17.5. 算法</a></li>
<li><a class="reference internal" href="#id6">1.17.6. 复杂度</a></li>
<li><a class="reference internal" href="#id7">1.17.7. 数学表达式</a></li>
<li><a class="reference internal" href="#mlp-tips">1.17.8. 实用小建议</a></li>
<li><a class="reference internal" href="#warm-start">1.17.9. 使用warm_start进行更多控制</a></li>
</ul>
</li>
</ul>

        <br />
        <p>
          <a href="https://study.163.com/course/introduction/1209532843.htm?share=2&shareId=400000000535031" target="_blank">
            <img src="../static/img/advitise1.png" alt="座右铭" />
          </a>
        </p>
        <br />
        <p class="doc-version" style="font-size:10%">
          注意!本网站的网址是以 <em>https://</em> 开头的，而不是以 <em>http://</em> 开头的!!!
        </p>
      </div>
    </div>
    
    <input type="checkbox" id="nav-trigger" class="nav-trigger" checked />
    <label for="nav-trigger"></label>
    
    


    <div class="content">
      
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="supervised-neural-network-models">
<span id="neural-networks-supervised"></span><h1>1.17. 监督神经网络模型(Supervised Neural network models)<a class="headerlink" href="#supervised-neural-network-models" title="Permalink to this headline">¶</a></h1>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">此实现不打算用于大规模应用程序。特别是，scikit-learn 没有提供GPU支持。
为了更快，基于GPU的实现以及框架提供了构建深度学习体系结构的更多灵活性,
请参考 <a class="reference internal" href="../related_projects.html#related-projects"><span class="std std-ref">相关工程</span></a>。</p>
</div>
<div class="section" id="multilayer-perceptron">
<span id="id1"></span><h2>1.17.1. 多层感知器<a class="headerlink" href="#multilayer-perceptron" title="Permalink to this headline">¶</a></h2>
<p><strong>Multi-layer Perceptron (MLP)</strong> 是一种监督学习算法，通过在数据集上训练学习一个函数 <span class="math notranslate nohighlight">\(f(\cdot): R^m \rightarrow R^o\)</span> ,
其中 <span class="math notranslate nohighlight">\(m\)</span> 是输入数据的维数， <span class="math notranslate nohighlight">\(o\)</span> 是输出数据的维数。给定一个特征向量的集合 <span class="math notranslate nohighlight">\(X = {x_1, x_2, ..., x_m}\)</span> 和
一个目标 <span class="math notranslate nohighlight">\(y\)</span> , MLP可以学习一个非线性函数逼近器(non-linear function approximator)用于分类或回归。MLP不同于logistic regression,
因为MLP在输入层和输出层之间还可以有一个或更多的非线性层，这被称之为 隐藏层(hidden layers)。Figure 1 展示了有一个隐藏层和标量输出的MLP。</p>
<div class="figure align-center" id="id11">
<a class="reference internal image-reference" href="../images/multilayerperceptron_network.png"><img alt="../images/multilayerperceptron_network.png" src="../images/multilayerperceptron_network.png" style="width: 469.79999999999995px; height: 510.59999999999997px;" /></a>
<p class="caption"><span class="caption-text"><strong>Figure 1 : One hidden layer MLP.</strong></span></p>
</div>
<p>最左边的层, 称之为输入层(input layer), 由一系列神经元 <span class="math notranslate nohighlight">\(\{x_i | x_1, x_2, ..., x_m\}\)</span> 构成，表达了输入特征向量(input features)。
隐藏层的每个神经元使用一个加权线性汇总 <span class="math notranslate nohighlight">\(w_1x_1 + w_2x_2 + ... + w_mx_m\)</span> 对前面一层的数据进行变换, 变换以后再紧跟着一个非线性激活函数
( <span class="math notranslate nohighlight">\(g(\cdot):R \rightarrow R\)</span> ) - 比如双曲正切函数(hyperbolic tan function)。
输出层接受来自最后一个隐藏层的数据然后将其变换为输出值。</p>
<p>该模块有公共属性 <code class="docutils literal notranslate"><span class="pre">coefs_</span></code> 和 <code class="docutils literal notranslate"><span class="pre">intercepts_</span></code> 。
<code class="docutils literal notranslate"><span class="pre">coefs_</span></code> 是权重矩阵的列表，其中在第 <span class="math notranslate nohighlight">\(i\)</span> 个 索引位置上的权重矩阵代表了 <span class="math notranslate nohighlight">\(i\)</span> 层和 <span class="math notranslate nohighlight">\(i+1\)</span> 层的权重。
<code class="docutils literal notranslate"><span class="pre">intercepts_</span></code> 是偏置向量的列表，在第 <span class="math notranslate nohighlight">\(i\)</span> 个 索引位置上的向量代表了加在 <span class="math notranslate nohighlight">\(i+1\)</span> 层上的偏置值。</p>
<p>多层感知器的优点如下:</p>
<blockquote>
<div><ul class="simple">
<li>学习非线性模型的能力。</li>
<li>使用 <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code> 实时(在线)学习模型的能力。</li>
</ul>
</div></blockquote>
<p>多层感知器的缺点如下:</p>
<blockquote>
<div><ul class="simple">
<li>带有隐藏层的MLP有非凸(non-convex)损失函数,存在不止一个局部极小值。因此，不同的随机权重初始化会导致不同的验证准确率。</li>
<li>MLP 需要调节很多超参数比如 隐藏层的神经元数量，隐藏层的数量以及迭代次数等。</li>
<li>MLP 对特征尺度缩放(feature scaling)很敏感。</li>
</ul>
</div></blockquote>
<p>请查看 <a class="reference internal" href="#mlp-tips"><span class="std std-ref">Tips on Practical Use</span></a> 小节 ，强调了其中的一些缺点。</p>
</div>
<div class="section" id="id2">
<h2>1.17.2. 分类<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>类 <a class="reference internal" href="generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier" title="sklearn.neural_network.MLPClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">MLPClassifier</span></code></a> 实现了一个多层感知器(MLP)算法，使用反向传播进行训练
<a class="reference external" href="http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm">Backpropagation</a> 。</p>
<p>MLP的训练在两个数组上进行：size为 (n_samples, n_features) 的数组 X , 持有以浮点特征向量表示的训练样本；
size为(n_samples,)的数组 y , 持有训练样本的目标值(类标签):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="k">import</span> <span class="n">MLPClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
<span class="gp">... </span>                    <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>                         
<span class="go">MLPClassifier(activation=&#39;relu&#39;, alpha=1e-05, batch_size=&#39;auto&#39;,</span>
<span class="go">              beta_1=0.9, beta_2=0.999, early_stopping=False,</span>
<span class="go">              epsilon=1e-08, hidden_layer_sizes=(5, 2),</span>
<span class="go">              learning_rate=&#39;constant&#39;, learning_rate_init=0.001,</span>
<span class="go">              max_iter=200, momentum=0.9, n_iter_no_change=10,</span>
<span class="go">              nesterovs_momentum=True, power_t=0.5, random_state=1,</span>
<span class="go">              shuffle=True, solver=&#39;lbfgs&#39;, tol=0.0001,</span>
<span class="go">              validation_fraction=0.1, verbose=False, warm_start=False)</span>
</pre></div>
</div>
<p>训练好以后, 模型就可以对新的样本预测其类别标签啦</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">]])</span>
<span class="go">array([1, 0])</span>
</pre></div>
</div>
<p>MLP 可以为给定的训练数据拟合一个非线性模型。 <code class="docutils literal notranslate"><span class="pre">clf.coefs_</span></code> 包含构成模型的所有权重矩阵</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">coef</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">coef</span> <span class="ow">in</span> <span class="n">clf</span><span class="o">.</span><span class="n">coefs_</span><span class="p">]</span>
<span class="go">[(2, 5), (5, 2), (2, 1)]</span>
</pre></div>
</div>
<p>目前为止, <a class="reference internal" href="generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier" title="sklearn.neural_network.MLPClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">MLPClassifier</span></code></a> 仅支持交叉熵损失(Cross-Entropy loss)函数, 这就允许我们通过运行 <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> 方法进行概率估计了。</p>
<p>MLP 使用反向传播(Backpropagation)进行训练。更准确的说, 它使用某种形式的梯度下降法进行训练，而梯度的计算用到了反向传播技术。
对于分类来说，它最小化交叉熵损失函数,为每个样本 <span class="math notranslate nohighlight">\(x\)</span> 给出一个该样本属于各个类别的概率的估计 <span class="math notranslate nohighlight">\(P(y|x)\)</span></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>  
<span class="go">array([[1.967...e-04, 9.998...-01],</span>
<span class="go">       [1.967...e-04, 9.998...-01]])</span>
</pre></div>
</div>
<p><a class="reference internal" href="generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier" title="sklearn.neural_network.MLPClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">MLPClassifier</span></code></a> 类通过使用软最大化(<a class="reference external" href="https://en.wikipedia.org/wiki/Softmax_activation_function">Softmax</a>)
作为输出函数 支持多类别分类。</p>
<p>更进一步, MLP模型还支持多标签分类(<a class="reference internal" href="multiclass.html#multiclass"><span class="std std-ref">multi-label classification</span></a>),其中一个样本可以属于不止一个类。
对每个类，原始输出会被传递到logistic函数。凡是大于等于0.5的值就被弄成1，反之则为0。对一个样本的一个预测输出(向量)，
值为 <cite>1</cite> 的那个索引位置代表了该样本被分配的类别</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
<span class="gp">... </span>                    <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,),</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>                         
<span class="go">MLPClassifier(activation=&#39;relu&#39;, alpha=1e-05, batch_size=&#39;auto&#39;,</span>
<span class="go">              beta_1=0.9, beta_2=0.999, early_stopping=False,</span>
<span class="go">              epsilon=1e-08, hidden_layer_sizes=(15,),</span>
<span class="go">              learning_rate=&#39;constant&#39;, learning_rate_init=0.001,</span>
<span class="go">              max_iter=200, momentum=0.9, n_iter_no_change=10,</span>
<span class="go">              nesterovs_momentum=True, power_t=0.5,  random_state=1,</span>
<span class="go">              shuffle=True, solver=&#39;lbfgs&#39;, tol=0.0001,</span>
<span class="go">              validation_fraction=0.1, verbose=False, warm_start=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>
<span class="go">array([[1, 1]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]])</span>
<span class="go">array([[0, 1]])</span>
</pre></div>
</div>
<p>请看下面的例子和文档 详细了解 <a class="reference internal" href="generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier.fit" title="sklearn.neural_network.MLPClassifier.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">MLPClassifier.fit</span></code></a> 的更多信息。</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/neural_networks/plot_mlp_training_curves.html#sphx-glr-auto-examples-neural-networks-plot-mlp-training-curves-py"><span class="std std-ref">Compare Stochastic learning strategies for MLPClassifier</span></a></li>
<li><a class="reference internal" href="../auto_examples/neural_networks/plot_mnist_filters.html#sphx-glr-auto-examples-neural-networks-plot-mnist-filters-py"><span class="std std-ref">Visualization of MLP weights on MNIST</span></a></li>
</ul>
</div>
</div>
<div class="section" id="id3">
<h2>1.17.3. 回归<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor" title="sklearn.neural_network.MLPRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">MLPRegressor</span></code></a> 类实现了一个多层感知器(MLP)，使用反向传播训练，输出层没有激活函数，也可以认为输出层的激活函数是 identity function。
因此，它使用平方误差(square error)作为损失函数, 输出是一系列连续值。</p>
<p><a class="reference internal" href="generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor" title="sklearn.neural_network.MLPRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">MLPRegressor</span></code></a> 类也支持 多输出回归(multi-output regression), 每一个样本有不止一个目标值(target)。</p>
</div>
<div class="section" id="id4">
<h2>1.17.4. 正则化<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor" title="sklearn.neural_network.MLPRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">MLPRegressor</span></code></a> 和 <a class="reference internal" href="generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier" title="sklearn.neural_network.MLPClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">MLPClassifier</span></code></a> 使用参数 <code class="docutils literal notranslate"><span class="pre">alpha</span></code> 来控制正则化量(L2 正则化)以通过对权重的惩罚避免过拟合发生。
下面的图绘制了 决策函数随着 alpha 值的变化而变化。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/neural_networks/plot_mlp_alpha.html"><img alt="../images/sphx_glr_plot_mlp_alpha_0011.png" src="../images/sphx_glr_plot_mlp_alpha_0011.png" style="width: 1275.0px; height: 675.0px;" /></a>
</div>
<p>请看下面的例子获得更多信息。</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/neural_networks/plot_mlp_alpha.html#sphx-glr-auto-examples-neural-networks-plot-mlp-alpha-py"><span class="std std-ref">Varying regularization in Multi-layer Perceptron</span></a></li>
</ul>
</div>
</div>
<div class="section" id="id5">
<h2>1.17.5. 算法<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<p>MLP 使用 <a class="reference external" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">Stochastic Gradient Descent</a>,
<a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam</a>, 或
<a class="reference external" href="https://en.wikipedia.org/wiki/Limited-memory_BFGS">L-BFGS</a> 进行训练。
Stochastic Gradient Descent (SGD) 使用损失函数相对需要更新的那个参数的梯度对参数进行更新。如下所示：</p>
<div class="math notranslate nohighlight">
\[w \leftarrow w - \eta (\alpha \frac{\partial R(w)}{\partial w}
+ \frac{\partial Loss}{\partial w})\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\eta\)</span> 是控制参数空间搜索步长的学习率。 <span class="math notranslate nohighlight">\(Loss\)</span> 是网络使用的损失函数。</p>
<p>更多详细信息请参考 <a class="reference external" href="http://scikit-learn.org/stable/modules/sgd.html">SGD</a></p>
<p>Adam 与 SGD 类似，因为它也是一个随机优化器(stochastic optimizer), 但是它可以根据低阶矩(lower-order moments)的自适应估计，自动调整参数更新量。</p>
<p>使用 SGD 或 Adam, 训练过程支持在线和小批量学习(online and mini-batch learning)。</p>
<p>L-BFGS 是一个逼近Hessian matrix的求解器，Hessian matrix是一个函数的二阶偏导数。
更进一步，它通过逼近 Hessian matrix 的逆来执行参数更新。该算法的实现使用了SciPy的版本
<a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html">L-BFGS</a> 。</p>
<p>如果选择了 ‘L-BFGS’, 训练过程不支持 在线学习 或 小批量学习 。</p>
</div>
<div class="section" id="id6">
<h2>1.17.6. 复杂度<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<p>假定我们有 <span class="math notranslate nohighlight">\(n\)</span> 个训练样本, <span class="math notranslate nohighlight">\(m\)</span> 个特征分量, <span class="math notranslate nohighlight">\(k\)</span> 个隐藏层,
每个包含 <span class="math notranslate nohighlight">\(h\)</span> 个神经元(简单点儿), <span class="math notranslate nohighlight">\(o\)</span> 个输出神经元。反向传播的时间复杂度是
<span class="math notranslate nohighlight">\(O(n\cdot m \cdot h^k \cdot o \cdot i)\)</span>, 其中 <span class="math notranslate nohighlight">\(i\)</span> 是迭代次数。
因为反向传播有很高的时间复杂度，建议大家从一个较小数量的隐藏层以及隐藏层神经元数量 开始设计模型并用于训练。</p>
</div>
<div class="section" id="id7">
<h2>1.17.7. 数学表达式<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h2>
<p>给定一个训练样本集合 <span class="math notranslate nohighlight">\((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\)</span>
其中 <span class="math notranslate nohighlight">\(x_i \in \mathbf{R}^n\)</span> 和 <span class="math notranslate nohighlight">\(y_i \in \{0, 1\}\)</span>, 具有一个隐藏层一个隐藏神经元的MLP
学习这样一个函数 <span class="math notranslate nohighlight">\(f(x) = W_2 g(W_1^T x + b_1) + b_2\)</span>
其中 <span class="math notranslate nohighlight">\(W_1 \in \mathbf{R}^m\)</span> 和 <span class="math notranslate nohighlight">\(W_2, b_1, b_2 \in \mathbf{R}\)</span> 是
模型参数。 <span class="math notranslate nohighlight">\(W_1, W_2\)</span> 分别表示输入层和隐藏层的权重； <span class="math notranslate nohighlight">\(b_1, b_2\)</span> 分别表示添加到隐藏层和输出层的偏置。
<span class="math notranslate nohighlight">\(g(\cdot) : R \rightarrow R\)</span> 是激活函数, 默认设为 双曲正切函数(hyperbolic tan)，其表达式如下：</p>
<div class="math notranslate nohighlight">
\[g(z)= \frac{e^z-e^{-z}}{e^z+e^{-z}}\]</div>
<p>对于二分类, <span class="math notranslate nohighlight">\(f(x)\)</span> 被传递到 logistic 函数 <span class="math notranslate nohighlight">\(g(z)=1/(1+e^{-z})\)</span> 里面去
以获得介于0和1之间的输出值。如果某个样本对应的网络输出值大于等于 0.5，那么这个样本就被判断为正样本，反之则为负样本。</p>
<p>如果类的个数不止两个, 该 <span class="math notranslate nohighlight">\(f(x)\)</span> 函数自身将是一个size为 (n_classes,) 的向量。
这时候我们不会把它传递到logistic函数中去了，而是传递到软最大化函数(softmax)中去，表达式如下：</p>
<div class="math notranslate nohighlight">
\[\text{softmax}(z)_i = \frac{\exp(z_i)}{\sum_{l=1}^k\exp(z_l)}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(z_i\)</span> 是输入到softmax的数据中的第 <span class="math notranslate nohighlight">\(i\)</span> 个元素,
与第 <span class="math notranslate nohighlight">\(i\)</span> 个类相对应，<span class="math notranslate nohighlight">\(K\)</span> 是类的总数量。
网络输出的结果是一个向量，包含了样本 <span class="math notranslate nohighlight">\(x\)</span> 属于每个类的概率。网络的最终预测结果是拥有最高类概率的那个类。</p>
<p>在回归问题中，输出任然是 <span class="math notranslate nohighlight">\(f(x)\)</span>; 因此, 输出激活函数只是一个 identity function。</p>
<p>MLP 根据问题的类型选择不同的损失函数。 用于分类的损失函数是交叉熵(Cross-Entropy)损失，在二元分类的情况下由下式给出：</p>
<div class="math notranslate nohighlight">
\[Loss(\hat{y},y,W) = -y \ln {\hat{y}} - (1-y) \ln{(1-\hat{y})} + \alpha ||W||_2^2\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\alpha ||W||_2^2\)</span> 是一个 L2-regularization 项 (aka penalty) 用来惩罚模型复杂度；
<span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span> 是一个非负超参数用来控制惩罚量。</p>
<p>对于回归问题, MLP 使用平方误差(Square Error)损失函数; 如下所示：</p>
<div class="math notranslate nohighlight">
\[Loss(\hat{y},y,W) = \frac{1}{2}||\hat{y} - y ||_2^2 + \frac{\alpha}{2} ||W||_2^2\]</div>
<p>从一个初始的随机权重开始，多层感知器(MLP)通过不断的更新这些权重来最小化损失函数。
当损失被计算出来后，反向传递将它从输出层传播到前一层，为每一个权重参数提供一个更新值使得损失下降。</p>
<p>在梯度下降算法中，损失相对于权重的梯度 <span class="math notranslate nohighlight">\(\nabla Loss_{W}\)</span> 被计算出来然后再从梯度值 <span class="math notranslate nohighlight">\(W\)</span> 中减去。
这个过程的正式的数学表达式如下：</p>
<div class="math notranslate nohighlight">
\[W^{i+1} = W^i - \epsilon \nabla {Loss}_{W}^{i}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(i\)</span> 是迭代步数,  <span class="math notranslate nohighlight">\(\epsilon\)</span> 是学习率，是一个大于0的数。</p>
<p>当达到预设的最大迭代次数时，该算法停止；或 当损失的改善低于某个较小的数字时也会停止。</p>
</div>
<div class="section" id="mlp-tips">
<span id="id8"></span><h2>1.17.8. 实用小建议<a class="headerlink" href="#mlp-tips" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul>
<li><p class="first">MLP 对特征尺度缩放(feature scaling)很敏感，因此，强烈建议对你的数据进行尺度变换(scale your data) 。
比如，把输入向量 X 的 每一个属性(特征分量)缩放到 [0, 1] 或 [-1, +1], 或者 将其标准化到 均值为0方差为1。
请注意，你必须对测试数据也应用 <strong>相同</strong> 的变换操作以获得有意义的结果。</p>
<p>你可以使用 <code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code> 类进行数据标准化变换(standardization)。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">StandardScaler</span>  <span class="c1"># doctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>  <span class="c1"># doctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Don&#39;t cheat - fit only on training data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>  <span class="c1"># doctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>  <span class="c1"># doctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># apply same transformation to test data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>  <span class="c1"># doctest: +SKIP</span>
</pre></div>
</div>
<p>另一种可替代的推荐方案是在管道流 <code class="xref py py-class docutils literal notranslate"><span class="pre">Pipeline</span></code> 中使用 <code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code> 。</p>
</li>
<li><p class="first">找到一个合理的正则化参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的最好方法是使用网格搜索交叉验证类 <code class="xref py py-class docutils literal notranslate"><span class="pre">GridSearchCV</span></code>,
这个参数的取值通常在 <code class="docutils literal notranslate"><span class="pre">10.0</span> <span class="pre">**</span> <span class="pre">-np.arange(1,</span> <span class="pre">7)</span></code> 区间内。</p>
</li>
<li><p class="first">经验上来说, 我们观察到 <cite>L-BFGS</cite> 优化器 在小数据集上收敛较快结果也更好点儿。对相对较大的数据集， <cite>Adam</cite> 优化器非常鲁棒。
它通常快速收敛并给出相当好的结果。另一方面，如果学习率被正确的调节的话，带有 momentum 或 nesterov’s momentum 的 <cite>SGD</cite>
比其他的两种优化器表现更好。</p>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="warm-start">
<h2>1.17.9. 使用warm_start进行更多控制<a class="headerlink" href="#warm-start" title="Permalink to this headline">¶</a></h2>
<p>如果您希望更多地控制SGD中的停止标准或学习速度，或者希望进行额外的监视，
使用 <code class="docutils literal notranslate"><span class="pre">warm_start=True</span></code> 和 <code class="docutils literal notranslate"><span class="pre">max_iter=1</span></code> 并自己进行迭代过程是很有用的:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,),</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">... </span>    <span class="c1"># additional monitoring / inspection </span>
<span class="go">MLPClassifier(...</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="https://www.iro.umontreal.ca/~pift6266/A06/refs/backprop_old.pdf">“Learning representations by back-propagating errors.”</a>
Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams.</li>
<li><a class="reference external" href="https://leon.bottou.org/projects/sgd">“Stochastic Gradient Descent”</a> L. Bottou - Website, 2010.</li>
<li><a class="reference external" href="http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm">“Backpropagation”</a>
Andrew Ng, Jiquan Ngiam, Chuan Yu Foo, Yifan Mai, Caroline Suen - Website, 2011.</li>
<li><a class="reference external" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">“Efficient BackProp”</a>
Y. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: Tricks
of the Trade 1998.</li>
<li><a class="reference external" href="https://arxiv.org/pdf/1412.6980v8.pdf">“Adam: A method for stochastic optimization.”</a>
Kingma, Diederik, and Jimmy Ba. arXiv preprint arXiv:1412.6980 (2014).</li>
</ul>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
  </div>
  
  <div class="footer">
    &copy; 2007 - 2018, scikit-learn developers (BSD License).
    <!--
    <a href="../_sources/modules/neural_networks_supervised.rst.txt" rel="nofollow">Show this page source</a> -->
  </div>
  <div class="rel">
    
      <div class="buttonPrevious">
        <a href="calibration.html">Previous
        </a>
      </div>
      <div class="buttonNext">
        <a href="../unsupervised_learning.html">Next
        </a>
      </div>
      
    </div>

    
    <script>
      window.ga = window.ga || function () { (ga.q = ga.q || []).push(arguments) }; ga.l = +new Date;
      ga('create', 'UA-22606712-2', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    
    <script>
      (function () {
        var cx = '016639176250731907682:tjtqbvtvij0';
        var gcse = document.createElement('script'); gcse.type = 'text/javascript'; gcse.async = true;
        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s);
      })();
    </script>
  </body>
</html>