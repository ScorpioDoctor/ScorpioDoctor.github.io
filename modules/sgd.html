

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

    <title>1.5. 随机梯度下降(Stochastic Gradient Descent) &#8212; scikit-learn 0.20.2 documentation</title>
<!-- htmltitle is before nature.css - we use this hack to load bootstrap first -->
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="stylesheet" href="../_static/css/bootstrap.min.css" media="screen" />
<link rel="stylesheet" href="../_static/css/bootstrap-responsive.css" />

    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/js/copybutton.js"></script>
    <script type="text/javascript" src="../_static/js/extra.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_SVG"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1.6. 最近邻方法(Nearest Neighbors)" href="neighbors.html" />
    <link rel="prev" title="1.4. 支持向量机(Support Vector Machines)" href="svm.html" />


<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<script src="../_static/js/bootstrap.min.js" type="text/javascript"></script>
<script>
  VERSION_SUBDIR = (function (groups) {
    return groups ? groups[1] : null;
  })(location.href.match(/^https?:\/\/scikit-learn.org\/([^\/]+)/));
</script>
<link rel="canonical" href="http://scikit-learn.org/stable/modules/sgd.html" />

<script type="text/javascript">
  $("div.buttonNext, div.buttonPrevious").hover(
    function () {
      $(this).css('background-color', '#FF9C34');
    },
    function () {
      $(this).css('background-color', '#A7D6E2');
    }
  );
  function showMenu() {
    var topNav = document.getElementById("scikit-navbar");
    if (topNav.className === "navbar") {
      topNav.className += " responsive";
    } else {
      topNav.className = "navbar";
    }
  };
</script>

<!-- 百度站长统计代码 -->
<script>
  var _hmt = _hmt || [];
  (function () {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?e7836e37a4cb7584127a787e9b44e3f1";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
  })();
</script>


  </head><body>

<div class="header-wrapper">
  <div class="header">
    <p class="logo"><a href="../index.html">
        <img src="../_static/scikit-learn-logo-small.png" alt="Logo" />
      </a>
    </p><div class="navbar" id="scikit-navbar">
      <ul>
        <li><a href="../index.html">首页</a></li>
        <li><a href="../install.html">安装</a></li>
        <li class="btn-li">
          <div class="btn-group">
            <a href="../documentation.html">文档</a>
            <a class="btn dropdown-toggle" data-toggle="dropdown">
              <span class="caret"></span>
            </a>
            <ul class="dropdown-menu">
              <li class="link-title">Scikit-learn
                <script>document.write(DOCUMENTATION_OPTIONS.VERSION + (VERSION_SUBDIR ? " (" + VERSION_SUBDIR + ")" : ""));</script>
              </li>
              <li><a href="../tutorial/index.html">教程</a></li>
              <li><a href="../user_guide.html">用户指南</a></li>
              <li><a href="classes.html">API</a></li>
              <li><a href="../glossary.html">词汇表</a></li>
              <li><a href="../faq.html">FAQ</a></li>
              <li><a href="../developers/contributing.html">贡献</a></li>
              <li><a href="../roadmap.html">路线图</a></li>
              <li class="divider"></li>
              <script>if (VERSION_SUBDIR != "stable") document.write('<li><a href="https://www.studyai.cn">稳定版</a></li>')</script>
              <script>if (VERSION_SUBDIR != "dev") document.write('<li><a href="http://scikit-learn.org/dev/documentation.html" target="_blank">开发版</a></li>')</script>
              <li><a href="http://scikit-learn.org/dev/versions.html">所有可用版本</a></li>
              <li><a href="../_downloads/scikit-learn-docs.pdf">PDF 文档</a></li>
            </ul>
          </div>
        </li>
        <li><a href="../auto_examples/index.html">案例</a></li>
      </ul>
      <a href="javascript:void(0);" onclick="showMenu()">
        <div class="nav-icon">
          <div class="hamburger-line"></div>
          <div class="hamburger-line"></div>
          <div class="hamburger-line"></div>
        </div>
      </a>
      <div class="search_form">
        <div class="gcse-search" id="cse" style="width: 100%;"></div>
      </div>
    </div> <!-- end navbar --></div>
</div>


<!-- GitHub "fork me" ribbon -->
<a href="https://github.com/scikit-learn/scikit-learn">
  <img class="fork-me" style="position: absolute; top: 0; right: 0; border: 0;" src="../_static/img/forkme.png"
    alt="Fork me on GitHub" />
</a>

<div class="content-wrapper">
  <div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
      <div class="rel">
        
          <div class="rellink">
            <a href="svm.html" accesskey="P">Previous
              <br />
              <span class="smallrellink">
                1.4. 支持向量机(Su...
              </span>
              <span class="hiddenrellink">
                1.4. 支持向量机(Support Vector Machines)
              </span>
            </a>
          </div>
          <div class="spacer">
            &nbsp;
          </div>
          <div class="rellink">
            <a href="neighbors.html" accesskey="N">Next
              <br />
              <span class="smallrellink">
                1.6. 最近邻方法(Ne...
              </span>
              <span class="hiddenrellink">
                1.6. 最近邻方法(Nearest Neighbors)
              </span>
            </a>
          </div>

          <!-- Ad a link to the 'up' page -->
          <div class="spacer">
            &nbsp;
          </div>
          <div class="rellink">
            <a href="../supervised_learning.html">
              Up
              <br />
              <span class="smallrellink">
                1. 监督学习(super...
              </span>
                <span class="hiddenrellink">
                  1. 监督学习(supervised learning)
                </span>
                
            </a>
          </div>
        </div>
        
        <p class="doc-version"><b>scikit-learn v0.20.2</b><br />
          <a href="http://scikit-learn.org/dev/versions.html">其他版本</a></p>
        <!-- <p class="citing">Please <b><a href="../about.html#citing-scikit-learn" style="font-size: 110%;">cite
              us </a></b>if you use the software.</p> -->
        <p class="citing">该中文文档由人工智能社区的<a href="http://www.studyai.com/antares" target="_blank">Antares</a>翻译!
        </p>
        <ul>
<li><a class="reference internal" href="#">1.5. 随机梯度下降(Stochastic Gradient Descent)</a><ul>
<li><a class="reference internal" href="#id2">1.5.1. 分类</a></li>
<li><a class="reference internal" href="#id3">1.5.2. 回归</a></li>
<li><a class="reference internal" href="#id4">1.5.3. 用于稀疏数据的SGD</a></li>
<li><a class="reference internal" href="#id5">1.5.4. 复杂度</a></li>
<li><a class="reference internal" href="#id6">1.5.5. 停止准则</a></li>
<li><a class="reference internal" href="#id7">1.5.6. 实用小建议</a></li>
<li><a class="reference internal" href="#sgd-mathematical-formulation">1.5.7. 数学表达式</a><ul>
<li><a class="reference internal" href="#id9">1.5.7.1. SGD</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id10">1.5.8. 实现细节</a></li>
</ul>
</li>
</ul>

        <br />
        <p>
          <a href="http://www.studyai.com" target="_blank">
            <img src="../_static/img/xxx.png" alt="座右铭" />
          </a>
        </p>
        <br />
        <p class="doc-version" style="font-size:10%">
          注意!本网站的网址是以 <em>https://</em> 开头的，而不是以 <em>http://</em> 开头的!!!
        </p>
      </div>
    </div>
    
    <input type="checkbox" id="nav-trigger" class="nav-trigger" checked />
    <label for="nav-trigger"></label>
    
    


    <div class="content">
      
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="stochastic-gradient-descent">
<span id="sgd"></span><h1>1.5. 随机梯度下降(Stochastic Gradient Descent)<a class="headerlink" href="#stochastic-gradient-descent" title="Permalink to this headline">¶</a></h1>
<p><strong>Stochastic Gradient Descent (SGD)</strong> 是一种简单但又非常高效的方法，主要用于凸损失函数下线性分类器的判别式学习，
例如(线性) <a class="reference external" href="https://en.wikipedia.org/wiki/Support_vector_machine">支持向量机</a> 和
<a class="reference external" href="https://en.wikipedia.org/wiki/Logistic_regression">Logistic 回归</a> 。
尽管 SGD 在机器学习社区已经存在了很长时间, 但是最近在 large-scale learning （大规模学习）方面 SGD 获得了相当大的关注。</p>
<p>SGD 已成功应用于在文本分类和自然语言处理中经常遇到的大规模和稀疏的机器学习问题。
对于稀疏数据，本模块的分类器可以轻易的处理超过 10^5 的训练样本和超过 10^5 的特征。</p>
<p>随机梯度下降法 的优势:</p>
<blockquote>
<div><ul class="simple">
<li>高效。</li>
<li>易于实现 (有大量优化代码的机会)。</li>
</ul>
</div></blockquote>
<p>随机梯度下降法的劣势:</p>
<blockquote>
<div><ul class="simple">
<li>SGD 需要一些超参数，例如 regularization （正则化）参数和 number of iterations （迭代次数）。</li>
<li>SGD 对 feature scaling （特征缩放）敏感。</li>
</ul>
</div></blockquote>
<div class="section" id="id2">
<h2>1.5.1. 分类<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">在拟合模型前，确保你重新排列了（打乱）)你的训练数据，或者在每次迭代后用 <code class="docutils literal notranslate"><span class="pre">shuffle=True</span></code> 来打乱。</p>
</div>
<p><a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a> 类实现了一个简单的随机梯度下降学习例程, 支持不同的 loss functions（损失函数）和 penalties for classification（分类惩罚）。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_sgd_separating_hyperplane.html"><img alt="../_images/sphx_glr_plot_sgd_separating_hyperplane_0011.png" src="../_images/sphx_glr_plot_sgd_separating_hyperplane_0011.png" style="width: 480.0px; height: 360.0px;" /></a>
</div>
<p>与其他分类器一样，拟合 SGD 我们需要两个 array （数组）：保存训练样本的 size 为 <code class="docutils literal notranslate"><span class="pre">[n_samples,</span> <span class="pre">n_features]</span></code> 的数组 X 以及保存训练样本目标值（类标签）
的 size 为 <code class="docutils literal notranslate"><span class="pre">[n_samples]</span></code> 的数组 Y</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">SGDClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;hinge&quot;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>   
<span class="go">SGDClassifier(alpha=0.0001, average=False, class_weight=None,</span>
<span class="go">           early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,</span>
<span class="go">           l1_ratio=0.15, learning_rate=&#39;optimal&#39;, loss=&#39;hinge&#39;, max_iter=5,</span>
<span class="go">           n_iter=None, n_iter_no_change=5, n_jobs=None, penalty=&#39;l2&#39;,</span>
<span class="go">           power_t=0.5, random_state=None, shuffle=True, tol=None,</span>
<span class="go">           validation_fraction=0.1, verbose=0, warm_start=False)</span>
</pre></div>
</div>
<p>当模型拟合好以后，我们就可以用它对新的观测向量作出预测:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>
<span class="go">array([1])</span>
</pre></div>
</div>
<p>SGD 会在训练数据上拟合出一个线性模型。 类成员 <code class="docutils literal notranslate"><span class="pre">coef_</span></code> 里面保存着学习到的模型参数(线性模型的系数)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span>                                         
<span class="go">array([[9.9..., 9.9...]])</span>
</pre></div>
</div>
<p>类成员 <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> 保存着学习到的线性模型的截距(intercept) (也叫 偏置(offset or bias)):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span>                                    
<span class="go">array([-9.9...])</span>
</pre></div>
</div>
<p>模型是否要用到截距(intercept), i.e. 一个有偏置的超平面(a biased hyperplane), 可以通过参数 <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> 进行控制。</p>
<p>如果要获得某个样本点到超平面的有正负的距离 ， 可以使用 决策函数 <a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.decision_function" title="sklearn.linear_model.SGDClassifier.decision_function"><code class="xref py py-meth docutils literal notranslate"><span class="pre">SGDClassifier.decision_function</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">decision_function</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>                 
<span class="go">array([29.6...])</span>
</pre></div>
</div>
<p>模型用到的具体的损失函数可以通过参数 <code class="docutils literal notranslate"><span class="pre">loss</span></code> 进行设置。 <a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a> 类支持以下的损失函数:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">loss=&quot;hinge&quot;</span></code>: (soft-margin) linear Support Vector Machine,</li>
<li><code class="docutils literal notranslate"><span class="pre">loss=&quot;modified_huber&quot;</span></code>: smoothed hinge loss,</li>
<li><code class="docutils literal notranslate"><span class="pre">loss=&quot;log&quot;</span></code>: logistic regression,</li>
<li>and all regression losses below.</li>
</ul>
</div></blockquote>
<p>前两个损失函数是懒惰的(lazy)，只有当某个/些样本点违反了边界约束(margin constraint），它们才会去更新模型的参数, 这使得训练非常有效率,
即使使用了 L2 penalty（惩罚）我们仍然可能得到稀疏的模型。</p>
<p>使用 <code class="docutils literal notranslate"><span class="pre">loss=&quot;log&quot;</span></code> 或 <code class="docutils literal notranslate"><span class="pre">loss=&quot;modified_huber&quot;</span></code> 可以激活 <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> 方法,
该方法会为每个样本 <span class="math notranslate nohighlight">\(x\)</span> 估计出它属于每个类的概率向量 <span class="math notranslate nohighlight">\(P(y|x)\)</span> （译者注：也就是给定一个样本以后预测这个样本属于每个类的概率）</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;log&quot;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span>                      
<span class="go">array([[0.00..., 0.99...]])</span>
</pre></div>
</div>
<p>具体的惩罚项可以通过参数 <code class="docutils literal notranslate"><span class="pre">penalty</span></code> 设置, SGD 支持下列的惩罚项:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">penalty=&quot;l2&quot;</span></code>: L2 norm penalty on <code class="docutils literal notranslate"><span class="pre">coef_</span></code>.</li>
<li><code class="docutils literal notranslate"><span class="pre">penalty=&quot;l1&quot;</span></code>: L1 norm penalty on <code class="docutils literal notranslate"><span class="pre">coef_</span></code>.</li>
<li><code class="docutils literal notranslate"><span class="pre">penalty=&quot;elasticnet&quot;</span></code>:  L2 和 L1 的 凸组合(Convex combination): <code class="docutils literal notranslate"><span class="pre">(1</span> <span class="pre">-</span> <span class="pre">l1_ratio)</span> <span class="pre">*</span> <span class="pre">L2</span> <span class="pre">+</span> <span class="pre">l1_ratio</span> <span class="pre">*</span> <span class="pre">L1</span></code>.</li>
</ul>
</div></blockquote>
<p>默认的设置是 <code class="docutils literal notranslate"><span class="pre">penalty=&quot;l2&quot;</span></code>。 L1 penalty 导致稀疏解，使得大多数系数为零。 弹性网（Elastic Net）解决了在特征高相关时
L1 penalty 的一些不足。 参数 <code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code> 用来控制 L1 penalty 和 L2 penalty 的凸组合。</p>
<p><a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a> 通过利用 “one versus all” (OVA) 机制来组合多个二分类器，从而实现多分类。
对于每一个 <span class="math notranslate nohighlight">\(K\)</span> 类, 可以训练一个二分类器来区分自身和其他 <span class="math notranslate nohighlight">\(K-1\)</span> 个类。在测试阶段，
我们计算每个分类器的 confidence score（置信度分数）（也就是与超平面的距离），并选择置信度最高的分类。
下图阐释了基于 iris（鸢尾花）数据集上的 OVA 方法。虚线表示三个 OVA 分类器; 不同背景色代表由三个分类器产生的决策面。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_sgd_iris.html"><img alt="../_images/sphx_glr_plot_sgd_iris_0011.png" src="../_images/sphx_glr_plot_sgd_iris_0011.png" style="width: 480.0px; height: 360.0px;" /></a>
</div>
<p>在 multi-class classification的情况下， <code class="docutils literal notranslate"><span class="pre">coef_</span></code> 是 <code class="docutils literal notranslate"><span class="pre">shape=[n_classes,</span> <span class="pre">n_features]</span></code> 的一个二维数组，
<code class="docutils literal notranslate"><span class="pre">intercept_</span></code> 是 <code class="docutils literal notranslate"><span class="pre">shape=[n_classes]</span></code> 的一个一维数组。 <code class="docutils literal notranslate"><span class="pre">coef_</span></code> 的第 i 行保存了第 i 类的 OVA 分类器的权重向量；
类以升序索引 （参照属性 <code class="docutils literal notranslate"><span class="pre">classes_</span></code> ）。 注意，原则上，由于它们允许创建一个概率模型，所以 <code class="docutils literal notranslate"><span class="pre">loss=&quot;log&quot;</span></code> 和 <code class="docutils literal notranslate"><span class="pre">loss=&quot;modified_huber&quot;</span></code>
更适合于 one-vs-all 分类。</p>
<p><a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a> 类通过参数 <code class="docutils literal notranslate"><span class="pre">class_weight</span></code> 和 <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> 来支持 加权类(weighted classes) 和 加权实例(weighted instances)。
更多信息请参照下面的示例和 <a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.fit" title="sklearn.linear_model.SGDClassifier.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">SGDClassifier.fit</span></code></a> 的文档。</p>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/linear_model/plot_sgd_separating_hyperplane.html#sphx-glr-auto-examples-linear-model-plot-sgd-separating-hyperplane-py"><span class="std std-ref">SGD: Maximum margin separating hyperplane</span></a>,</li>
<li><a class="reference internal" href="../auto_examples/linear_model/plot_sgd_iris.html#sphx-glr-auto-examples-linear-model-plot-sgd-iris-py"><span class="std std-ref">Plot multi-class SGD on the iris dataset</span></a></li>
<li><a class="reference internal" href="../auto_examples/linear_model/plot_sgd_weighted_samples.html#sphx-glr-auto-examples-linear-model-plot-sgd-weighted-samples-py"><span class="std std-ref">SGD: Weighted samples</span></a></li>
<li><a class="reference internal" href="../auto_examples/linear_model/plot_sgd_comparison.html#sphx-glr-auto-examples-linear-model-plot-sgd-comparison-py"><span class="std std-ref">Comparing various online solvers</span></a></li>
<li><a class="reference internal" href="../auto_examples/svm/plot_separating_hyperplane_unbalanced.html#sphx-glr-auto-examples-svm-plot-separating-hyperplane-unbalanced-py"><span class="std std-ref">SVM: Separating hyperplane for unbalanced classes</span></a> (See the <cite>Note</cite>)</li>
</ul>
</div>
<p><a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a> 支持 averaged SGD (ASGD)。均值化(Averaging)可以通过设置 <code class="docutils literal notranslate"><span class="pre">average=True</span></code> 来启用。
ASGD 工作原理是在普通 SGD 的基础上，对每个样本的每次迭代后的系数取均值。
当使用 ASGD 时，学习速率可以更大甚至是恒定，在一些数据集上能够加速训练过程。</p>
<p>对于使用 logistic loss 的分类，在 <a class="reference internal" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a> 中提供了另一个采取
averaging strategy（平均策略）的 SGD 变体，其使用了随机平均梯度 (SAG) 算法。</p>
</div>
<div class="section" id="id3">
<h2>1.5.2. 回归<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDRegressor</span></code></a> 类实现了一个简单的随机梯度下降学习例程，它支持用不同的损失函数和惩罚来拟合线性回归模型。
<a class="reference internal" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDRegressor</span></code></a> 非常适用于有大量训练样本（&gt;10.000)的回归问题，对于其他问题，我们推荐使用 <a class="reference internal" href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-class docutils literal notranslate"><span class="pre">Ridge</span></code></a>,
<a class="reference internal" href="generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code class="xref py py-class docutils literal notranslate"><span class="pre">Lasso</span></code></a>, 或者 <a class="reference internal" href="generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet" title="sklearn.linear_model.ElasticNet"><code class="xref py py-class docutils literal notranslate"><span class="pre">ElasticNet</span></code></a> 。</p>
<p>具体的损失函数可以通过 <code class="docutils literal notranslate"><span class="pre">loss</span></code> 参数设置。 <a class="reference internal" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDRegressor</span></code></a> 支持以下的损失函数:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">loss=&quot;squared_loss&quot;</span></code>: Ordinary least squares,</li>
<li><code class="docutils literal notranslate"><span class="pre">loss=&quot;huber&quot;</span></code>: Huber loss for robust regression,</li>
<li><code class="docutils literal notranslate"><span class="pre">loss=&quot;epsilon_insensitive&quot;</span></code>: linear Support Vector Regression.</li>
</ul>
</div></blockquote>
<p>Huber 和 epsilon-insensitive 损失函数可用于鲁棒回归(robust regression)。
不敏感区域的宽度必须通过参数 <code class="docutils literal notranslate"><span class="pre">epsilon</span></code> 来设定。这个参数取决于目标变量的尺度(scale)。</p>
<p><a class="reference internal" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDRegressor</span></code></a> 支持 ASGD（平均随机梯度下降）,就像 <a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a> 一样。 均值化可以通过设置 <code class="docutils literal notranslate"><span class="pre">average=True</span></code> 来启用。</p>
<p>对于利用了平方损失(squared loss) 和 L2 惩罚 的回归算法，在 <a class="reference internal" href="generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-class docutils literal notranslate"><span class="pre">Ridge</span></code></a> 中提供了另一个采取平均策略(averaging strategy)的 SGD 变体，
其使用了随机平均梯度 (SAG) 算法。</p>
</div>
<div class="section" id="id4">
<h2>1.5.3. 用于稀疏数据的SGD<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">由于对截距(intercept)使用了收缩的学习速率，导致稀疏实现(sparse implementation)与密集实现(dense implementation)相比产生的结果略有不同。</p>
</div>
<p>在 <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/sparse.html">scipy.sparse</a> 支持的格式中，任意的矩阵都有对稀疏数据的内置支持方法。
但是，为了获得最高的效率，请使用 <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html">scipy.sparse.csr_matrix</a>
中定义的 CSR 矩阵格式。</p>
<div class="topic">
<p class="topic-title first">案列:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py"><span class="std std-ref">Classification of text documents using sparse features</span></a></li>
</ul>
</div>
</div>
<div class="section" id="id5">
<h2>1.5.4. 复杂度<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<p>SGD 主要的优势在于它的高效性，对于不同规模的训练样本，处理复杂度基本上是线性的。
假如 X 是 size 为 (n, p) 的矩阵，训练成本为 <span class="math notranslate nohighlight">\(O(k n \bar p)\)</span>，其中 k 是迭代次数， <span class="math notranslate nohighlight">\(\bar p\)</span> 是每个样本非零特征的平均数。</p>
<p>但是，最近的理论结果表明，得到我们期望的优化精度的运行时间并不会随着训练集规模扩大而增加。</p>
</div>
<div class="section" id="id6">
<h2>1.5.5. 停止准则<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<p>当给定的收敛水平达到以后，<a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a> 和 <a class="reference internal" href="generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDRegressor</span></code></a> 提供了两种停止准则:</p>
<blockquote>
<div><ul class="simple">
<li>如果设定了 <code class="docutils literal notranslate"><span class="pre">early_stopping=True</span></code>, 输入数据会被划分成训练集和验证集。模型首先在训练集上拟合，
然后停止准则是基于在验证集上计算出的预测得分进行的。验证集的大小可以由参数 <code class="docutils literal notranslate"><span class="pre">validation_fraction</span></code> 来修改。</li>
<li>如果设定了 <code class="docutils literal notranslate"><span class="pre">early_stopping=False</span></code>, 模型将会在整个输入数据集上进行拟合，此时的停止准则是基于在输入数据上计算出的目标函数。</li>
</ul>
</div></blockquote>
<p>在上述两种情形下，停止准则会在每一轮(epoch)中评估一次。并且当 “准则不再改进” 这一事件发生了 <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> 次时，该算法就会停止。
“准则的改进” 使用一个容忍度参数(tolerance <code class="docutils literal notranslate"><span class="pre">tol</span></code>)进行评估。最后，无论何种情况下，只要最大迭代次数 <code class="docutils literal notranslate"><span class="pre">max_iter</span></code> 达到以后，算法都会停止。</p>
</div>
<div class="section" id="id7">
<h2>1.5.6. 实用小建议<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul>
<li><p class="first">随机梯度下降法对特征缩放(feature scaling)很敏感，因此 <strong>强烈建议您缩放您的数据</strong> 。
例如，将输入向量 X 上的每个特征缩放到 [0,1] 或 [- 1，+1]， 或将其标准化，使其均值为 0，方差为 1。
请注意，必须将 <strong>相同</strong> 的缩放应用于对应的测试向量中，以获得有意义的结果。使用 <code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code> 很容易做到这一点</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">StandardScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>  <span class="c1"># Don&#39;t cheat - fit only on training data</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>  <span class="c1"># apply same transformation to test data</span>
</pre></div>
</div>
<p>假如你的 attributes （属性）有一个固有尺度（例如 word frequencies （词频）或 indicator features（指标型特征））就不需要缩放。</p>
</li>
<li><p class="first">最好使用 <code class="xref py py-class docutils literal notranslate"><span class="pre">GridSearchCV</span></code> 找到一个合理的正则化项(regularization term): <span class="math notranslate nohighlight">\(\alpha\)</span>  ， 它的范围通常在 <code class="docutils literal notranslate"><span class="pre">10.0**-np.arange(1,7)</span></code>。</p>
</li>
<li><p class="first">经验表明，SGD 在处理约 10^6 训练样本后基本收敛。因此，对于迭代次数第一个合理的猜想是 <code class="docutils literal notranslate"><span class="pre">max_iter</span> <span class="pre">=</span> <span class="pre">np.ceil(10**6</span> <span class="pre">/</span> <span class="pre">n)</span></code>，其中 <code class="docutils literal notranslate"><span class="pre">n</span></code> 是训练集的大小。</p>
</li>
<li><p class="first">如过将 SGD 应用于使用 PCA 做特征提取得到的数据上，我们发现通过某个常数 <cite>c</cite> 来缩放特征值是明智的，这样可以使训练数据的 L2 norm 平均值为 1。</p>
</li>
<li><p class="first">我们发现，当特征很多或 eta0 很大时， ASGD（平均随机梯度下降） 效果更好。</p>
</li>
</ul>
</div></blockquote>
<div class="topic">
<p class="topic-title first">案例:</p>
<ul class="simple">
<li><a class="reference external" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">“Efficient BackProp”</a>
Y. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: Tricks
of the Trade 1998.</li>
</ul>
</div>
</div>
<div class="section" id="sgd-mathematical-formulation">
<span id="id8"></span><h2>1.5.7. 数学表达式<a class="headerlink" href="#sgd-mathematical-formulation" title="Permalink to this headline">¶</a></h2>
<p>给定一组训练样本 <span class="math notranslate nohighlight">\((x_1, y_1), \ldots, (x_n, y_n)\)</span> 其中 <span class="math notranslate nohighlight">\(x_i \in \mathbf{R}^m\)</span> 并且 <span class="math notranslate nohighlight">\(y_i \in \{-1,1\}\)</span>,
我们的目标是学习一个线性评分函数(linear scoring function):  <span class="math notranslate nohighlight">\(f(x) = w^T x + b\)</span> ； <span class="math notranslate nohighlight">\(w \in \mathbf{R}^m\)</span> 是待学习的模型参数，
<span class="math notranslate nohighlight">\(b \in \mathbf{R}\)</span> 是待学习的截距。在做预测的时候，我们只需要简单的判断一下 <span class="math notranslate nohighlight">\(f(x)\)</span> 的正负符号。
寻找模型参数的通常做法就是 最小化一个带有正则化项的训练误差，如下：</p>
<div class="math notranslate nohighlight">
\[E(w,b) = \frac{1}{n}\sum_{i=1}^{n} L(y_i, f(x_i)) + \alpha R(w)\]</div>
<p>其中 <span class="math notranslate nohighlight">\(L\)</span> 度量 模型拟合程度的损失函数，<span class="math notranslate nohighlight">\(R\)</span> 是惩罚模型复杂度的正则化项（也叫作惩罚）; <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span> 是一个非负超参数。</p>
<p><span class="math notranslate nohighlight">\(L\)</span> 的不同选择会产生本质上不同的分类器，如下所示：</p>
<blockquote>
<div><ul class="simple">
<li>Hinge: (soft-margin) Support Vector Machines.</li>
<li>Log:   Logistic Regression.</li>
<li>Least-Squares: Ridge Regression.</li>
<li>Epsilon-Insensitive: (soft-margin) Support Vector Regression.</li>
</ul>
</div></blockquote>
<p>所有上述损失函数可以看作是错误分类误差（Zero-one loss即0-1损失）的上限，如下图所示：</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_sgd_loss_functions.html"><img alt="../_images/sphx_glr_plot_sgd_loss_functions_0011.png" src="../_images/sphx_glr_plot_sgd_loss_functions_0011.png" style="width: 480.0px; height: 360.0px;" /></a>
</div>
<p>比较流行的正则化项 <span class="math notranslate nohighlight">\(R\)</span> 包括：</p>
<blockquote>
<div><ul class="simple">
<li>L2 norm: <span class="math notranslate nohighlight">\(R(w) := \frac{1}{2} \sum_{i=1}^{n} w_i^2\)</span>,</li>
<li>L1 norm: <span class="math notranslate nohighlight">\(R(w) := \sum_{i=1}^{n} |w_i|\)</span>, 会产生稀疏解，</li>
<li>Elastic Net: <span class="math notranslate nohighlight">\(R(w) := \frac{\rho}{2} \sum_{i=1}^{n} w_i^2 + (1-\rho) \sum_{i=1}^{n} |w_i|\)</span>, L2 和 L1 的凸组合, 其中 <span class="math notranslate nohighlight">\(\rho\)</span> is given by <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">-</span> <span class="pre">l1_ratio</span></code>.</li>
</ul>
</div></blockquote>
<p>下图显示当 <span class="math notranslate nohighlight">\(R(w) = 1\)</span> 时参数空间中不同正则项的轮廓。</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/linear_model/plot_sgd_penalties.html"><img alt="../_images/sphx_glr_plot_sgd_penalties_0011.png" src="../_images/sphx_glr_plot_sgd_penalties_0011.png" style="width: 750.0px; height: 750.0px;" /></a>
</div>
<div class="section" id="id9">
<h3>1.5.7.1. SGD<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p>随机梯度下降法是一种无约束优化问题的优化方法。与（批量）梯度下降法相反，SGD 通过一次只考虑单个训练样本来近似 <span class="math notranslate nohighlight">\(E(w,b)\)</span> 的真实梯度。</p>
<p><a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a> 类实现了一个一阶 SGD 学习程序(first-order SGD learning routine)。
算法在训练样本上遍历，并且对每个样本根据由以下式子给出的更新规则来更新模型参数:</p>
<div class="math notranslate nohighlight">
\[w \leftarrow w - \eta (\alpha \frac{\partial R(w)}{\partial w}
+ \frac{\partial L(w^T x_i + b, y_i)}{\partial w})\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\eta\)</span> 是在参数空间中控制步长的学习速率(learning rate)； 截距(intercept) <span class="math notranslate nohighlight">\(b\)</span> 的更新方式与权重系数的更新方式类似但不需要正则化。</p>
<p>学习率 <span class="math notranslate nohighlight">\(\eta\)</span> 可以恒定或者逐渐减小。对于分类来说， 默认的学习率设定方案 （<code class="docutils literal notranslate"><span class="pre">learning_rate='optimal'</span></code>）由下式给出：</p>
<div class="math notranslate nohighlight">
\[\eta^{(t)} = \frac {1}{\alpha  (t_0 + t)}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(t\)</span> 是时间步（总共有 <cite>n_samples * n_iter</cite> 个时间步）， <span class="math notranslate nohighlight">\(t_0\)</span> 是由 Léon Bottou 提出的启发式算法决定的，
这样的话预期的初始更新与预期的权重大小可以保持相当(comparable)（这里假定了训练样本的范数近似为1）。
在 <code class="xref py py-class docutils literal notranslate"><span class="pre">BaseSGD</span></code> 中的 <code class="docutils literal notranslate"><span class="pre">_init_t</span></code> 中可以找到确切的定义。</p>
<p>对于回归来说，默认的学习率是逆向缩放(inverse scaling) (<code class="docutils literal notranslate"><span class="pre">learning_rate='invscaling'</span></code>)，由下式给出:</p>
<div class="math notranslate nohighlight">
\[\eta^{(t)} = \frac{eta_0}{t^{power\_t}}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(eta_0\)</span> 和 <span class="math notranslate nohighlight">\(power\_t\)</span> 是用户通过 <code class="docutils literal notranslate"><span class="pre">eta0</span></code> 和 <code class="docutils literal notranslate"><span class="pre">power_t</span></code> 分别选择的超参数。</p>
<p>如果使用固定的学习速率则设置 <code class="docutils literal notranslate"><span class="pre">learning_rate='constant'</span></code> ，或者设置 <code class="docutils literal notranslate"><span class="pre">eta0</span></code> 来指定学习速率。</p>
<p>如果要使用自适应下降的学习率, 则设置 <code class="docutils literal notranslate"><span class="pre">learning_rate='adaptive'</span></code>，并使用 <code class="docutils literal notranslate"><span class="pre">eta0</span></code> 指定一个起始学习率。
当停止准则达到以后，学习率会被除以5并且算法不会立刻停止。自适应情况下，算法只有在学习率降低到1e-6时才会停止。</p>
<p>模型参数可以通过成员 <code class="docutils literal notranslate"><span class="pre">coef_</span></code> 和 <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> 来获得：</p>
<blockquote>
<div><ul class="simple">
<li>Member <code class="docutils literal notranslate"><span class="pre">coef_</span></code> holds the weights <span class="math notranslate nohighlight">\(w\)</span></li>
<li>Member <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> holds <span class="math notranslate nohighlight">\(b\)</span></li>
</ul>
</div></blockquote>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.58.7377">“Solving large scale linear prediction problems using stochastic
gradient descent algorithms”</a>
T. Zhang - In Proceedings of ICML ‘04.</li>
<li><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.124.4696">“Regularization and variable selection via the elastic net”</a>
H. Zou, T. Hastie - Journal of the Royal Statistical Society Series B,
67 (2), 301-320.</li>
<li><a class="reference external" href="https://arxiv.org/pdf/1107.2490v2.pdf">“Towards Optimal One Pass Large Scale Learning with
Averaged Stochastic Gradient Descent”</a>
Xu, Wei</li>
</ul>
</div>
</div>
</div>
<div class="section" id="id10">
<h2>1.5.8. 实现细节<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h2>
<p>SGD 的实现受到了 Léon Bottou <a class="reference external" href="https://leon.bottou.org/projects/sgd">Stochastic Gradient SVM</a> 的影响。
类似于 SvmSGD，权重向量表达为一个标量和一个向量的内积，这样保证在使用L2正则项时可以高效更新权重。
在特征向量稀疏的情况下， intercept （截距）是以更小的学习率（乘以 0.01）更新的，这导致了它的更新更加频繁。
训练样本按顺序选取并且每处理一个样本就要降低学习速率。我们采用了 Shalev-Shwartz 等人2007年提出的的学习速率设定方案。
对于多类分类，我们使用了 “one versus all” 方法。 我们在 L1 正则化（和 Elastic Net ）中使用 Tsuruoka 等人2009年提出的
truncated gradient algorithm （截断梯度算法）。代码是用 Cython 编写的。</p>
<div class="topic">
<p class="topic-title first">参考文献:</p>
<ul class="simple">
<li><a class="reference external" href="https://leon.bottou.org/projects/sgd">“Stochastic Gradient Descent”</a> L. Bottou - Website, 2010.</li>
<li><a class="reference external" href="https://leon.bottou.org/slides/largescale/lstut.pdf">“The Tradeoffs of Large Scale Machine Learning”</a> L. Bottou - Website, 2011.</li>
<li><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.74.8513">“Pegasos: Primal estimated sub-gradient solver for svm”</a>
S. Shalev-Shwartz, Y. Singer, N. Srebro - In Proceedings of ICML ‘07.</li>
<li><a class="reference external" href="https://www.aclweb.org/anthology/P/P09/P09-1054.pdf">“Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty”</a>
Y. Tsuruoka, J. Tsujii, S. Ananiadou -  In Proceedings of the AFNLP/ACL ‘09.</li>
</ul>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
  </div>
  
  <div class="footer">
    &copy; 2007 - 2018, scikit-learn developers (BSD License).
    <!--
    <a href="../_sources/modules/sgd.rst.txt" rel="nofollow">Show this page source</a> -->
  </div>
  <div class="rel">
    
      <div class="buttonPrevious">
        <a href="svm.html">Previous
        </a>
      </div>
      <div class="buttonNext">
        <a href="neighbors.html">Next
        </a>
      </div>
      
    </div>

    
    <script>
      window.ga = window.ga || function () { (ga.q = ga.q || []).push(arguments) }; ga.l = +new Date;
      ga('create', 'UA-22606712-2', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    
    <script>
      (function () {
        var cx = '016639176250731907682:tjtqbvtvij0';
        var gcse = document.createElement('script'); gcse.type = 'text/javascript'; gcse.async = true;
        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s);
      })();
    </script>
  </body>
</html>