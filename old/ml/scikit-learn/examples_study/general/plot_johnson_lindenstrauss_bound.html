<!DOCTYPE html>
<html lang="zh-cn">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <link rel="stylesheet" href="../../../../css-fonts-js/css/bootstrap.min.css" />
    <link rel="stylesheet" href="../../../../css-fonts-js/css/bootstrap-responsive.css" />
    <link rel="stylesheet" href="../../../../css-fonts-js/css/nature.css" />
    <link rel="stylesheet" href="../../../../css-fonts-js/css/pygments.css" />
    <link rel="stylesheet" href="../../../../css-fonts-js/css/gallery.css" />
    <style>p{font-size:large}</style>
    <script type="text/javascript" src="../../../../css-fonts-js/js/jquery.min.js"></script>
    <script type="text/javascript" src="../../../../css-fonts-js/js/underscore.js"></script>
    <script type="text/javascript" src="../../../../css-fonts-js/js/doctools.js"></script>
    <script type="text/javascript" src="../../../../css-fonts-js/js/copybutton.js"></script>
    <script type="text/javascript" src="../../../../css-fonts-js/js/bootstrap.min.js"></script>

    <title>人工智能研究网/机器学习</title>
</head>


<body>

    <nav class="navbar navbar-inverse">
        <div class="container">
            <div class="navbar-header">
                <a class="navbar-brand" href="#">studyai.cn</a>
            </div>
            <ul class="nav navbar-nav">
                <li class="active"><a href="../../../../index.html">网站主页</a></li>
                <li><a href="../../../../../webmaster.html">站长风采</a></li> 
                <li><a href="../../../../../sponsor.html">赞助我们</a></li>
                <li><a href="../../../index.html">机器学习主页</a></li>
            </ul> 
        </div>
    </nav>

    <div class="container">

        <div class="section" id="the-johnson-lindenstrauss-bound-for-embedding-with-random-projections">
            <span id="example-plot-johnson-lindenstrauss-bound-py"></span><h1>The Johnson-Lindenstrauss bound for embedding with random projections<a class="headerlink" href="#the-johnson-lindenstrauss-bound-for-embedding-with-random-projections" title="Permalink to this headline">¶</a></h1>
            <p>
                The <a class="reference external" href="http://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma">Johnson-Lindenstrauss lemma</a> states that any high dimensional
                dataset can be randomly projected into a lower dimensional Euclidean
                space while controlling the distortion in the pairwise distances.
            </p>
            <div class="section" id="theoretical-bounds">
                <h2>Theoretical bounds<a class="headerlink" href="#theoretical-bounds" title="Permalink to this headline">¶</a></h2>
                <p>
                    The distortion introduced by a random projection <cite>p</cite> is asserted by
                    the fact that <cite>p</cite> is defining an eps-embedding with good probability
                    as defined by:
                </p>
                <div class="math">
                    <p><img src="./images/89dae7627a99d599a94ac757eb45c31b8597f493.png" alt="(1 - eps) \|u - v\|^2 &lt; \|p(u) - p(v)\|^2 &lt; (1 + eps) \|u - v\|^2" /></p>
                </div><p>
                    Where u and v are any rows taken from a dataset of shape [n_samples,
                    n_features] and p is a projection by a random Gaussian N(0, 1) matrix
                    with shape [n_components, n_features] (or a sparse Achlioptas matrix).
                </p>
                <p>
                    The minimum number of components to guarantees the eps-embedding is
                    given by:
                </p>
                <div class="math">
                    <p><img src="./images/8e616d45ff825ef696e70ed1394729f97dc155d3.png" alt="n\_components &gt;= 4 log(n\_samples) / (eps^2 / 2 - eps^3 / 3)" /></p>
                </div><p>
                    The first plot shows that with an increasing number of samples <code class="docutils literal"><span class="pre">n_samples</span></code>,
                    the minimal number of dimensions <code class="docutils literal"><span class="pre">n_components</span></code> increased logarithmically
                    in order to guarantee an <code class="docutils literal"><span class="pre">eps</span></code>-embedding.
                </p>
                <p>
                    The second plot shows that an increase of the admissible
                    distortion <code class="docutils literal"><span class="pre">eps</span></code> allows to reduce drastically the minimal number of
                    dimensions <code class="docutils literal"><span class="pre">n_components</span></code> for a given number of samples <code class="docutils literal"><span class="pre">n_samples</span></code>
                </p>
            </div>
            <div class="section" id="empirical-validation">
                <h2>Empirical validation<a class="headerlink" href="#empirical-validation" title="Permalink to this headline">¶</a></h2>
                <p>
                    We validate the above bounds on the the digits dataset or on the 20 newsgroups
                    text document (TF-IDF word frequencies) dataset:
                </p>
                <ul class="simple">
                    <li>
                        for the digits dataset, some 8x8 gray level pixels data for 500
                        handwritten digits pictures are randomly projected to spaces for various
                        larger number of dimensions <code class="docutils literal"><span class="pre">n_components</span></code>.
                    </li>
                    <li>
                        for the 20 newsgroups dataset some 500 documents with 100k
                        features in total are projected using a sparse random matrix to smaller
                        euclidean spaces with various values for the target number of dimensions
                        <code class="docutils literal"><span class="pre">n_components</span></code>.
                    </li>
                </ul>
                <p>
                    The default dataset is the digits dataset. To run the example on the twenty
                    newsgroups dataset, pass the &#8211;twenty-newsgroups command line argument to this
                    script.
                </p>
                <p>For each value of <code class="docutils literal"><span class="pre">n_components</span></code>, we plot:</p>
                <ul class="simple">
                    <li>
                        2D distribution of sample pairs with pairwise distances in original
                        and projected spaces as x and y axis respectively.
                    </li>
                    <li>1D histogram of the ratio of those distances (projected / original).</li>
                </ul>
                <p>
                    We can see that for low values of <code class="docutils literal"><span class="pre">n_components</span></code> the distribution is wide
                    with many distorted pairs and a skewed distribution (due to the hard
                    limit of zero ratio on the left as distances are always positives)
                    while for larger values of n_components the distortion is controlled
                    and the distances are well preserved by the random projection.
                </p>
            </div>
            <div class="section" id="remarks">
                <h2>Remarks<a class="headerlink" href="#remarks" title="Permalink to this headline">¶</a></h2>
                <p>
                    According to the JL lemma, projecting 500 samples without too much distortion
                    will require at least several thousands dimensions, irrespective of the
                    number of features of the original dataset.
                </p>
                <p>
                    Hence using random projections on the digits dataset which only has 64 features
                    in the input space does not make sense: it does not allow for dimensionality
                    reduction in this case.
                </p>
                <p>
                    On the twenty newsgroups on the other hand the dimensionality can be decreased
                    from 56436 down to 10000 while reasonably preserving pairwise distances.
                </p>
                <ul class="horizontal">
                    <li>
                        <a class="first reference internal image-reference" href="./images/plot_johnson_lindenstrauss_bound_001.png"><img alt="./images/plot_johnson_lindenstrauss_bound_001.png" src="./images/plot_johnson_lindenstrauss_bound_001.png" style="width: 376.0px; height: 282.0px;" /></a>
                    </li>
                    <li>
                        <a class="first reference internal image-reference" href="./images/plot_johnson_lindenstrauss_bound_002.png"><img alt="./images/plot_johnson_lindenstrauss_bound_002.png" src="./images/plot_johnson_lindenstrauss_bound_002.png" style="width: 376.0px; height: 282.0px;" /></a>
                    </li>
                    <li>
                        <a class="first reference internal image-reference" href="./images/plot_johnson_lindenstrauss_bound_003.png"><img alt="./images/plot_johnson_lindenstrauss_bound_003.png" src="./images/plot_johnson_lindenstrauss_bound_003.png" style="width: 376.0px; height: 282.0px;" /></a>
                    </li>
                    <li>
                        <a class="first reference internal image-reference" href="./images/plot_johnson_lindenstrauss_bound_004.png"><img alt="./images/plot_johnson_lindenstrauss_bound_004.png" src="./images/plot_johnson_lindenstrauss_bound_004.png" style="width: 376.0px; height: 282.0px;" /></a>
                    </li>
                    <li>
                        <a class="first reference internal image-reference" href="./images/plot_johnson_lindenstrauss_bound_005.png"><img alt="./images/plot_johnson_lindenstrauss_bound_005.png" src="./images/plot_johnson_lindenstrauss_bound_005.png" style="width: 376.0px; height: 282.0px;" /></a>
                    </li>
                    <li>
                        <a class="first reference internal image-reference" href="./images/plot_johnson_lindenstrauss_bound_006.png"><img alt="./images/plot_johnson_lindenstrauss_bound_006.png" src="./images/plot_johnson_lindenstrauss_bound_006.png" style="width: 376.0px; height: 282.0px;" /></a>
                    </li>
                    <li>
                        <a class="first reference internal image-reference" href="./images/plot_johnson_lindenstrauss_bound_007.png"><img alt="./images/plot_johnson_lindenstrauss_bound_007.png" src="./images/plot_johnson_lindenstrauss_bound_007.png" style="width: 376.0px; height: 282.0px;" /></a>
                    </li>
                    <li>
                        <a class="first reference internal image-reference" href="./images/plot_johnson_lindenstrauss_bound_008.png"><img alt="./images/plot_johnson_lindenstrauss_bound_008.png" src="./images/plot_johnson_lindenstrauss_bound_008.png" style="width: 376.0px; height: 282.0px;" /></a>
                    </li>
                </ul>
                <p><strong>Script output</strong>:</p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre>Embedding 500 samples with dim 64 using various random projections
Projected 500 samples from 64 to 300 in 0.021s
Random matrix with size: 0.030MB
Mean distances rate: 1.05 (0.11)
Projected 500 samples from 64 to 1000 in 0.065s
Random matrix with size: 0.096MB
Mean distances rate: 1.01 (0.05)
Projected 500 samples from 64 to 10000 in 0.624s
Random matrix with size: 0.957MB
Mean distances rate: 1.00 (0.01)
</pre>
                    </div>
                </div>
                <p><strong>Python source code:</strong> <a class="reference download internal" href="../_downloads/plot_johnson_lindenstrauss_bound.py"><code class="xref download docutils literal"><span class="pre">plot_johnson_lindenstrauss_bound.py</span></code></a></p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span class="k">print</span><span class="p">(</span><span class="n">__doc__</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.random_projection</span> <span class="kn">import</span> <a href="../modules/generated/sklearn.random_projection.johnson_lindenstrauss_min_dim.html#sklearn.random_projection.johnson_lindenstrauss_min_dim"><span class="n">johnson_lindenstrauss_min_dim</span></a>
<span class="kn">from</span> <span class="nn">sklearn.random_projection</span> <span class="kn">import</span> <a href="../modules/generated/sklearn.random_projection.SparseRandomProjection.html#sklearn.random_projection.SparseRandomProjection"><span class="n">SparseRandomProjection</span></a>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <a href="../modules/generated/sklearn.datasets.fetch_20newsgroups_vectorized.html#sklearn.datasets.fetch_20newsgroups_vectorized"><span class="n">fetch_20newsgroups_vectorized</span></a>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <a href="../modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits"><span class="n">load_digits</span></a>
<span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">euclidean_distances</span>
<span class="c1"># Part 1: plot the theoretical dependency between n_components_min and</span>
<span class="c1"># n_samples</span>
<span class="c1"># range of admissible distortions</span>
<span class="n">eps_range</span> <span class="o">=</span> <a href="http://docs.scipy.org/doc/numpy-1.6.0/reference/generated/numpy.linspace.html#numpy.linspace"><span class="n">np</span><span class="o">.</span><span class="n">linspace</span></a><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">colors</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">(</span><a href="http://docs.scipy.org/doc/numpy-1.6.0/reference/generated/numpy.linspace.html#numpy.linspace"><span class="n">np</span><span class="o">.</span><span class="n">linspace</span></a><span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">eps_range</span><span class="p">)))</span>
<span class="c1"># range of number of samples (observation) to embed</span>
<span class="n">n_samples_range</span> <span class="o">=</span> <a href="http://docs.scipy.org/doc/numpy-1.6.0/reference/generated/numpy.logspace.html#numpy.logspace"><span class="n">np</span><span class="o">.</span><span class="n">logspace</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<a href="http://matplotlib.org/api/figure_api.html#matplotlib.figure"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span></a><span class="p">()</span>
<span class="k">for</span> <span class="n">eps</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">eps_range</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
                        <span class="n">min_n_components</span> <span class="o">=</span> <a href="../modules/generated/sklearn.random_projection.johnson_lindenstrauss_min_dim.html#sklearn.random_projection.johnson_lindenstrauss_min_dim"><span class="n">johnson_lindenstrauss_min_dim</span></a><span class="p">(</span><span class="n">n_samples_range</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
                        <a href="http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.loglog"><span class="n">plt</span><span class="o">.</span><span class="n">loglog</span></a><span class="p">(</span><span class="n">n_samples_range</span><span class="p">,</span> <span class="n">min_n_components</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
<a href="http://matplotlib.org/api/legend_api.html#matplotlib.legend"><span class="n">plt</span><span class="o">.</span><span class="n">legend</span></a><span class="p">([</span><span class="s2">&quot;eps = </span><span class="si">%0.1f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">eps</span> <span class="k">for</span> <span class="n">eps</span> <span class="ow">in</span> <span class="n">eps_range</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">)</span>
<a href="http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.xlabel"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span></a><span class="p">(</span><span class="s2">&quot;Number of observations to eps-embed&quot;</span><span class="p">)</span>
<a href="http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.ylabel"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span></a><span class="p">(</span><span class="s2">&quot;Minimum number of dimensions&quot;</span><span class="p">)</span>
<a href="http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.title"><span class="n">plt</span><span class="o">.</span><span class="n">title</span></a><span class="p">(</span><span class="s2">&quot;Johnson-Lindenstrauss bounds:</span><span class="se">\n</span><span class="s2">n_samples vs n_components&quot;</span><span class="p">)</span>
<span class="c1"># range of admissible distortions</span>
<span class="n">eps_range</span> <span class="o">=</span> <a href="http://docs.scipy.org/doc/numpy-1.6.0/reference/generated/numpy.linspace.html#numpy.linspace"><span class="n">np</span><span class="o">.</span><span class="n">linspace</span></a><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="c1"># range of number of samples (observation) to embed</span>
<span class="n">n_samples_range</span> <span class="o">=</span> <a href="http://docs.scipy.org/doc/numpy-1.6.0/reference/generated/numpy.logspace.html#numpy.logspace"><span class="n">np</span><span class="o">.</span><span class="n">logspace</span></a><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">colors</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">(</span><a href="http://docs.scipy.org/doc/numpy-1.6.0/reference/generated/numpy.linspace.html#numpy.linspace"><span class="n">np</span><span class="o">.</span><span class="n">linspace</span></a><span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">n_samples_range</span><span class="p">)))</span>
<a href="http://matplotlib.org/api/figure_api.html#matplotlib.figure"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span></a><span class="p">()</span>
<span class="k">for</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">n_samples_range</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
                        <span class="n">min_n_components</span> <span class="o">=</span> <a href="../modules/generated/sklearn.random_projection.johnson_lindenstrauss_min_dim.html#sklearn.random_projection.johnson_lindenstrauss_min_dim"><span class="n">johnson_lindenstrauss_min_dim</span></a><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps_range</span><span class="p">)</span>
                        <a href="http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.semilogy"><span class="n">plt</span><span class="o">.</span><span class="n">semilogy</span></a><span class="p">(</span><span class="n">eps_range</span><span class="p">,</span> <span class="n">min_n_components</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
<a href="http://matplotlib.org/api/legend_api.html#matplotlib.legend"><span class="n">plt</span><span class="o">.</span><span class="n">legend</span></a><span class="p">([</span><span class="s2">&quot;n_samples = </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">n_samples_range</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>
<a href="http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.xlabel"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span></a><span class="p">(</span><span class="s2">&quot;Distortion eps&quot;</span><span class="p">)</span>
<a href="http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.ylabel"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span></a><span class="p">(</span><span class="s2">&quot;Minimum number of dimensions&quot;</span><span class="p">)</span>
<a href="http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.title"><span class="n">plt</span><span class="o">.</span><span class="n">title</span></a><span class="p">(</span><span class="s2">&quot;Johnson-Lindenstrauss bounds:</span><span class="se">\n</span><span class="s2">n_components vs eps&quot;</span><span class="p">)</span>
<span class="c1"># Part 2: perform sparse random projection of some digits images which are</span>
<span class="c1"># quite low dimensional and dense or documents of the 20 newsgroups dataset</span>
<span class="c1"># which is both high dimensional and sparse</span>
<span class="k">if</span> <span class="s1">&#39;--twenty-newsgroups&#39;</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">:</span>
                        <span class="c1"># Need an internet connection hence not enabled by default</span>
                        <span class="n">data</span> <span class="o">=</span> <a href="../modules/generated/sklearn.datasets.fetch_20newsgroups_vectorized.html#sklearn.datasets.fetch_20newsgroups_vectorized"><span class="n">fetch_20newsgroups_vectorized</span></a><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="mi">500</span><span class="p">]</span>
<span class="k">else</span><span class="p">:</span>
                        <span class="n">data</span> <span class="o">=</span> <a href="../modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits"><span class="n">load_digits</span></a><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="mi">500</span><span class="p">]</span>
<span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Embedding </span><span class="si">%d</span><span class="s2"> samples with dim </span><span class="si">%d</span><span class="s2"> using various random projections&quot;</span>
                        <span class="o">%</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">))</span>
<span class="n">n_components_range</span> <span class="o">=</span> <a href="http://docs.scipy.org/doc/numpy-1.6.0/reference/generated/numpy.array.html#numpy.array"><span class="n">np</span><span class="o">.</span><span class="n">array</span></a><span class="p">([</span><span class="mi">300</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">])</span>
<span class="n">dists</span> <span class="o">=</span> <span class="n">euclidean_distances</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="c1"># select only non-identical samples pairs</span>
<span class="n">nonzero</span> <span class="o">=</span> <span class="n">dists</span> <span class="o">!=</span> <span class="mi">0</span>
<span class="n">dists</span> <span class="o">=</span> <span class="n">dists</span><span class="p">[</span><span class="n">nonzero</span><span class="p">]</span>
<span class="k">for</span> <span class="n">n_components</span> <span class="ow">in</span> <span class="n">n_components_range</span><span class="p">:</span>
                        <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
                        <span class="n">rp</span> <span class="o">=</span> <a href="../modules/generated/sklearn.random_projection.SparseRandomProjection.html#sklearn.random_projection.SparseRandomProjection"><span class="n">SparseRandomProjection</span></a><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">)</span>
                        <span class="n">projected_data</span> <span class="o">=</span> <span class="n">rp</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
                        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Projected </span><span class="si">%d</span><span class="s2"> samples from </span><span class="si">%d</span><span class="s2"> to </span><span class="si">%d</span><span class="s2"> in </span><span class="si">%0.3f</span><span class="s2">s&quot;</span>
                        <span class="o">%</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">n_components</span><span class="p">,</span> <span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">))</span>
                        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">rp</span><span class="p">,</span> <span class="s1">&#39;components_&#39;</span><span class="p">):</span>
                        <span class="n">n_bytes</span> <span class="o">=</span> <span class="n">rp</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">nbytes</span>
                        <span class="n">n_bytes</span> <span class="o">+=</span> <span class="n">rp</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">nbytes</span>
                        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Random matrix with size: </span><span class="si">%0.3f</span><span class="s2">MB&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">n_bytes</span> <span class="o">/</span> <span class="mf">1e6</span><span class="p">))</span>
                        <span class="n">projected_dists</span> <span class="o">=</span> <span class="n">euclidean_distances</span><span class="p">(</span>
                        <span class="n">projected_data</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()[</span><span class="n">nonzero</span><span class="p">]</span>
                        <a href="http://matplotlib.org/api/figure_api.html#matplotlib.figure"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span></a><span class="p">()</span>
                        <a href="http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.hexbin"><span class="n">plt</span><span class="o">.</span><span class="n">hexbin</span></a><span class="p">(</span><span class="n">dists</span><span class="p">,</span> <span class="n">projected_dists</span><span class="p">,</span> <span class="n">gridsize</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">PuBu</span><span class="p">)</span>
                        <a href="http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.xlabel"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span></a><span class="p">(</span><span class="s2">&quot;Pairwise squared distances in original space&quot;</span><span class="p">)</span>
                        <a href="http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.ylabel"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span></a><span class="p">(</span><span class="s2">&quot;Pairwise squared distances in projected space&quot;</span><span class="p">)</span>
                        <a href="http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.title"><span class="n">plt</span><span class="o">.</span><span class="n">title</span></a><span class="p">(</span><span class="s2">&quot;Pairwise distances distribution for n_components=</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span>
                        <span class="n">n_components</span><span class="p">)</span>
                        <span class="n">cb</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
                        <span class="n">cb</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="s1">&#39;Sample pairs counts&#39;</span><span class="p">)</span>
                        <span class="n">rates</span> <span class="o">=</span> <span class="n">projected_dists</span> <span class="o">/</span> <span class="n">dists</span>
                        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Mean distances rate: </span><span class="si">%0.2f</span><span class="s2"> (</span><span class="si">%0.2f</span><span class="s2">)&quot;</span>
                        <span class="o">%</span> <span class="p">(</span><a href="http://docs.scipy.org/doc/numpy-1.6.0/reference/generated/numpy.mean.html#numpy.mean"><span class="n">np</span><span class="o">.</span><span class="n">mean</span></a><span class="p">(</span><span class="n">rates</span><span class="p">),</span> <a href="http://docs.scipy.org/doc/numpy-1.6.0/reference/generated/numpy.std.html#numpy.std"><span class="n">np</span><span class="o">.</span><span class="n">std</span></a><span class="p">(</span><span class="n">rates</span><span class="p">)))</span>
                        <a href="http://matplotlib.org/api/figure_api.html#matplotlib.figure"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span></a><span class="p">()</span>
                        <a href="http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.hist"><span class="n">plt</span><span class="o">.</span><span class="n">hist</span></a><span class="p">(</span><span class="n">rates</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">normed</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="nb">range</span><span class="o">=</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">))</span>
                        <a href="http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.xlabel"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span></a><span class="p">(</span><span class="s2">&quot;Squared distances rate: projected / original&quot;</span><span class="p">)</span>
                        <a href="http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.ylabel"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span></a><span class="p">(</span><span class="s2">&quot;Distribution of samples pairs&quot;</span><span class="p">)</span>
                        <a href="http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.title"><span class="n">plt</span><span class="o">.</span><span class="n">title</span></a><span class="p">(</span><span class="s2">&quot;Histogram of pairwise distance rates for n_components=</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span>
                        <span class="n">n_components</span><span class="p">)</span>
                        <span class="c1"># TODO: compute the expected value of eps and add them to the previous plot</span>
                        <span class="c1"># as vertical lines / region</span>
<a href="http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.show"><span class="n">plt</span><span class="o">.</span><span class="n">show</span></a><span class="p">()</span>
</pre>
                    </div>
                </div>
                <p>
                    <strong>Total running time of the example:</strong>  4.83 seconds
                    ( 0 minutes  4.83 seconds)
                </p>
            </div>
        </div>

    </div>


</body>
</html>


