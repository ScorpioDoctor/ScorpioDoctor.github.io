<!DOCTYPE html>
<html lang="zh-cn">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <link rel="stylesheet" href="../../../../css-fonts-js/css/bootstrap.min.css" />
    <!--<link rel="stylesheet" href="../../../../css-fonts-js/css/bootstrap-responsive.css" />
    <link rel="stylesheet" href="../../../../css-fonts-js/css/gallery.css" />
    <link rel="stylesheet" href="../../../../css-fonts-js/css/nature.css" />
    <link rel="stylesheet" href="../../../../css-fonts-js/css/pygments.css" />-->

    <style>
        p {
            font-size: large;
        }

        li {
            font-size: large;
        }
    </style>

    <script src="../../../../css-fonts-js/js/jquery.min.js"></script>
    <script src="../../../../css-fonts-js/js/jquery.js"></script>
    <!--<script src="../../../../css-fonts-js/js/copybutton.js"></script>
    <script src="../../../../css-fonts-js/js/doctools.js"></script>
    <script src="../../../../css-fonts-js/js/jquery.maphilight.js"></script>
    <script src="../../../../css-fonts-js/js/jquery.maphilight.min.js"></script>
    <script src="../../../../css-fonts-js/js/underscore.js"></script>-->
    <script src="../../../../css-fonts-js/js/bootstrap.min.js"></script>


    <title>人工智能研究网</title>
    <!--网站访问量统计-->
    <script>
        var _hmt = _hmt || [];
        (function () {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?e7836e37a4cb7584127a787e9b44e3f1";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>
</head>


<body>

    <nav class="navbar navbar-inverse">
        <div class="container">
            <div class="navbar-header">
                <a class="navbar-brand" href="../../../../index.html">studyai.cn</a>
            </div>
            <ul class="nav navbar-nav">
                <li class="active"><a href="../../../../index.html">网站主页</a></li>
                <li><a href="../../../../webmaster.html">站长风采</a></li>
                <li><a href="../../../../sponsor.html">赞助我们</a></li>
                <li><a href="../../../index.html">机器学习主页</a></li>
            </ul>
        </div>
    </nav>

    <div class="container">

        <div class="section" id="linear-and-quadratic-discriminant-analysis">
            <h1 class="page-header">1.2. 线性和二次判别分析（LDA 和 QDA）<a>¶</a></h1>
            <p>
                线性判别分析 <code>(discriminant_analysis.LinearDiscriminantAnalysis)</code> 与 二次判别分析 <code>(discriminant_analysis.QuadraticDiscriminantAnalysis)</code>
                 是两个经典的分类器。就像它们的名字所暗示的那样，LDA产生线性决策面，QDA产生二次决策面。
            </p>
            <p>
                这些分类器有相当的吸引力，因为它们具有易于计算的解析解(closed-form solutions)而且还是天
                生的多类分类器，已经被证明在实践中工作的很好而且没有需要小心调节的超参数。
            </p>
            <p class="text-center">
                <strong><a >
                    <img alt="ldaqda" src="./images/plot_lda_qda_0011.png" style="width: 640.0px; height: 480.0px;" />
                    </a></strong>
            </p>
            <p>
                上图展示了LDA和QDA的决策边界。此图通过对比说明了LDA只能学习到线性边界， QDA则可以学习到二次边界而且其边界也因此更有弹性。
            </p>
            <div class="well well-sm">
                <p >例子:</p>
                <p>
                    <a class="reference internal" href="../auto_examples/classification/plot_lda_qda.html#example-classification-plot-lda-qda-py">
                    <span>Linear and Quadratic Discriminant Analysis with confidence ellipsoid</span></a>
                    :  LDA和QDA在人工合成的数据集上的对比
                </p>
            </div>


            <div class="section" id="dimensionality-reduction-using-linear-discriminant-analysis">
                <h2>1.2.1. LDA 用于维数约简<a>¶</a></h2>
                <p>
                    <code>discriminant_analysis.LinearDiscriminantAnalysis</code> 可以被用于执行有监督的维数约简操作。
                   其原理是将输入数据投影到线性子空间中。该线性子空间是由能够最大化类间分离性的那些方向所张成
                    (在下面的数学表述小结中将更精确的讨论此问题)。 
                    输出的维数必需小于类的数量,所以此方法是一个普遍来说比较强的降维方法而且仅仅在多类问题中才有使用的意义。
                </p>
                <p>
                    <code>discriminant_analysis.LinearDiscriminantAnalysis.transform</code>函数实现了LDA维数约简。
                     我们想要的维数大小可以通过构造器参数<code>n_components</code>来设置。这一参数的设置不会影响
                    <code>discriminant_analysis.LinearDiscriminantAnalysis.fit</code>
                    和<code>discriminant_analysis.LinearDiscriminantAnalysis.predict</code>.
                </p>
                <div class="well well-sm">
                    <p>例子:</p>
                    <p>
                        <a class="reference internal">
                        <span>Comparison of LDA and PCA 2D projection of Iris dataset</span></a>:
                         在Iris数据集上对比LDA和PCA的维数约简功能
                    </p>
                </div>
            </div>

            <div class="section" id="mathematical-formulation-of-the-lda-and-qda-classifiers">
                <h2>1.2.2. LDA分类器与QDA分类器的数学表述<a>¶</a></h2>
                <p>
                    LDA和QDA都可以从简单的概率模型推导出来。此简单概率模型为每一个类
                    <img class="math" src="./images/e9203da50e1059455123460d4e716c9c7f440cc3.png" alt="k" />
                    建立其数据的类条件概率分布：
                    <img class="math" src="./images/6349652f4f3fed24cf0e66ba44b19b5c170cae96.png" alt="P(X|y=k)" /> 
                    。一旦为所有类都建立上述条件概率模型后，就可以使用Bayes决策规则进行预则：
                </p>
                <div class="math">
                    <p class="text-center">
                        <img src="./images/9e7b9be3116147caceb05da29eb4a66905f66481.png" 
                            alt="P(y=k | X) = \frac{P(X | y=k) P(y=k)}{P(X)} 
                            = \frac{P(X | y=k) P(y = k)}{ \sum_{l} P(X | y=l) \cdot P(y=l)}" />
                    </p>
                </div>
                <p>
                    更具体地说，对于线性和二次判别分析来说，类条件概率密度函数
                    <img class="math" src="./images/e69ebd95e385ac5c254ef15635c37b01668decbf.png" alt="P(X|y)" />
                     被建模为多变量高斯分布的形式：
                </p>
                <div class="math">
                    <p class="text-center">
                        <img src="./images/048bd26b8085a30c1baddcc048621fff532ffe47.png" 
                            alt="p(X | y=k) = \frac{1}{(2\pi)^n |\Sigma_k|^{1/2}}\exp\left(-\frac{1}{2} 
                            (X-\mu_k)^t \Sigma_k^{-1} (X-\mu_k)\right)" />
                    </p>
                </div>
                <p>
                    想要使用此模型作为分类器，我们只需要从训练数据中估计出每个类的先验概率
                    <img class="math" src="./images/ca8cad6134f86bef8de0efaa68c0d24323256728.png"  alt="P(y=k)" /> 
                    (通常是训练集中类<img class="math" src="./images/e9203da50e1059455123460d4e716c9c7f440cc3.png" alt="k" />
                    的样本所占的比例), 
                    类均值 <img class="math" src="./images/e0f4c204f26e9afa4f5bbeda60af5262e624c148.png"alt="\mu_k" />
                     (通常取为该类所有样本上计算出的经验均值)
                    以及协方差矩阵 (通常取为从该类样本上得到的经验协方差；或者是使用一个正则化估计器
                    请看下面的 shrinkage 小节).
                </p>
                <p>
                    在LDA中, 所有类的高斯分布被假定为具有相同的协方差矩阵:
                     <img class="math" src="./images/5a0d5681512c9237a828e2667fdf0499af7fad60.png" alt="\Sigma_k = \Sigma" /> 对所有的类
                    <img class="math" src="./images/e9203da50e1059455123460d4e716c9c7f440cc3.png" alt="k" />都成立。
                    这个假设将会导致一个线性决策面，通过对比对数概率比(the log-probability ratios)
                    <img class="math" src="./images/e97ecabcc51db3bd271f1b5b0cf3fcebdd989108.png" alt="\log[P(y=k | X) / P(y=l | X)]" />
                    就可以看到这一点：
                </p>
                <div class="math">
                    <p class="text-center"><img src="./images/9dd706ccca2f0ca465f83a2ee3dc72e7ea904c7c.png"
                             alt="\log\left(\frac{P(y=k|X)}{P(y=l | X)}\right) = 
                            0 \Leftrightarrow (\mu_k-\mu_l)\Sigma^{-1} X = 
                            \frac{1}{2} (\mu_k^t \Sigma^{-1} \mu_k - \mu_l^t \Sigma^{-1} \mu_l)" />
                    </p>
                </div>
                <p>
                    在QDA中，没有对各个类的高斯分布的协方差矩阵<img class="math" src="./images/07b943c6dd1061bde470bebfa3e9b5e373216870.png" alt="\Sigma_k" />
                    做任何假设。这导致了一个二次决策面，详情请看<a class="footnote-reference" href="#id4" id="id1">[3]</a>。 
                </p>
                <div class="well well-sm">
                    <p >Note</p>
                    <p><strong>与高斯朴素分类器的关系</strong></p>
                    <p >
                        如果我们假定在QDA模型中协方差矩阵是对角阵，那么这意味着我们假定了所有类都是条件独立的。
                        而且由此产生的分类器就与高斯朴素贝叶斯分类器是等价的
                         <code class="xref py py-class docutils literal"><span class="pre">naive_bayes.GaussianNB</span></code>。
                    </p>
                </div>
            </div>

            <div class="section" id="mathematical-formulation-of-lda-dimensionality-reduction">
                <h2>1.2.3. LDA维数约简的数学表述<a>¶</a></h2>
                <p>
                    为了理解LDA在维数约简中的使用，我们要从几何角度重新解释和表述上述的LDA分类器规则。
                    我们把所有类的数量记为<img class="math" src="./images/28e003020d0ae96250b302d7d779c791f183f707.png" alt="K" />
                    既然我们已经假定在LDA中所有类的样本上估计出的各自的协方差矩阵是相等的
                    <img class="math" src="./images/19ac15bf260b22dcb61a1042c60259e4b0bfbd64.png" alt="\Sigma" />,
                    那么我们就可以重新调正样本数据的尺度使得这个协方差矩阵变为单位矩阵：
                </p>
                <div class="math">
                    <p class="text-center"><img src="./images/e6da702a9f1619c67e81ec8cd9d976702ffa4a2e.png" alt="X^* = D^{-1/2}U^t X\text{ with }\Sigma = UDU^t" /></p>
                </div>
                <p>
                    当所有类的数据被重调尺度后对一个数据点的分类就等价于在欧氏空间中找到一个距离该点最近的类均值<img class="math" src="./images/5ee369a0219cf8053e73a6926064947a02d97434.png" alt="\mu^*_k" />
                    并认为该点就属于那个最近的类均值所代表的类。但是，这种方法只有在投影到由所有类的均值向量<img class="math" src="./images/5ee369a0219cf8053e73a6926064947a02d97434.png" alt="\mu^*_k" />
                    所张成的<img class="math" src="./images/da6935a2384c32204e1462bc2b64c3b0c02aaee0.png" alt="K-1" />维仿射子空间
                    <img class="math" src="./images/dfe93a68f5d21483dc469d7d85fd6b43ce87f30a.png" alt="H_K" />
                    上才可以工作的很好。这就表明在LDA中隐含了一个将原始数据线性投影到
                    <img class="math" src="./images/da6935a2384c32204e1462bc2b64c3b0c02aaee0.png" alt="K-1" />维空间的维数约简操作。
                </p>
                <p>
                    我们还可以约减更多的维数：选择一个<img class="math" src="./images/0a5711c7a37994043b2bc3bb374adca232491762.png" alt="L" />，
                    然后将数据投影到一个线性子空间 <img class="math" src="./images/7e92db8660cb5a445e9d83da1b9e47a123dece1a.png" alt="H_L" />
                    最大化投影以后的<img class="math" src="./images/5ee369a0219cf8053e73a6926064947a02d97434.png" alt="\mu^*_k" />的方差。
                    (从效果上说，我们是在对投影变换后的类均值<img class="math" src="./images/5ee369a0219cf8053e73a6926064947a02d97434.png" alt="\mu^*_k" />做PCA变换)
                    这个<img class="math" src="./images/0a5711c7a37994043b2bc3bb374adca232491762.png" alt="L" />
                     对应于成员方法<code>discriminant_analysis.LinearDiscriminantAnalysis.transform</code>的参数<code class="docutils literal"><span class="pre">n_components</span></code>。
                     详情请看<a class="footnote-reference" href="#id4" id="id2">[3]</a>。
                </p>
            </div>

            <div class="section" id="shrinkage">
                <h2>1.2.4. 缩减(Shrinkage)<a>¶</a></h2>
                <p>
                    Shrinkage 是一个用来在训练样本数目相对于特征数量比较少的情况下提升协方差估计质量的工具。
                    在这种情形下，经验样本协方差是一个比较差的估计。
                    Shrinkage LDA 可以通过设置<code>discriminant_analysis.LinearDiscriminantAnalysis</code>
                    类的<code class="docutils literal"><span class="pre">shrinkage</span></code> 参数为‘auto’来使用。
                    This automatically determines the optimal shrinkage parameter in an analytic
                    way following the lemma introduced by Ledoit and Wolf <a class="footnote-reference" href="#id5" id="id3">[4]</a>. Note that
                    currently shrinkage only works when setting the <code class="docutils literal"><span class="pre">solver</span></code> parameter to &#8216;lsqr&#8217;
                    or &#8216;eigen&#8217;.
                </p>
                <p>
                    The <code class="docutils literal"><span class="pre">shrinkage</span></code> parameter can also be manually set between 0 and 1. In
                    particular, a value of 0 corresponds to no shrinkage (which means the empirical
                    covariance matrix will be used) and a value of 1 corresponds to complete
                    shrinkage (which means that the diagonal matrix of variances will be used as
                    an estimate for the covariance matrix). Setting this parameter to a value
                    between these two extrema will estimate a shrunk version of the covariance
                    matrix.
                </p>
                <p class="centered">
                    <strong><a class="reference external" href="../auto_examples/classification/plot_lda.html">
                        <img alt="shrinkage" src="./images/plot_lda_0011.png" style="width: 600.0px; height: 450.0px;" /></a></strong>
                </p>
            </div>

            <div class="section" id="estimation-algorithms">
                <h2>1.2.5. 估计算法<a>¶</a></h2>
                <p>
                    默认求解器(solver)是&#8216;svd&#8217;，既可用于分类也可用于数据的维数约简变换。
                    而且它不依赖于协方差矩阵的计算。然而，求解器&#8216;svd&#8217;无法与shrinkage共同使用。
                </p>
                <p>
                    求解器&#8216;lsqr&#8217;是一个只能用来分类的高效算法，而且可以支持shrinkage。
                </p>
                <p>
                    求解器 &#8216;eigen&#8217; 是通过优化类间散布与类内散布的比率来工作。它既可用于分类也可用于数据的维数约简变换
                    而且支持 shrinkage。 然而, 求解器&#8216;eigen&#8217; 需要计算协方差矩阵,
                    所以它不适合用在当特征数量比较多的情况。
                </p>
                <div class="well well-sm">
                    <p >例子:</p>
                    <p>
                        <a>
                        <span>Normal and Shrinkage Linear Discriminant Analysis for classification</span></a>:
                         带shrinkage和不带shrinkage的LDA分类器的比较。
                    </p>
                </div>
                <div class="well">
                    <p class="topic-title first">参考文献:</p>
                    <table class="docutils footnote" frame="void" id="id4" rules="none">
                        <colgroup><col class="label" /><col /></colgroup>
                        <tbody valign="top">
                            <tr>
                                <td class="label">[3]</td>
                                <td>
                                    <em>(<a class="fn-backref" href="#id1">1</a>, <a class="fn-backref" href="#id2">2</a>)</em> &#8220;The Elements of Statistical Learning&#8221;, Hastie T., Tibshirani R.,
                                    Friedman J., Section 4.3, p.106-119, 2008.
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table class="docutils footnote" frame="void" id="id5" rules="none">
                        <colgroup><col class="label" /><col /></colgroup>
                        <tbody valign="top">
                            <tr>
                                <td class="label"><a class="fn-backref" href="#id3">[4]</a></td>
                                <td>
                                    Ledoit O, Wolf M. Honey, I Shrunk the Sample Covariance Matrix.
                                    The Journal of Portfolio Management 30(4), 110-119, 2004.
                                </td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

        </div>

    </div>


    <hr />
    <div class="container">
        <div class="footer text-center">
            &copy; 2016-2100, 版权属于张金明博士 (BSD License).
        </div>
    </div>

</body>
</html>
