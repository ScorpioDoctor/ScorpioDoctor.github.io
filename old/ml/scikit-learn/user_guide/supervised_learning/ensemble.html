<!DOCTYPE html>
<html lang="zh-cn">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <link rel="stylesheet" href="../../../../css-fonts-js/css/bootstrap.min.css" />
    <link rel="stylesheet" href="../../../../css-fonts-js/css/nature.css" />
    <link rel="stylesheet" href="../../../../css-fonts-js/css/pygments.css" />

    <style>
        p {
            font-size: large;
        }
        li {
            font-size: large;
        }
    </style>

    <script src="../../../../css-fonts-js/js/jquery.min.js"></script>
    <script src="../../../../css-fonts-js/js/jquery.js"></script>
    <script src="../../../../css-fonts-js/js/bootstrap.min.js"></script>


    <title>人工智能研究网</title>
    <!--网站访问量统计-->
    <!--<script>
        var _hmt = _hmt || [];
        (function () {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?e7836e37a4cb7584127a787e9b44e3f1";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>-->
</head>


<body>

    <nav class="navbar navbar-inverse">
        <div class="container">
            <div class="navbar-header">
                <a class="navbar-brand" href="../../../../index.html">studyai.cn</a>
            </div>
            <ul class="nav navbar-nav">
                <li class="active"><a href="../../../../index.html">网站主页</a></li>
                <li><a href="../../../../webmaster.html">站长风采</a></li>
                <li><a href="../../../../sponsor.html">赞助我们</a></li>
                <li><a href="../../../index.html">机器学习主页</a></li>
            </ul>
        </div>
    </nav>

    <div class="container">

        <div class="section" id="ensemble-methods">
            <h1 class="page-header">1.11 集成方法(Ensemble Methods)<a>¶</a></h1>

            <p>
                <strong>集成方法</strong> 的目标是组合若干基础估计器(base estimator由给定的学习算法构建)
                的预测来提高只使用单个估计器无法达到的泛化性/鲁棒性。
            </p>
            <p>集成方法通常有两个家族:</p>
            <ul>
                <li>
                    <p class="first">
                        在 <strong>平均方法(averaging methods)</strong>中, 驱动原则是构建若干独立的估计器然后对所有估计器的
                        输出做平均。从平均意义上来说，组合估计器通常总是比任意单个基础估计器更好，因为组合估计器的输出的方差
                        被减小了。
                    </p>
                    <p>
                        <strong>例子:</strong>
                        <a class="reference internal" href="#bagging"><span>Bagging methods</span></a>,
                        <a class="reference internal" href="#forest"><span>Forests of randomized trees</span></a>,
                        ...
                    </p>
                </li>
                <li>
                    <p class="first">
                        作为对比, 在<strong>推举方法(boosting methods)</strong>中,
                        若干基础估计器被序列化构建并且我们试图减小组合估计器的偏差。主要动机是
                        通过组合若干弱模型来获得一个强大的集成估计器。
                    </p>
                    <p>
                        <strong>例子:</strong>
                        <a class="reference internal" href="#adaboost"><span>自适应推举(AdaBoost)</span></a>,
                        <a class="reference internal" href="#gradient-boosting"><span>梯度树推举(Gradient Tree Boosting)</span></a>,
                        ...
                    </p>
                </li>
            </ul>

            <div class="section" id="bagging-meta-estimator">
                <h2>1.11.1 Bagging meta-estimator<a>¶</a></h2>
                <p>
                    在集成方法中, bagging 方法形成了一类算法，这些算法在原始训练集的随机子集上构造一种黑盒估计器的若干
                    实例，然后总体合计这些估计器实例各自的预测结果产生一个最终的预测。这些方法用来减小基础估计器(如决策树)
                    的预测输出的方差。这通过引入随机性到它的构造过程中然后从这些带有随机性的估计器实例中制造出一个集成方法来实现。
                    在很多情况下，bagging方法可被视为是一种提高单个模型的预测能力的非常简单的方法，而且无需改动那个单个模型。
                    由于它们提供了一种减小过拟合的方法，
                    与那些通常只使用弱模型(比如shallow decision trees)的推举方法(boosting methods)相比，bagging方法配合强而复
                    杂的模型(例如fully developed decision trees)能够工作的很好。
                </p>
                <p>
                    Bagging方法有很多种，但大多数时候，他们之间的区别在于从训练集中抽取随机子集的方法不同：
                </p>
                <blockquote>

                    <div>
                        <ul class="simple">
                            <li>
                                当数据集的随机子集被抽取作为样本随机子集的时候，
                                这种方法被称为 Pasting <a class="reference internal" href="#b1999" id="id1">[B1999]</a>.
                            </li>
                            <li>
                                当样本以替换的方式被抽取时，这种方法被称作
                                Bagging <a class="reference internal" href="#b1996" id="id2">[B1996]</a>.
                            </li>
                            <li>
                                当数据集的随机子集被抽取作为特征的随机子集的时候，这种方法被称作 Random Subspaces <a class="reference internal" href="#h1998" id="id3">[H1998]</a>.
                            </li>
                            <li>
                                最后，当若干基础估计器构建在样本子集和特征子集上时，该方法被称为 Random Patches <a class="reference internal" href="#lg2012" id="id4">[LG2012]</a>.
                            </li>
                        </ul>
                    </div>
                </blockquote>
                <p>
                    在 scikit-learn 中, bagging 方法以统一的
                    BaggingClassifier meta-estimator的形式提供 (resp. BaggingRegressor),
                    以用户指定的基础估计器和用于指定抽取随机子集的策略的参数作为输入。
                    特别的，max_samples 和 max_features 分别控制样本子集和特征子集的大小；而
                    bootstrap和bootstrap_features控制着样本和特征是否以替换方式被抽取出来。
                    当使用可用样本的子集时，泛化误差可以通过设置oob_score=True用out-of-bag的样本来估计。
                    作为一个例子，下面的片段展示了如何以KNeighborsClassifier为基础估计器来
                    实例化一个bagging集成估计器。构建每一个基础估计器使用了所有样本的50%的子集和所有特征的50%的子集。
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bagging</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">KNeighborsClassifier</span><span class="p">(),</span>
<span class="gp">... </span>                            <span class="n">max_samples</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre>
                    </div>
                </div>
                <div class="topic">
                    <p class="topic-title first">例子:</p>
                    <ul class="simple">
                        <li><a class="reference internal" href="../auto_examples/ensemble/plot_bias_variance.html#example-ensemble-plot-bias-variance-py"><span>Single estimator versus bagging: bias-variance decomposition</span></a></li>
                    </ul>
                </div>
                <div class="topic">
                    <p class="topic-title first">参考文献：</p>
                    <table class="docutils citation" frame="void" id="b1999" rules="none">
                        <colgroup><col class="label" /><col /></colgroup>
                        <tbody valign="top">
                            <tr>
                                <td class="label"><a class="fn-backref" href="#id1">[B1999]</a></td>
                                <td>
                                    L. Breiman, &#8220;Pasting small votes for classification in large
                                    databases and on-line&#8221;, Machine Learning, 36(1), 85-103, 1999.
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table class="docutils citation" frame="void" id="b1996" rules="none">
                        <colgroup><col class="label" /><col /></colgroup>
                        <tbody valign="top">
                            <tr>
                                <td class="label"><a class="fn-backref" href="#id2">[B1996]</a></td>
                                <td>
                                    L. Breiman, &#8220;Bagging predictors&#8221;, Machine Learning, 24(2),
                                    123-140, 1996.
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table class="docutils citation" frame="void" id="h1998" rules="none">
                        <colgroup><col class="label" /><col /></colgroup>
                        <tbody valign="top">
                            <tr>
                                <td class="label"><a class="fn-backref" href="#id3">[H1998]</a></td>
                                <td>
                                    T. Ho, &#8220;The random subspace method for constructing decision
                                    forests&#8221;, Pattern Analysis and Machine Intelligence, 20(8), 832-844,
                                    1998.
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table class="docutils citation" frame="void" id="lg2012" rules="none">
                        <colgroup><col class="label" /><col /></colgroup>
                        <tbody valign="top">
                            <tr>
                                <td class="label"><a class="fn-backref" href="#id4">[LG2012]</a></td>
                                <td>
                                    G. Louppe and P. Geurts, &#8220;Ensembles on Random Patches&#8221;,
                                    Machine Learning and Knowledge Discovery in Databases, 346-361, 2012.
                                </td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <div class="section" id="forests-of-randomized-trees">
                <h2>1.11.2. Forests of randomized trees<a>¶</a></h2>
                <p>
                    scikit-learn的<code>sklearn.ensemble</code> 模块包含两个基于随机决策树的<strong>平均算法</strong>：
                    随机森林算法(RandomForest algorithm)和极大树(Extra-Trees method)方法。这两个算法都是特别为树算法设计的
                    扰动-组合(perturb-and-combine)技术<a class="reference internal" href="#b1998" id="id5">[B1998]</a>。
                    这意味着在分类器的构造过程中引入的随机性将会产生一组不同分类器的集合。最后，集成分类器的预测是将
                    各个独立分类器的预测做平均然后输出。
                </p>
                <p>
                    就像其他分类器一样，森林分类器必须要在两个数组上拟合：一个大小为<code class="docutils literal"><span class="pre">[n_samples,</span> <span class="pre">n_features]</span></code>
                    的包含训练样本的稀疏或稠密的数组X;以及一个大小为<code class="docutils literal"><span class="pre">[n_samples]</span></code>
                    的包含了目标变量(类标签)的数组Y:
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre>
                    </div>
                </div>
                <p>
                    与 <a class="reference internal" href="tree.html#tree"><span>决策树</span></a>一样, 森林算法也
                    被扩展用于多输出问题(<a class="reference internal" href="tree.html#tree-multioutput"><span>multi-output problems</span></a>)
                    :数组Y的大小为<code class="docutils literal"><span class="pre">[n_samples,</span> <span class="pre">n_outputs]</span></code>。
                </p>

                <div class="section" id="random-forests">
                    <h3>1.11.2.1. 随机森林(Random Forests)<a>¶</a></h3>
                    <p>
                        在随机森林(<code>RandomForestClassifier</code>类和<code>RandomForestRegressor</code>类)中，
                        森林中的每个树都是构造于一个样本，该样本以置换方式(replacement--> i.e., a bootstrap sample)从训练集
                        中抽取。另外，在划分节点构造树的过程中，选择的划分也不再是所有特征上的最好划分。相反的，选择的划分是在
                        所有特征的一个随机子集上的最好划分。作为此随机性的一个结果，随机森林的偏差相比于那些单个无随机性决策树
                        的偏差来说稍微有些大；但是，由于最后的平均操作，随机森林的预测方差也会降低(通常足以弥补偏差的增加)，
                        因此产生了整体上比较好的模型。
                    </p>
                    <p>
                        与原始的版本<a class="reference internal" href="#b2001" id="id6">[B2001]</a>对比 ,
                        scikit-learn 通过平均单个分类器的概率性预测来组合若干单个分类器去实现随机森林，而不是让每个分类器为单个类投票。
                    </p>
                </div>

                <div class="section" id="extremely-randomized-trees">
                    <h3>1.11.2.2. 极大随机树(Extremely Randomized Trees)<a>¶</a></h3>
                    <p>
                        在极大随机树(<code>ExtraTreesClassifier</code>类和<code>ExtraTreesRegressor</code>类)中，
                        在划分(or 分裂 splits)的计算方式上随机性又更进一步。就像在随机森林中那样，极大随机树也使用
                        候选特征集合的一个随机子集。与寻找最有判别力的阈值的策略不同，在极大随机树中每一个候选特征的阈值
                        也是随机抽取的。然后，这些随机产生的阈值中的最好的阈值将被用到分裂规则中去。这种策略通常能够
                        使得模型方差降的更低，但代价是模型的偏差也会稍微增加。
                    </p>
                    <div class="highlight-python">
                        <div class="highlight">
                            <pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">ExtraTreesClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>                             
<span class="go">0.97...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>                             
<span class="go">0.999...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">ExtraTreesClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.999</span>
<span class="go">True</span>
</pre>
                        </div>
                    </div>
                    <p class="text-center">
                        <a>
                            <img alt="./images/plot_forest_iris_0011.png" src="./images/plot_forest_iris_0011.png"
                                 style="width: 600.0px; height: 450.0px;" />
                        </a>
                    </p>
                </div>

                <div class="section" id="parameters">
                    <h3>1.11.2.3. 参数 <a>¶</a></h3>
                    <p>
                        使用这些方法的时候，需要调节的主要参数有两个：<code>n_estimators</code>和<code>max_features</code>。前一个参数
                        森林中树的数量。树越多，分类器越好，当然计算代价也越大。另外需要注意的是，当树的数量增加到一定程度，分类性能就再也不增加了。
                        后一个参数是在树节点上进行分裂时特征的随机子集的大小。这个随机子集越小，输出方差也会越小但同时也会导致偏差增加。
                        这两个参数的比较好的默认经验值是：对于回归问题<code>max_features=n_features</code>；对于分类问题<code>max_features=sqrt(n_features)</code>。
                        其中，<code>n_features</code>是数据中特征的总数量。当设置<code>max_depth=None</code>并且<code>min_samples_split=1</code>时，
                        也就是说当我们允许树完全发展的时候，通常能够获得比较好的结果。但是这样的设置通常不是最优的，而且会消耗大量内存。我们
                        应该使用交叉验证来获得最佳参数。另外，在随机森林中，默认使用了boostrap样本<code>bootstrap=True</code>；而在极大随机树中，
                        默认使用整个样本集，即<code>bootstrap=False</code>。当使用bootstrap样本的时候，泛化误差可以在剩下的样本集上估计得到。这可以通过设置
                        <code>oob_score=True</code>来开启。
                    </p>
                </div>


                <div class="section" id="parallelization">
                    <h3>1.11.2.4. 并行计算<a>¶</a></h3>
                    <p>
                        最后，ensemble模块支持很多的树的并行构建以及通过<code>n_jobs</code>进行并行预测。如果<code>n_jobs=k</code>，
                        那么计算将被划分成<code>k</code>个任务然后在计算机的<code>k</code>个核上并行计算。如果<code>n_jobs=-1</code>，
                        那么计算机中所有的可用核都将被使用。注意到由于inter处理器的通信等，加速可能不会是线性的(也就是不会是k倍的)。
                        但当我们要构建一大批树或者数据集很大的时候，仍然可以获得显著的加速。
                    </p>
                    <div class="topic">
                        <p class="topic-title first">例子:</p>
                        <ul class="simple">
                            <li><a><span>Plot the decision surfaces of ensembles of trees on the iris dataset</span></a></li>
                            <li><a><span>Pixel importances with a parallel forest of trees</span></a></li>
                            <li><a><span>Face completion with a multi-output estimators</span></a></li>
                        </ul>
                    </div>
                    <div class="topic">
                        <p class="topic-title first">参考文献</p>
                        <table class="docutils citation" frame="void" id="b2001" rules="none">
                            <colgroup><col class="label" /><col /></colgroup>
                            <tbody valign="top">
                                <tr>
                                    <td class="label"><a class="fn-backref" href="#id6">[B2001]</a></td>
                                    <td>
                                        <ol class="first last upperalpha simple" start="12">
                                            <li>Breiman, &#8220;Random Forests&#8221;, Machine Learning, 45(1), 5-32, 2001.</li>
                                        </ol>
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                        <table class="docutils citation" frame="void" id="b1998" rules="none">
                            <colgroup><col class="label" /><col /></colgroup>
                            <tbody valign="top">
                                <tr>
                                    <td class="label"><a class="fn-backref" href="#id5">[B1998]</a></td>
                                    <td>
                                        <ol class="first last upperalpha simple" start="12">
                                            <li>Breiman, &#8220;Arcing Classifiers&#8221;, Annals of Statistics 1998.</li>
                                        </ol>
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                        <table class="docutils citation" frame="void" id="gew2006" rules="none">
                            <colgroup><col class="label" /><col /></colgroup>
                            <tbody valign="top">
                                <tr>
                                    <td class="label">[GEW2006]</td>
                                    <td>
                                        P. Geurts, D. Ernst., and L. Wehenkel, &#8220;Extremely randomized
                                        trees&#8221;, Machine Learning, 63(1), 3-42, 2006.
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>

                <div class="section" id="feature-importance-evaluation">
                    <span id="random-forest-feature-importance"></span><h3>1.11.2.5. 特征重要性评估<a href="#feature-importance-evaluation" title="Permalink to this headline">¶</a></h3>
                    <p>
                        在一个树中，用作决策节点的特征的相对排名(也就是深度)可以被用于评价那个特征
                        对于目标变量的预测的相对重要性。在树顶部的那些特征有助于更大的一部分的输入样本的
                        最终预测决策。因此，这些特征贡献的<strong>expected fraction of the samples</strong>可被用于
                        估计特征的相对重要性(<strong>relative importance of the features</strong>)。
                    </p>
                    <p>
                        通过<strong>平均</strong>那些在若干个树上的期望激活率(expected activity rates),我们可以
                        <strong>减小估计器的方差</strong>并且用它进行特征选择。
                    </p>
                    <p>
                        下面的例子展示了使用<code>ExtraTreesClassifier</code>模型进行人脸识别过程中每一个独立的像素的
                        相对重要性的彩色表示。
                    </p>
                    <p class="text-center">
                        <a><img alt=".//images//plot_forest_importances_faces_0011.png" src="./images/plot_forest_importances_faces_0011.png" style="width: 450.0px; height: 450.0px;" /></a>
                    </p>
                    <p>
                        在实践中，这些特征重要性的估计结果被存储在拟合好的树模型的属性<code>feature_importances_</code>中。
                        这是一个大小为<code>(n_features,)</code>的数组，其中的值是正的而且和为1。值越大，代表那个对应特征对预测函数的贡献的重要性越大。
                    </p>
                    <div class="topic">
                        <p class="topic-title first">例子:</p>
                        <ul class="simple">
                            <li><a><span>Pixel importances with a parallel forest of trees</span></a></li>
                            <li><a><span>Feature importances with forests of trees</span></a></li>
                        </ul>
                    </div>
                </div>

                <div class="section" id="totally-random-trees-embedding">
                    <span id="random-trees-embedding"></span><h3>1.11.2.6. 完全随机树嵌入(Totally Random Trees Embedding)<a class="headerlink" href="#totally-random-trees-embedding" title="Permalink to this headline">¶</a></h3>
                    <p>
                        <a><code class="xref py py-class docutils literal"><span class="pre">RandomTreesEmbedding</span></code></a>
                        实现了一个对数据的无监督变换操作。使用一个包含若干完全随机树的森林，<code>RandomTreesEmbedding</code>对象使用
                        一个数据点在树上结束的那些叶节点的索引来编码此数据点。这些叶节点索引随后以one-of-K的方式被编码，
                        这会产生高维稀疏二值编码。编码操作可以高效的计算出来然后被用做其他学习任务的基础。
                        编码的长度与稀疏性会受到树的数量以及每棵树的最大深度的影响。 For each tree in the ensemble, the coding
                        contains one entry of one. The size of the coding is at most <code class="docutils literal">
                            <span class="pre">n_estimators</span> <span class="pre">*</span> <span class="pre">2</span>
                            <span class="pre">**</span> <span class="pre">max_depth</span>
                        </code>, the maximum number of leaves in the forest.
                    </p>
                    <p>
                        由于邻居节点会以很大的可能性落入一棵树的同一个叶节点，所以此变换执行了一个隐式的无参数的密度估计过程。
                    </p>
                    <div class="topic">
                        <p class="topic-title first">例子:</p>
                        <ul class="simple">
                            <li><a><span>使用完全随机树(Totally Random Trees)进行哈希特征变换</span></a></li>
                            <li>
                                <a><span>Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...</span></a>
                                在手写字符集上比较非线性维数约简技术。
                            </li>
                            <li>
                                <a><span>Feature transformations with ensembles of trees</span></a>基于特征变换比较监督树和无监督树。
                            </li>
                        </ul>
                    </div>
                    <div class="admonition seealso">
                        <p class="first admonition-title">See also</p>
                        <p class="last">
                            <a><span>Manifold learning</span></a> 技术也可以用于实现特征空间的非线性表达，而且这些方法也聚焦于
                            维数约简。
                        </p>
                    </div>
                </div>

            </div>

            <div class="section" id="adaboost">
                <h2>1.11.3. 自适应推举算法：AdaBoost<a>¶</a></h2>
                <p>
                    <code>sklearn.ensemble</code> 模块包含了著名推举算法：AdaBoost； 由 Freund 和 Schapire 于 1995
                    <a class="reference internal" href="#fs1995" id="id8">[FS1995]</a>提出。
                </p>
                <p>
                    AdaBoost算法的核心原则是在采样权重可重复修改的数据上拟合一个弱学习器序列。此处，弱学习器是指那些仅仅比随机猜测稍微好一点儿
                    的学习器模型，例如小决策树。使用这些拟合好的弱学习器进行预测的方法是组合弱学习器的加权投票结果来产生最终预测。在每一个
                    称为推举(boosting)的迭代中，对数据的修改操作是将权重
                    <img class="math" src="./images/ee71d9a60992887aa4b7a1e010886633661a7ada.png" alt="w_1" />,
                    <img class="math" src="./images/9c87339c285f85c875225faea7ee0af7107cf77b.png" alt="w_2" />,
                    ..., <img class="math" src="./images/093edac7548ff6dcb81cfbee5ef2249fb8295d23.png" alt="w_N" />
                    添加到每一个训练样本上。 初始化的时候，这些权重都被设置为
                    <img class="math" src="./images/f517ce4e0d7328c27b2820bb5259535d3e1d8aa7.png" alt="w_i = 1/N" />,
                    所以在第一步只是在原始数据上简单的训练一个弱分类器。在每一次的后继迭代中，样本权重会被单独修改，然后学习算法会被重新用在这些权重已被修改的数据上进行训练。
                    在一个给定的迭代步中，那些被上一迭代步的推举模型错误预测的样本会获得更高的权重，而预测正确的样本的权重会下降。随着迭代的持续进行，
                    那些比较难于预测的样本将会获得推举模型的持续关注。因此，每一个后续添加的弱学习器将会被强制聚焦到那些之前一直被错误预测的样本上。
                    <a class="reference internal" href="#htf" id="id9">[HTF]</a>.
                </p>
                <p class="text-center">
                    <a class="reference external image-reference">
                        <img alt="./images/plot_adaboost_hastie_10_2_0011.png" src="./images/plot_adaboost_hastie_10_2_0011.png"
                             style="width: 600.0px; height: 450.0px;" />
                    </a>
                </p>
                <p>AdaBoost可被用于分类和回归问题:</p>
                <blockquote>
                    <div>
                        <ul class="simple">
                            <li>
                                对于多任务分类问题,<code>AdaBoostClassifier</code>实现了AdaBoost-SAMME 和 AdaBoost-SAMME.R <a class="reference internal" href="#zzrh2009" id="id10">[ZZRH2009]</a>.
                            </li>
                            <li>对于回归问题, <code>AdaBoostRegressor</code>实现了 AdaBoost.R2 <a class="reference internal" href="#d1997" id="id11">[D1997]</a>.</li>
                        </ul>
                    </div>
                </blockquote>
                <div class="section" id="usage">
                    <h3>1.11.3.1. AdaBoost用法<a>¶</a></h3>
                    <p>
                        下面的例子展示了如何使用100个弱学习器拟合一个AdaBoost分类器：
                    </p>
                    <div class="highlight-python">
                        <div class="highlight">
                            <pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>                             
<span class="go">0.9...</span>
</pre>
                        </div>
                    </div>
                    <p>
                        弱学习器的个数通过参数<code>n_estimators</code>来控制。参数<code>learning_rate</code>控制着弱学习器在最终的组合预测输出中的高效率。
                        默认情况下，所有弱学习器都是决策树桩(decision stumps)。 不同的弱学习器可以通过参数<code>base_estimator</code>来指定。
                        用于获得好结果的可调参数主要是<code>n_estimators</code>以及基础估计器(base estimators)的复杂度。如果基础估计器是是
                        决策树的话，那么复杂度就要通过树的最大深度<code>max_depth</code>或者每个叶节点需要的最小样本量<code>min_samples_leaf</code>。
                    </p>

                    <div class="topic">
                        <p class="topic-title first">例子:</p>
                        <ul class="simple">
                            <li>
                                <a class="reference internal"><span>Discrete versus Real AdaBoost</span></a>比较分类误差：decision stump, decision tree, 和 a boosted
                                decision stump using AdaBoost-SAMME and AdaBoost-SAMME.R.
                            </li>
                            <li>
                                <a class="reference internal"><span>Multi-class AdaBoosted Decision Trees</span></a> 展示AdaBoost-SAMME 和 AdaBoost-SAMME.R 在多分类问题上的性能。
                            </li>
                            <li>
                                <a class="reference internal"><span>Two-class AdaBoost</span></a>
                                展示AdaBoost-SAMME在非线性可分的两类问题上的决策边界和决策函数值。
                            </li>
                            <li>
                                <a class="reference internal"><span>Decision Tree Regression with AdaBoost</span></a>
                                展示AdaBoost.R2算法如何用于回归问题。
                            </li>
                        </ul>
                    </div>

                    <div class="topic">
                        <p class="topic-title first">参考文献：</p>
                        <table class="docutils citation" frame="void" id="fs1995" rules="none">
                            <colgroup><col class="label" /><col /></colgroup>
                            <tbody valign="top">
                                <tr>
                                    <td class="label"><a class="fn-backref" href="#id8">[FS1995]</a></td>
                                    <td>
                                        Y. Freund, and R. Schapire, &#8220;A Decision-Theoretic Generalization of
                                        On-Line Learning and an Application to Boosting&#8221;, 1997.
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                        <table class="docutils citation" frame="void" id="zzrh2009" rules="none">
                            <colgroup><col class="label" /><col /></colgroup>
                            <tbody valign="top">
                                <tr>
                                    <td class="label"><a class="fn-backref" href="#id10">[ZZRH2009]</a></td>
                                    <td>
                                        J. Zhu, H. Zou, S. Rosset, T. Hastie. &#8220;Multi-class AdaBoost&#8221;,
                                        2009.
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                        <table class="docutils citation" frame="void" id="d1997" rules="none">
                            <colgroup><col class="label" /><col /></colgroup>
                            <tbody valign="top">
                                <tr>
                                    <td class="label"><a class="fn-backref" href="#id11">[D1997]</a></td>
                                    <td>
                                        <ol class="first last upperalpha simple" start="8">
                                            Drucker. &#8220;Improving Regressors using Boosting Techniques&#8221;, 1997.
                                        </ol>
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                        <table class="docutils citation" frame="void" id="htf" rules="none">
                            <colgroup><col class="label" /><col /></colgroup>
                            <tbody valign="top">
                                <tr>
                                    <td class="label"><a class="fn-backref" href="#id9">[HTF]</a></td>
                                    <td>
                                        T. Hastie, R. Tibshirani and J. Friedman, &#8220;Elements of
                                        Statistical Learning Ed. 2&#8221;, Springer, 2009.
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                </div>
            </div>

            <div class="section" id="gradient-tree-boosting">
                <h2>1.11.4 梯度树推举:Gradient Tree Boosting<a>¶</a></h2>
                <p>
                    <a class="reference external" href="http://en.wikipedia.org/wiki/Gradient_boosting">Gradient Tree Boosting</a>
                    或 Gradient Boosted Regression Trees (GBRT) 是推举算法的一个推广模型，可以使用任意的可微损失函数。
                    GBRT是准确的和有效的现成的程序可被用于回归和分类任务。梯度树推举模型用途广泛包括网络搜索排序和以及生态学。
                </p>
                <p>GBRT的优点是:</p>
                <blockquote>
                    <div>
                        <ul class="simple">
                            <li>天然具备处理混合数据(或异构特征)的能力</li>
                            <li>强大的预测能力</li>
                            <li>对输出空间的离群值有很好的鲁棒性(即，鲁棒损失函数)</li>
                        </ul>
                    </div>
                </blockquote>
                <p>GBRT的缺点是:</p>
                <blockquote>
                    <div>
                        <ul class="simple">
                            <li>
                                可扩展性, 由于推举算法的序列化天性，它很难并行计算。
                            </li>
                        </ul>
                    </div>
                </blockquote>
                <p>
                    <code>sklearn.ensemble</code>模块提供了用于回归和分类的GBRT方法。
                </p>

                <div class="section" id="classification">
                    <h3>1.11.4.1. 分类<a>¶</a></h3>
                    <p>
                        <code>GradientBoostingClassifier</code>对象支持二分类和多分类问题。
                        下面的例子演示了怎样拟合一个带有100个决策树桩作为弱学习器的梯度推举分类器。
                    </p>
                    <div class="highlight-python">
                        <div class="highlight">
                            <pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_hastie_10_2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_hastie_10_2</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">2000</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">2000</span><span class="p">:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="mi">2000</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">2000</span><span class="p">:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>                 
<span class="go">0.913...</span>
</pre>
                        </div>
                    </div>
                    <p>
                        弱学习器(即回归树)的数量由参数<code>n_estimators</code>来控制；每棵树的大小可以通过设置树的最大深度
                        <code>max_depth</code>或设置叶节点的最大个数<code>max_leaf_nodes</code>来控制。参数<code>learning_rate</code>是一个在范围(0.0,1.0]之间的超参数，使用
                        <a class="reference internal" href="#gradient-boosting-shrinkage"><span>shrinkage</span></a>控制过拟合。
                    </p>
                    <div class="admonition note">
                        <p class="first admonition-title">注意</p>
                        <p class="last">
                            两类以上的分类问题在每次迭代中需要<code>n_classes</code>个回归树的归纳，因此，归纳树的总数量等于<code>n_classes</code>*<code>n_estimators</code>。
                            如果数据集包含了数量很多的类，那么建议使用<a><code><span class="pre">RandomForestClassifier</span></code></a>作为对<code>GradientBoostingClassifier</code>的替代。
                        </p>
                    </div>
                </div>

                <div class="section" id="regression">
                    <h3>1.11.4.2. 回归<a>¶</a></h3>
                    <p>
                        <code>GradientBoostingRegressor</code>支持一系列<a class="reference internal" href="#gradient-boosting-loss"><span>不同的损失函数</span></a>
                        用于回归，可以通过参数<code class="docutils literal"><span class="pre">loss</span></code>来指定; 默认的回归损失函数是least squares (<code class="docutils literal"><span class="pre">'ls'</span></code>).
                    </p>
                    <div class="highlight-python">
                        <div class="highlight">
                            <pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_friedman1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_friedman1</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1200</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">200</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">200</span><span class="p">:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="mi">200</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">200</span><span class="p">:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">est</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;ls&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>    
<span class="go">5.00...</span>
</pre>
                        </div>
                    </div>
                    <p>
                        下图展示了将带有最小均方损失以及500个弱学习器的<code>GradientBoostingRegressor</code>回归器对象应用到
                        波士顿房价数据集(<code class="pre">sklearn.datasets.load_boston</code>)上得到的结果。左边的图展示了迭代过程中的训练和测试误差；
                        每次迭代产生的训练误差存储在梯度推举模型的<code>train_score_</code>属性中，而每次迭代后的测试误差通过成员方法<code>staged_predict</code>来获得，该函数返回一个产生每阶段预测结果的生成器。
                        这些训练过程的结果图可以帮助我们确定弱学习器的最优数量<code>n_estimators</code>。右边的图展示了特征重要性，通过<code>feature_importances_</code>属性来获得。
                    </p>
                    <p class="text-center">
                        <a><img alt="./images/plot_gradient_boosting_regression_0011.png" src="./images/plot_gradient_boosting_regression_0011.png" style="width: 900.0px; height: 450.0px;" /></a>
                    </p>
                    <div class="topic">
                        <p class="topic-title first">例子:</p>
                        <ul class="simple">
                            <li><a class="reference internal" href="../auto_examples/ensemble/plot_gradient_boosting_regression.html#example-ensemble-plot-gradient-boosting-regression-py"><span>Gradient Boosting regression</span></a></li>
                            <li><a class="reference internal" href="../auto_examples/ensemble/plot_gradient_boosting_oob.html#example-ensemble-plot-gradient-boosting-oob-py"><span>Gradient Boosting Out-of-Bag estimates</span></a></li>
                        </ul>
                    </div>
                </div>

                <div class="section" id="fitting-additional-weak-learners">
                    <span id="gradient-boosting-warm-start"></span><h3>1.11.4.3. 拟合后期添加的弱学习器<a class="headerlink" href="#fitting-additional-weak-learners" title="Permalink to this headline">¶</a></h3>
                    <p>
                        <code>GradientBoostingRegressor</code> 和 <code>GradientBoostingClassifier</code>对象都支持<code>warm_start=True</code>，
                        它允许你添加更多的弱学习器到一个已经拟合好的模型中。
                    </p>
                    <div class="highlight-python">
                        <div class="highlight">
                            <pre><span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># set warm_start and new nr of trees</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="c1"># fit additional 100 trees to est</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>    
<span class="go">3.84...</span>
</pre>
                        </div>
                    </div>
                </div>

                <div class="section" id="controlling-the-tree-size">
                    <span id="gradient-boosting-tree-size"></span><h3>1.11.4.4. 控制树的大小<a>¶</a></h3>
                    <p>
                        回归树基本学习器的大小决定了梯度推举模型能够捕获的变量之间相互影响的水平。一般来说，一颗深度为<code>h</code>的树可以捕获<code>h</code>阶的相互影响。
                        有两种方法来控制每个独立的回归树的大小。
                    </p>
                    <p>
                        如果你指定了<code>max_depth=h</code>，那么一个深度为<code>h</code>的完整二叉树就会生长出来。这棵完整二叉树最多会有<code>2**h</code>个叶节点，以及<code>2**h-1</code>个分裂节点。
                    </p>
                    <p>
                        另一种控制树的大小的方法是通过参数<code>max_leaf_nodes</code>指定叶节点的数量。这种情况下，树的生长使用最好节点优先分裂的搜索策略(best-first search):那些在impurity上有最大提升的节点将会被首先扩展。
                        一颗拥有最大节点数量为<code>max_leaf_nodes=k</code>的树将会有<code>k-1</code>个分裂节点，因此它可以建模的相互影响的阶数可以达到<code>max_leaf_nodes-1</code>阶。
                    </p>
                    <p>

                        We found that <code class="docutils literal"><span class="pre">max_leaf_nodes=k</span></code> gives comparable results to <code class="docutils literal"><span class="pre">max_depth=k-1</span></code>
                        but is significantly faster to train at the expense of a slightly higher
                        training error.
                        The parameter <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> corresponds to the variable <code class="docutils literal"><span class="pre">J</span></code> in the
                        chapter on gradient boosting in <a class="reference internal" href="#f2001" id="id13">[F2001]</a> and is related to the parameter
                        <code class="docutils literal"><span class="pre">interaction.depth</span></code> in R&#8217;s gbm package where <code class="docutils literal"><span class="pre">max_leaf_nodes</span> <span class="pre">==</span> <span class="pre">interaction.depth</span> <span class="pre">+</span> <span class="pre">1</span></code> .
                    </p>
                </div>

            </div>

            <div class="section" id="votingclassifier">
                <h2>1.11.5 VotingClassifier<a>¶</a></h2>

            </div>


        </div>

    </div>


    <hr />
    <div class="container">
        <div class="footer text-center">
            &copy; 2016-2100, 版权属于张金明博士 (BSD License).
        </div>
    </div>

</body>
</html>
