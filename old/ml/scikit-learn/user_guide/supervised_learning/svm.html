<!DOCTYPE html>
<html lang="zh-cn">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <link rel="stylesheet" href="../../../../css-fonts-js/css/bootstrap.min.css" />
   <!-- <link rel="stylesheet" href="../../../../css-fonts-js/css/bootstrap-responsive.css" />-->
    <!--<link rel="stylesheet" href="../../../../css-fonts-js/css/gallery.css" />-->
   <!-- <link rel="stylesheet" href="../../../../css-fonts-js/css/nature.css" />-->
    <link rel="stylesheet" href="../../../../css-fonts-js/css/pygments.css" />

    <style>
        p {
            font-size: large;
        }

        li {
            font-size: large;
        }
    </style>

    <script src="../../../../css-fonts-js/js/jquery.min.js"></script>
    <script src="../../../../css-fonts-js/js/jquery.js"></script>
    <!--<script src="../../../../css-fonts-js/js/copybutton.js"></script>
    <script src="../../../../css-fonts-js/js/doctools.js"></script>
    <script src="../../../../css-fonts-js/js/jquery.maphilight.js"></script>
    <script src="../../../../css-fonts-js/js/jquery.maphilight.min.js"></script>
    <script src="../../../../css-fonts-js/js/underscore.js"></script>-->
    <script src="../../../../css-fonts-js/js/bootstrap.min.js"></script>


    <title>人工智能研究网</title>
    <!--网站访问量统计-->
    <script>
        var _hmt = _hmt || [];
        (function () {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?e7836e37a4cb7584127a787e9b44e3f1";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>
</head>


<body>

    <nav class="navbar navbar-inverse">
        <div class="container">
            <div class="navbar-header">
                <a class="navbar-brand" href="../../../../index.html">studyai.cn</a>
            </div>
            <ul class="nav navbar-nav">
                <li class="active"><a href="../../../../index.html">网站主页</a></li>
                <li><a href="../../../../webmaster.html">站长风采</a></li>
                <li><a href="../../../../sponsor.html">赞助我们</a></li>
                <li><a href="../../../index.html">机器学习主页</a></li>
            </ul>
        </div>
    </nav>

    <div class="container">

        <div class="section" id="support-vector-machines">
            <h1 class="page-header">1.4 支持向量机(Support Vector Machines)<a>¶</a></h1>
            <p>
                <strong>Support vector machines (SVMs)</strong> are a set of supervised learning
                methods used for <a class="reference internal" href="#svm-classification"><span>classification</span></a>,
                <a class="reference internal" href="#svm-regression"><span>regression</span></a> and <a class="reference internal" href="#svm-outlier-detection"><span>outliers detection</span></a>.
            </p>
            <p>The advantages of support vector machines are:</p>
            <blockquote>
                <div>
                    <ul class="simple">
                        <li>Effective in high dimensional spaces.</li>
                        <li>
                            Still effective in cases where number of dimensions is greater
                            than the number of samples.
                        </li>
                        <li>
                            Uses a subset of training points in the decision function (called
                            support vectors), so it is also memory efficient.
                        </li>
                        <li>
                            Versatile: different <a class="reference internal" href="#svm-kernels"><span>Kernel functions</span></a> can be
                            specified for the decision function. Common kernels are
                            provided, but it is also possible to specify custom kernels.
                        </li>
                    </ul>
                </div>
            </blockquote>
            <p>The disadvantages of support vector machines include:</p>
            <blockquote>
                <div>
                    <ul class="simple">
                        <li>
                            If the number of features is much greater than the number of
                            samples, the method is likely to give poor performances.
                        </li>
                        <li>
                            SVMs do not directly provide probability estimates, these are
                            calculated using an expensive five-fold cross-validation
                            (see <a class="reference internal" href="#scores-probabilities"><span>Scores and probabilities</span></a>, below).
                        </li>
                    </ul>
                </div>
            </blockquote>
            <p>
                The support vector machines in scikit-learn support both dense
                (<code class="docutils literal"><span class="pre">numpy.ndarray</span></code> and convertible to that by <code class="docutils literal"><span class="pre">numpy.asarray</span></code>) and
                sparse (any <code class="docutils literal"><span class="pre">scipy.sparse</span></code>) sample vectors as input. However, to use
                an SVM to make predictions for sparse data, it must have been fit on such
                data. For optimal performance, use C-ordered <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> (dense) or
                <code class="docutils literal"><span class="pre">scipy.sparse.csr_matrix</span></code> (sparse) with <code class="docutils literal"><span class="pre">dtype=float64</span></code>.
            </p>


            <div class="section" id="classification">
                <h2>1.4.1 分类(Classification)<a>¶</a></h2>
                <p>
                    <a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="xref py py-class docutils literal"><span class="pre">SVC</span></code></a>, <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code class="xref py py-class docutils literal"><span class="pre">NuSVC</span></code></a> and <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></code></a> are classes
                    capable of performing multi-class classification on a dataset.
                </p>
                <div class="figure align-center">
                    <a class="reference external image-reference" href="../auto_examples/svm/plot_iris.html">
                    <img alt="./images/plot_iris_0012.png" src="./images/plot_iris_0012.png" /></a>
                </div>
                <p>
                    <a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="xref py py-class docutils literal"><span class="pre">SVC</span></code></a> and <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code class="xref py py-class docutils literal"><span class="pre">NuSVC</span></code></a> are similar methods, but accept
                    slightly different sets of parameters and have different mathematical
                    formulations (see section <a class="reference internal" href="#svm-mathematical-formulation"><span>Mathematical formulation</span></a>). On the
                    other hand, <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></code></a> is another implementation of Support
                    Vector Classification for the case of a linear kernel. Note that
                    <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></code></a> does not accept keyword <code class="docutils literal"><span class="pre">kernel</span></code>, as this is
                    assumed to be linear. It also lacks some of the members of
                    <a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="xref py py-class docutils literal"><span class="pre">SVC</span></code></a> and <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code class="xref py py-class docutils literal"><span class="pre">NuSVC</span></code></a>, like <code class="docutils literal"><span class="pre">support_</span></code>.
                </p>
                <p>
                    As other classifiers, <a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="xref py py-class docutils literal"><span class="pre">SVC</span></code></a>, <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code class="xref py py-class docutils literal"><span class="pre">NuSVC</span></code></a> and
                    <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></code></a> take as input two arrays: an array X of size <code class="docutils literal">
                        <span class="pre">[n_samples,</span>
                        <span class="pre">n_features]</span>
                    </code> holding the training samples, and an array y of class labels
                    (strings or integers), size <code class="docutils literal"><span class="pre">[n_samples]</span></code>:
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  
<span class="go">SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,</span>
<span class="go">    decision_function_shape=None, degree=3, gamma=&#39;auto&#39;, kernel=&#39;rbf&#39;,</span>
<span class="go">    max_iter=-1, probability=False, random_state=None, shrinking=True,</span>
<span class="go">    tol=0.001, verbose=False)</span>
</pre>
                    </div>
                </div>
                <p>After being fitted, the model can then be used to predict new values:</p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>
<span class="go">array([1])</span>
</pre>
                    </div>
                </div>
                <p>
                    SVMs decision function depends on some subset of the training data,
                    called the support vectors. Some properties of these support vectors
                    can be found in members <code class="docutils literal"><span class="pre">support_vectors_</span></code>, <code class="docutils literal"><span class="pre">support_</span></code> and
                    <code class="docutils literal"><span class="pre">n_support</span></code>:
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># get support vectors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">support_vectors_</span>
<span class="go">array([[ 0.,  0.],</span>
<span class="go">       [ 1.,  1.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># get indices of support vectors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">support_</span> 
<span class="go">array([0, 1]...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># get number of support vectors for each class</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">n_support_</span> 
<span class="go">array([1, 1]...)</span>
</pre>
                    </div>
                </div>
                <div class="section" id="multi-class-classification">
                    <span id="svm-multi-class"></span><h3>1.4.1.1. Multi-class classification<a class="headerlink" href="#multi-class-classification" title="Permalink to this headline">¶</a></h3>
                    <p>
                        <a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="xref py py-class docutils literal"><span class="pre">SVC</span></code></a> and <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code class="xref py py-class docutils literal"><span class="pre">NuSVC</span></code></a> implement the &#8220;one-against-one&#8221;
                        approach (Knerr et al., 1990) for multi- class classification. If
                        <code class="docutils literal"><span class="pre">n_class</span></code> is the number of classes, then <code class="docutils literal"><span class="pre">n_class</span> <span class="pre">*</span> <span class="pre">(n_class</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">/</span> <span class="pre">2</span></code>
                        classifiers are constructed and each one trains data from two classes.
                        To provide a consistent interface with other classifiers, the
                        <code class="docutils literal"><span class="pre">decision_function_shape</span></code> option allows to aggregate the results of the
                        &#8220;one-against-one&#8221; classifiers to a decision function of shape <code class="docutils literal">
                            <span class="pre">(n_samples,</span>
                            <span class="pre">n_classes)</span>
                        </code>:
                    </p>
                    <div class="highlight-python">
                        <div class="highlight">
                            <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">decision_function_shape</span><span class="o">=</span><span class="s1">&#39;ovo&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span> 
<span class="go">SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,</span>
<span class="go">    decision_function_shape=&#39;ovo&#39;, degree=3, gamma=&#39;auto&#39;, kernel=&#39;rbf&#39;,</span>
<span class="go">    max_iter=-1, probability=False, random_state=None, shrinking=True,</span>
<span class="go">    tol=0.001, verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dec</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">decision_function</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dec</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># 4 classes: 4*3/2 = 6</span>
<span class="go">6</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">decision_function_shape</span> <span class="o">=</span> <span class="s2">&quot;ovr&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dec</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">decision_function</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dec</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># 4 classes</span>
<span class="go">4</span>
</pre>
                        </div>
                    </div>
                    <p>
                        On the other hand, <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></code></a> implements &#8220;one-vs-the-rest&#8221;
                        multi-class strategy, thus training n_class models. If there are only
                        two classes, only one model is trained:
                    </p>
                    <div class="highlight-python">
                        <div class="highlight">
                            <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">lin_clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lin_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span> 
<span class="go">LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,</span>
<span class="go">     intercept_scaling=1, loss=&#39;squared_hinge&#39;, max_iter=1000,</span>
<span class="go">     multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;, random_state=None, tol=0.0001,</span>
<span class="go">     verbose=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dec</span> <span class="o">=</span> <span class="n">lin_clf</span><span class="o">.</span><span class="n">decision_function</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dec</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="go">4</span>
</pre>
                        </div>
                    </div>
                    <p>
                        See <a class="reference internal" href="#svm-mathematical-formulation"><span>Mathematical formulation</span></a> for a complete description of
                        the decision function.
                    </p>
                    <p>
                        Note that the <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></code></a> also implements an alternative multi-class
                        strategy, the so-called multi-class SVM formulated by Crammer and Singer, by
                        using the option <code class="docutils literal"><span class="pre">multi_class='crammer_singer'</span></code>. This method is consistent,
                        which is not true for one-vs-rest classification.
                        In practice, one-vs-rest classification is usually preferred, since the results
                        are mostly similar, but the runtime is significantly less.
                    </p>
                    <p>
                        For &#8220;one-vs-rest&#8221; <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></code></a> the attributes <code class="docutils literal"><span class="pre">coef_</span></code> and <code class="docutils literal"><span class="pre">intercept_</span></code>
                        have the shape <code class="docutils literal"><span class="pre">[n_class,</span> <span class="pre">n_features]</span></code> and <code class="docutils literal"><span class="pre">[n_class]</span></code> respectively.
                        Each row of the coefficients corresponds to one of the <code class="docutils literal"><span class="pre">n_class</span></code> many
                        &#8220;one-vs-rest&#8221; classifiers and similar for the intercepts, in the
                        order of the &#8220;one&#8221; class.
                    </p>
                    <p>
                        In the case of &#8220;one-vs-one&#8221; <a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="xref py py-class docutils literal"><span class="pre">SVC</span></code></a>, the layout of the attributes
                        is a little more involved. In the case of having a linear kernel,
                        The layout of <code class="docutils literal"><span class="pre">coef_</span></code> and <code class="docutils literal"><span class="pre">intercept_</span></code> is similar to the one
                        described for <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></code></a> described above, except that the shape of
                        <code class="docutils literal"><span class="pre">coef_</span></code> is <code class="docutils literal"><span class="pre">[n_class</span> <span class="pre">*</span> <span class="pre">(n_class</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">/</span> <span class="pre">2,</span> <span class="pre">n_features]</span></code>, corresponding to as
                        many binary classifiers. The order for classes
                        0 to n is &#8220;0 vs 1&#8221;, &#8220;0 vs 2&#8221; , ... &#8220;0 vs n&#8221;, &#8220;1 vs 2&#8221;, &#8220;1 vs 3&#8221;, &#8220;1 vs n&#8221;, . .
                        . &#8220;n-1 vs n&#8221;.
                    </p>
                    <p>
                        The shape of <code class="docutils literal"><span class="pre">dual_coef_</span></code> is <code class="docutils literal"><span class="pre">[n_class-1,</span> <span class="pre">n_SV]</span></code> with
                        a somewhat hard to grasp layout.
                        The columns correspond to the support vectors involved in any
                        of the <code class="docutils literal"><span class="pre">n_class</span> <span class="pre">*</span> <span class="pre">(n_class</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">/</span> <span class="pre">2</span></code> &#8220;one-vs-one&#8221; classifiers.
                        Each of the support vectors is used in <code class="docutils literal"><span class="pre">n_class</span> <span class="pre">-</span> <span class="pre">1</span></code> classifiers.
                        The <code class="docutils literal"><span class="pre">n_class</span> <span class="pre">-</span> <span class="pre">1</span></code> entries in each row correspond to the dual coefficients
                        for these classifiers.
                    </p>
                    <p>This might be made more clear by an example:</p>
                    <p>
                        Consider a three class problem with with class 0 having three support vectors
                        <img class="math" src="../_images/math/4b88f1d1d8196c7bbe4ae71a937f701fee59589f.png" alt="v^{0}_0, v^{1}_0, v^{2}_0" /> and class 1 and 2 having two support vectors
                        <img class="math" src="../_images/math/749ba150f0874a6aeb9951c4d3874994bb8ce7a8.png" alt="v^{0}_1, v^{1}_1" /> and <img class="math" src="../_images/math/55717c27d8dc9fc74a47ab9c34e12fdee568d9ba.png" alt="v^{0}_2, v^{1}_2" /> respectively.  For each
                        support vector <img class="math" src="../_images/math/ae16e9774b173bfa6489c04d4d7b581ff31a55a5.png" alt="v^{j}_i" />, there are two dual coefficients.  Let&#8217;s call
                        the coefficient of support vector <img class="math" src="../_images/math/ae16e9774b173bfa6489c04d4d7b581ff31a55a5.png" alt="v^{j}_i" /> in the classifier between
                        classes <img class="math" src="../_images/math/a581f053bbfa5115f42c13094857cdd12a37ec49.png" alt="i" /> and <img class="math" src="../_images/math/e9203da50e1059455123460d4e716c9c7f440cc3.png" alt="k" /> <img class="math" src="../_images/math/e26798dbf37644239db0d4660e7d514993e0c293.png" alt="\alpha^{j}_{i,k}" />.
                        Then <code class="docutils literal"><span class="pre">dual_coef_</span></code> looks like this:
                    </p>
                    <table border="1" class="docutils">
                        <colgroup>
                            <col width="36%" />
                            <col width="36%" />
                            <col width="27%" />
                        </colgroup>
                        <tbody valign="top">
                            <tr class="row-odd">
                                <td><img class="math" src="../_images/math/a1b925a4dbbe65450fbebb81572f6222a434b9a0.png" alt="\alpha^{0}_{0,1}" /></td>
                                <td><img class="math" src="../_images/math/2d68a3b7f2e7b7d869d407df04909c805063cca5.png" alt="\alpha^{0}_{0,2}" /></td>
                                <td rowspan="3">
                                    Coefficients
                                    for SVs of class 0
                                </td>
                            </tr>
                            <tr class="row-even">
                                <td><img class="math" src="../_images/math/8c43465f3d8574113c7e15638bbf2fae35ee3896.png" alt="\alpha^{1}_{0,1}" /></td>
                                <td><img class="math" src="../_images/math/83a6b3500cafea6d2bc19e8dc4550cfc77f8b04a.png" alt="\alpha^{1}_{0,2}" /></td>
                            </tr>
                            <tr class="row-odd">
                                <td><img class="math" src="../_images/math/f812188f63141acf67e13dca94938ea66c93d8b3.png" alt="\alpha^{2}_{0,1}" /></td>
                                <td><img class="math" src="../_images/math/132dd1872743cc6a09b7224a438f031de75f935d.png" alt="\alpha^{2}_{0,2}" /></td>
                            </tr>
                            <tr class="row-even">
                                <td><img class="math" src="../_images/math/c071d5b7948b79b8bd2232aae2939399a487a24c.png" alt="\alpha^{0}_{1,0}" /></td>
                                <td><img class="math" src="../_images/math/8efaf51c35183f8aa1a1c4ccaefd5a49b9dbef8c.png" alt="\alpha^{0}_{1,2}" /></td>
                                <td rowspan="2">
                                    Coefficients
                                    for SVs of class 1
                                </td>
                            </tr>
                            <tr class="row-odd">
                                <td><img class="math" src="../_images/math/f639c61c3322f48687bf08827b851de13d220c0d.png" alt="\alpha^{1}_{1,0}" /></td>
                                <td><img class="math" src="../_images/math/76fb561a7220221493e4887d6c19668c2c0009c2.png" alt="\alpha^{1}_{1,2}" /></td>
                            </tr>
                            <tr class="row-even">
                                <td><img class="math" src="../_images/math/1ec89a125496001eea33264978ca18b2f28136cb.png" alt="\alpha^{0}_{2,0}" /></td>
                                <td><img class="math" src="../_images/math/715efcbd1f7148dd4d8e1b3a6a9ea03d12907d8b.png" alt="\alpha^{0}_{2,1}" /></td>
                                <td rowspan="2">
                                    Coefficients
                                    for SVs of class 2
                                </td>
                            </tr>
                            <tr class="row-odd">
                                <td><img class="math" src="../_images/math/d637f33c37a5688da805b551083d16e7a674f7d5.png" alt="\alpha^{1}_{2,0}" /></td>
                                <td><img class="math" src="../_images/math/8ab6fd0263c1e67950fc82aacea0ec613e7d37ed.png" alt="\alpha^{1}_{2,1}" /></td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                <div class="section" id="scores-and-probabilities">
                    <span id="scores-probabilities"></span><h3>1.4.1.2. Scores and probabilities<a class="headerlink" href="#scores-and-probabilities" title="Permalink to this headline">¶</a></h3>
                    <p>
                        The <a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="xref py py-class docutils literal"><span class="pre">SVC</span></code></a> method <code class="docutils literal"><span class="pre">decision_function</span></code> gives per-class scores
                        for each sample (or a single score per sample in the binary case).
                        When the constructor option <code class="docutils literal"><span class="pre">probability</span></code> is set to <code class="docutils literal"><span class="pre">True</span></code>,
                        class membership probability estimates
                        (from the methods <code class="docutils literal"><span class="pre">predict_proba</span></code> and <code class="docutils literal"><span class="pre">predict_log_proba</span></code>) are enabled.
                        In the binary case, the probabilities are calibrated using Platt scaling:
                        logistic regression on the SVM&#8217;s scores,
                        fit by an additional cross-validation on the training data.
                        In the multiclass case, this is extended as per Wu et al. (2004).
                    </p>
                    <p>
                        Needless to say, the cross-validation involved in Platt scaling
                        is an expensive operation for large datasets.
                        In addition, the probability estimates may be inconsistent with the scores,
                        in the sense that the &#8220;argmax&#8221; of the scores
                        may not be the argmax of the probabilities.
                        (E.g., in binary classification,
                        a sample may be labeled by <code class="docutils literal"><span class="pre">predict</span></code> as belonging to a class
                        that has probability &lt;½ according to <code class="docutils literal"><span class="pre">predict_proba</span></code>.)
                        Platt&#8217;s method is also known to have theoretical issues.
                        If confidence scores are required, but these do not have to be probabilities,
                        then it is advisable to set <code class="docutils literal"><span class="pre">probability=False</span></code>
                        and use <code class="docutils literal"><span class="pre">decision_function</span></code> instead of <code class="docutils literal"><span class="pre">predict_proba</span></code>.
                    </p>
                    <div class="topic">
                        <p class="topic-title first">References:</p>
                        <ul class="simple">
                            <li>
                                Wu, Lin and Weng,
                                <a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf">&#8220;Probability estimates for multi-class classification by pairwise coupling&#8221;</a>.
                                JMLR 5:975-1005, 2004.
                            </li>
                        </ul>
                    </div>
                </div>
                <div class="section" id="unbalanced-problems">
                    <h3>1.4.1.3. Unbalanced problems<a class="headerlink" href="#unbalanced-problems" title="Permalink to this headline">¶</a></h3>
                    <p>
                        In problems where it is desired to give more importance to certain
                        classes or certain individual samples keywords <code class="docutils literal"><span class="pre">class_weight</span></code> and
                        <code class="docutils literal"><span class="pre">sample_weight</span></code> can be used.
                    </p>
                    <p>
                        <a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="xref py py-class docutils literal"><span class="pre">SVC</span></code></a> (but not <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code class="xref py py-class docutils literal"><span class="pre">NuSVC</span></code></a>) implement a keyword
                        <code class="docutils literal"><span class="pre">class_weight</span></code> in the <code class="docutils literal"><span class="pre">fit</span></code> method. It&#8217;s a dictionary of the form
                        <code class="docutils literal"><span class="pre">{class_label</span> <span class="pre">:</span> <span class="pre">value}</span></code>, where value is a floating point number &gt; 0
                        that sets the parameter <code class="docutils literal"><span class="pre">C</span></code> of class <code class="docutils literal"><span class="pre">class_label</span></code> to <code class="docutils literal"><span class="pre">C</span> <span class="pre">*</span> <span class="pre">value</span></code>.
                    </p>
                    <div class="figure align-center">
                        <a class="reference external image-reference" href="../auto_examples/svm/plot_separating_hyperplane_unbalanced.html"><img alt="../_images/plot_separating_hyperplane_unbalanced_0011.png" src="../_images/plot_separating_hyperplane_unbalanced_0011.png" style="width: 600.0px; height: 450.0px;" /></a>
                    </div>
                    <p>
                        <a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="xref py py-class docutils literal"><span class="pre">SVC</span></code></a>, <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><code class="xref py py-class docutils literal"><span class="pre">NuSVC</span></code></a>, <a class="reference internal" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code class="xref py py-class docutils literal"><span class="pre">SVR</span></code></a>, <a class="reference internal" href="generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR" title="sklearn.svm.NuSVR"><code class="xref py py-class docutils literal"><span class="pre">NuSVR</span></code></a> and
                        <a class="reference internal" href="generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code class="xref py py-class docutils literal"><span class="pre">OneClassSVM</span></code></a> implement also weights for individual samples in method
                        <code class="docutils literal"><span class="pre">fit</span></code> through keyword <code class="docutils literal"><span class="pre">sample_weight</span></code>. Similar to <code class="docutils literal"><span class="pre">class_weight</span></code>, these
                        set the parameter <code class="docutils literal"><span class="pre">C</span></code> for the i-th example to <code class="docutils literal"><span class="pre">C</span> <span class="pre">*</span> <span class="pre">sample_weight[i]</span></code>.
                    </p>
                    <div class="figure align-center">
                        <a class="reference external image-reference" href="../auto_examples/svm/plot_weighted_samples.html"><img alt="../_images/plot_weighted_samples_0011.png" src="../_images/plot_weighted_samples_0011.png" style="width: 1050.0px; height: 450.0px;" /></a>
                    </div>
                    <div class="topic">
                        <p class="topic-title first">Examples:</p>
                        <ul class="simple">
                            <li><a class="reference internal" href="../auto_examples/svm/plot_iris.html#example-svm-plot-iris-py"><span>Plot different SVM classifiers in the iris dataset</span></a>,</li>
                            <li><a class="reference internal" href="../auto_examples/svm/plot_separating_hyperplane.html#example-svm-plot-separating-hyperplane-py"><span>SVM: Maximum margin separating hyperplane</span></a>,</li>
                            <li><a class="reference internal" href="../auto_examples/svm/plot_separating_hyperplane_unbalanced.html#example-svm-plot-separating-hyperplane-unbalanced-py"><span>SVM: Separating hyperplane for unbalanced classes</span></a></li>
                            <li><a class="reference internal" href="../auto_examples/svm/plot_svm_anova.html#example-svm-plot-svm-anova-py"><span>SVM-Anova: SVM with univariate feature selection</span></a>,</li>
                            <li><a class="reference internal" href="../auto_examples/svm/plot_svm_nonlinear.html#example-svm-plot-svm-nonlinear-py"><span>Non-linear SVM</span></a></li>
                            <li><a class="reference internal" href="../auto_examples/svm/plot_weighted_samples.html#example-svm-plot-weighted-samples-py"><span>SVM: Weighted samples</span></a>,</li>
                        </ul>
                    </div>
                </div>

            </div>


            <div class="section" id="regression">
                <h2>1.4.2. 回归(Regression)<a>¶</a></h2>

            </div>

            <div class="section" id="density-estimation-novelty-detection">
                <h2>1.4.3. 密度估计，奇异检测<a>¶</a></h2>

            </div>

            <div class="section" id="complexity">
                <h2>1.4.4. 复杂度<a>¶</a></h2>

            </div>

            <div class="section" id="tips-on-practical-use">
                <h2>1.4.5. 应用实践指南<a>¶</a></h2>

            </div>

            <div class="section" id="kernel-functions">
                <h2>1.4.6. 核函数<a>¶</a></h2>

            </div>

            <div class="section" id="mathematical-formulation">
                <h2>1.4.7. 数学表述<a>¶</a></h2>

            </div>

            <div class="section" id="implementation-details">
                <h2>1.4.8. 实现细节<a>¶</a></h2>

            </div>

        </div>

    </div>
    
    
    <hr />
    <div class="container">
        <div class="footer text-center">
            &copy; 2016-2100, 版权属于张金明博士 (BSD License).
        </div>
    </div>

</body>
</html>
