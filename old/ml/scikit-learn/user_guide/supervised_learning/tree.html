<!DOCTYPE html>
<html lang="zh-cn">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <link rel="stylesheet" href="../../../../css-fonts-js/css/bootstrap.min.css" />
    <!--<link rel="stylesheet" href="../../../../css-fonts-js/css/nature.css" />-->
    <link rel="stylesheet" href="../../../../css-fonts-js/css/pygments.css" />

    <style>
        p {
            font-size: large;
        }

        li {
            font-size: large;
        }
    </style>

    <script src="../../../../css-fonts-js/js/jquery.min.js"></script>
    <script src="../../../../css-fonts-js/js/jquery.js"></script>
    <script src="../../../../css-fonts-js/js/bootstrap.min.js"></script>


    <title>人工智能研究网</title>
    <!--网站访问量统计-->
    <script>
        var _hmt = _hmt || [];
        (function () {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?e7836e37a4cb7584127a787e9b44e3f1";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>
</head>


<body>

    <nav class="navbar navbar-inverse">
        <div class="container">
            <div class="navbar-header">
                <a class="navbar-brand" href="../../../../index.html">studyai.cn</a>
            </div>
            <ul class="nav navbar-nav">
                <li class="active"><a href="../../../../index.html">网站主页</a></li>
                <li><a href="../../../../webmaster.html">站长风采</a></li>
                <li><a href="../../../../sponsor.html">赞助我们</a></li>
                <li><a href="../../../index.html">机器学习主页</a></li>
            </ul>
        </div>
    </nav>

    <div class="container">

        <div class="section" id="support-vector-machines">
            <h1 class="page-header">1.10 决策树(Decision Trees)<a>¶</a></h1>
            <p>
                决策树<strong>Decision Trees (DTs)</strong> 是一类用于<a href="#classification">分类</a>
                和<a href="#regression">回归</a>的非参数监督学习算法。目标是创建一个模型通过从数据中学习一些简单的
                决策规则来预测目标变量的值。
            </p>
            <p>
                比如在下面的例子中，决策树从数据中学习一系列if-then-else决策规则去逼近一个正弦曲线。树的深度越深，决策规则
                越复杂，模型拟合的也越精确。
            </p>
            <p class="text-center">
                <img src="images/plot_tree_regression_0011.png" />
            </p>
            <p>
                决策树的优点有以下几个方面：
            </p>
            <blockquote>
                <div>
                    <ul class="simple">
                        <li>易于理解和解释。树能够被可视化。</li>
                        <li>
                            需要很少的数据准备工作。其他的学习机通常需要数据归一化、虚拟变量创建以及移除空白值(blank values)等预处理操作。
                            然而，需要注意的是 此模型 不支持 缺失值(missing values)。
                        </li>
                        <li>
                            使用决策树进行数据预测的代价是与训练树所用的数据点的数量呈对数关系的。
                        </li>
                        <li>
                            能够同时处理数值数据(numerical data)和类别数据(categorical data)。其他学习机技术通常被限定于
                            分析只有一种类型变量的数据集。更多详情请看<a href="#tree-algorithms"><span>决策树算法</span></a>。
                        </li>
                        <li>能够处理多输出问题。</li>
                        <li>
                            使用白盒模型(white box model)。如果某个给定的情形在模型中被观测到了,那么
                            对于条件的解释可以使用布尔逻辑很轻松的实现。
                            作为对比, 在黑盒模型(black box model)中 (例如在人工神经网络中), 
                            结果可能非常难于解释。
                        </li>
                        <li>
                            可以使用统计测试来验证模型，这使得我们能够对模型的可靠性(reliability)作出评估。 
                        </li>
                        <li>
                            即使当它的某些假设稍微违背了数据集的真实模型，决策树也可以很好的工作。
                        </li>
                    </ul>
                </div>
            </blockquote>
            <p>
                决策树的缺点有以下几个方面：
            </p>
            <blockquote>
                <div>
                    <ul class="simple">
                        <li>
                            决策树学习器可能会创造过于复杂的树使得对数据的泛化能力下降。这被称之为<mark>过拟合(overfitting)</mark>。
                            有一些机制比如剪枝操作、设置叶节点的最小样本量需求或者限制树的最大深度来避免过拟合发生。
                        </li>
                        <li>
                            数据集中的数据的细微变化可能会导致产生截然不同的树出来，所以决策树存在不稳定性。
                            解决不稳定性的一种方法是使用集成方法将多个决策树组织起来。
                        </li>
                        <li>
                            学习最优决策树的问题已经被证明是一个NP-完全问题，即使是某些比较简单的情形下获得最优解也非常困难。
                            因此，能够实际应用的决策树学习算法一般都是基于启发式优化方法的，比如使用贪心算法可以找到一个在叶节点上
                            进行局部最优决策的决策树。 
                        </li>
                        <li>
                            决策树很难学习和表达某些概念比如异或、奇偶校验和多路复用等问题。
                        </li>
                        <li>
                            如果某些类起了主导作用，那么决策树学习器将创建一个有偏树(biased trees)。
                            因此，强烈建议数据集中各类的先验概率应该做到平衡以便拟合出一个更加平衡的决策树。
                        </li>
                    </ul>
                </div>
            </blockquote>
            <div class="section" id="classification">
                <h2>1.10.1 分类(Classification)<a>¶</a></h2>
                <p>
                    <code>DecisionTreeClassifier</code> 是一个能够执行多类别分类任务的类。就像其他分类器一样，
                    <code>DecisionTreeClassifier</code>类的成员函数<code>fit</code>接受两个参数：一个是用于存放训练样本的
                    数组X,X可以是稀疏的或稠密的，其大小为<mark>[n_samples, n_features]</mark>； 另外一个参数是训练样本的类别
                    标签数组Y，Y是整型数组，大小为<mark>[n_samples]</mark>
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre>
                    </div>
                </div>
                <p>
                    当模型拟合好以后，我们就可以用它来预测样本类别了。
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>
<span class="go">array([1])</span>
</pre>
                    </div>
                </div>
                <p>
                    更进一步，我们还可以获得被预测的样本属于各个类的概率值。此概率值是叶节点上同一类的训练样本的比例分数。
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>
<span class="go">array([[ 0.,  1.]])</span>
</pre>
                    </div>
                </div>
                <p>
                    <code>DecisionTreeClassifier</code> 即能够处理二分类问题(其中的标签只有 -1 和 1 两种); 
                    也能够处理多类分类问题(其中标签集合为 [0, ..., K-1]).
                </p>
                <p>
                    我们可以使用iris数据集按如下的方式构造决策树分类器：
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre>
                    </div>
                </div>
                <p>
                    一旦决策树被训练好后，我们可以用export_graphviz将树以
                    <a href="http://www.graphviz.org" target="_blank">Graphviz</a>
                    的格式导出。下面的例子展示了如何将 iris数据集上训练得到的树导出来：
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.externals.six</span> <span class="kn">import</span> <span class="n">StringIO</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;iris.dot&quot;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">f</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">export_graphviz</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">out_file</span><span class="o">=</span><span class="n">f</span><span class="p">)</span>
</pre>
                    </div>
                </div>
                <p>
                    然后，我们就使用Graphviz的节点工具创建个PDF文件：<code>dot -Tpdf iris.dot -o iris.pdf</code>。
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">os</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">os</span><span class="o">.</span><span class="n">unlink</span><span class="p">(</span><span class="s1">&#39;iris.dot&#39;</span><span class="p">)</span>
</pre>
                    </div>
                </div>
                <p>
                    或者，如果我们安装了Python模块<code>pydot</code>，我们就能够直接在Python中创建pdf文件：
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.externals.six</span> <span class="kn">import</span> <span class="n">StringIO</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pydot</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">dot_data</span> <span class="o">=</span> <span class="n">StringIO</span><span class="p">()</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">tree</span><span class="o">.</span><span class="n">export_graphviz</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">out_file</span><span class="o">=</span><span class="n">dot_data</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">graph</span> <span class="o">=</span> <span class="n">pydot</span><span class="o">.</span><span class="n">graph_from_dot_data</span><span class="p">(</span><span class="n">dot_data</span><span class="o">.</span><span class="n">getvalue</span><span class="p">())</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">graph</span><span class="o">.</span><span class="n">write_pdf</span><span class="p">(</span><span class="s2">&quot;iris.pdf&quot;</span><span class="p">)</span> 
</pre>
                    </div>
                </div>
                <p>
                    <code class="xref py py-func docutils literal"><span class="pre">export_graphviz</span></code> 
                    也支持各种各样的美化选项，包括带有颜色的类节点，显式变量和类名。
                    IPython notebooks 也可以在其内部显示这些绘图，通过使用<cite>Image()</cite> 函数:
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">dot_data</span> <span class="o">=</span> <span class="n">StringIO</span><span class="p">()</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">tree</span><span class="o">.</span><span class="n">export_graphviz</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">out_file</span><span class="o">=</span><span class="n">dot_data</span><span class="p">,</span>  
<span class="go">                         feature_names=iris.feature_names,  </span>
<span class="go">                         class_names=iris.target_names,  </span>
<span class="go">                         filled=True, rounded=True,  </span>
<span class="go">                         special_characters=True)  </span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">graph</span> <span class="o">=</span> <span class="n">pydot</span><span class="o">.</span><span class="n">graph_from_dot_data</span><span class="p">(</span><span class="n">dot_data</span><span class="o">.</span><span class="n">getvalue</span><span class="p">())</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">Image</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">create_png</span><span class="p">())</span>  
</pre>
                    </div>
                </div>
                <p class="text-center">
                    <img src="./images/iris.svg" />
                </p>
                <p>当决策树被拟合好以后，就可以用来预测样本类别啦：</p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>
<span class="go">array([0])</span>
</pre>
                    </div>
                </div>
                <p>
                    更进一步，我们还可以获得被预测的样本属于各个类的概率值。此概率值是叶节点上同一类的训练样本的比例分数。
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>
<span class="go">array([[ 1.,  0.,  0.]])</span>
</pre>
                    </div>
                </div>
                <p class="text-center">
                    <img src="images/plot_iris_0013.png" />
                </p>
                <div class="well well-sm">
                    <p>
                        例子:<a>绘制鸢尾花数据集上的决策树分类判决界面</a>
                    </p> 
                </div>

            </div>

            <div class="section" id="regression">
                <h2>1.10.2 回归(Regression)<a>¶</a></h2>
                <p class="text-center">
                    <img alt="./images/plot_tree_regression_0011.png" src="./images/plot_tree_regression_0011.png" style="width: 600.0px; height: 450.0px;" />
                </p>
                <p>
                    决策树也可以用于解决回归问题，此时要使用<code>DecisionTreeRegressor</code>类。
                </p>
                <p>
                    就像在分类问题中的设置一样，<code>DecisionTreeRegressor</code>类的<code>fit</code>函数也接受
                    两个参数：X和y。但是，在回归问题中，目标变量y不再是整型的而是浮点型的连续变量。 
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeRegressor</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="go">array([ 0.5])</span>
</pre>
                    </div>
                </div>
                <div class="well well-sm">
                    <p>
                        例子:<a>决策树回归应用</a>
                    </p>
                </div>

            </div>

            <div class="section" id="multi-output-problems">
                <h2>1.10.3 多输出问题<a>¶</a></h2>
                <p>
                    多输出问题(multi-output problem)也是监督学习问题，它有多个输出需要预测。此时，目标变量Y是一个
                    大小为<code>[n_samples,n_outputs]</code>的2维数组。
                </p>
                <p>
                    当多个输出之间没有相关性的时候，解决此问题的一个简单办法就是为每一个输出建立一个独立于其他输出的
                    模型，然后对于每一个输出的预测就可以独立的使用其对应的模型。
                    然而，很多情况下多个输出变量都依赖于相同的输入变量，这就会导致这些依赖于相同输入的输出变量之间
                    存在相关性。这时候最好的做法通常是构建单个模型对多个输出进行同时预测。首先，因为我们只建立了一个
                    单模型，所以需要的训练时间就比较少。其次，单模型估计器的泛化精度通常比较高。
                </p>
                <p>
                    对于决策树来说，这种策略可以很容易地被用来支持多输出的问题。这时候，我们需要做以下改变：
                </p>
                <blockquote>
                    <div>
                        <ul class="simple">
                            <li>在叶子中存储n个输出值，而不是1个;</li>
                            <li>
                                使用分裂准则(splitting criteria)来计算所有n个输出上的平均缩减(average reduction)。
                            </li>
                        </ul>
                    </div>
                </blockquote>
                <p>
                    scikit-learn模块在<code>DecisionTreeClassifier</code>类和<code>DecisionTreeRegressor</code>类中
                    实现了上述策略来支持多输出问题。如果我们在一个大小为<code>[n_samples,n_outputs]</code>的多变量输出数组上拟合了
                    一个决策树模型，那么产生的估计器模型的输出具有下述特点：
                </p>
                <blockquote>
                    <div>
                        <ul class="simple">
                            <li>函数<code>predict</code>的输出具有n_output个值;</li>
                            <li>
                                函数<code>predict_proba</code>的输出是一个列表，包含了n_output个类概率数组。
                            </li>
                        </ul>
                    </div>
                </blockquote>
                <p>
                    <a >多输出决策树回归的例子</a>. 在这个例中，输入X是单精度实数，输出Y是X的正弦和余弦。 
                </p>
                <p class="text-center">
                    <img src="images/plot_tree_regression_multioutput_0011.png" />
                </p>
                <p>
                    多输出决策树用于分类的例子<a>人脸补全</a>. 在这个例中，
                    输入X是上半张脸的像素，而输出Y是对应的下半张脸。
                </p>
                <p class="text-center">
                    <img src="images/plot_multioutput_face_completion_0011.png" width="750" height="847" />
                </p>
                <div class="well well-sm">
                    <p>
                        例子:<a>多输出决策树回归：正余弦函数拟合</a>；<a>多输出决策树分类：人脸补全</a>
                    </p>
                </div>
                <div class="well well-sm">
                    <p>
                        参考文献:
                            M. Dumont et al,  <a class="reference external" href="http://www.montefiore.ulg.ac.be/services/stochastic/pubs/2009/DMWG09/dumont-visapp09-shortpaper.pdf">
                            Fast multi-class image annotation with random subwindows and multiple output randomized trees
                            </a>, International Conference on Computer Vision Theory and Applications 2009
                    </p>
                </div>
            </div>

            <div class="section" id="complexity">
                <h2>1.10.4 复杂度<a>¶</a></h2>

            </div>

            <div class="section" id="tips-on-practical-use">
                <h2>1.10.5 应用实践指南<a>¶</a></h2>
                <blockquote>
                    <div>
                        <ul>
                            <li>
                                决策树具有使用大量特征过拟合数据的倾向。由于空间高维但样本量少会使得决策树过拟合，
                                所以确保样本数量与特征数量的比值处在合理区间是很重要的。
                            </li>
                            <li>
                                考虑使用维数约简算法（例如：PCA,ICA,或特征选择方法）为你的决策树选择具有辨别力的特征。
                            </li>
                            <li>
                                通过使用<code>export</code>函数在训练过程中可视化你的树。选择<code>max_depth=3</code>作为
                                初始树的深度，然后找到初始树在拟合数据上的表现，根据这一表现再不断加深树的深度。
                            </li>
                            <li>
                                Remember that the number of samples required to populate the tree doubles for each
                                 additional level the tree grows to.
                                 使用<code>max_depth</code>来控制树的深度以防止过拟合。
                            </li>
                            <li>
                                使用<code>min_samples_split</code>和<code>min_samples_leaf</code>来控制叶节点的样本容量。
                                当叶节点上的样本容量非常小的时候往往意味着过拟合了；然而，样本容量太大又会妨碍决策树从数据中学习。
                                尝试以<code>min_samples_leaf=5</code>作为一个初始值。它们之间的主要不同在于<code>min_samples_leaf</code>
                                确保了叶节点上的最小样本量，而<code>min_samples_split</code>可以创建任意小的叶节点，尽管min_samples_split
                                在文献中更加的常见。
                            </li>
                            <li>
                                为了防止决策树偏向那些主导类，在训练之前要让数据集中不同类的样本容量之间的比例保持平衡。这可以通过为所有类获取
                                相同数量的样本做到；或者也可以通过归一化样本权重(sample_weight)到相同的值来做到。
                                另外，必须注意到加权预剪枝准则比如<code>min_weight_fraction_leaf</code>与那些没有意识到样本权重的准则比如
                                <code>min_samples_leaf</code>进行比较的话，前者能够确保更少的偏向那些主导类。
                            </li>
                            <li>
                                如果样本被加权，使用基于权重的预剪枝准则比如<code>min_weight_fraction_leaf</code>来优化树结构将会非常简单。
                                <code>min_weight_fraction_leaf</code>准则确保叶节点包含整体样本权重的一个最小规定比例。
                            </li>
                            <li>
                                所有决策树在其内部使用<code>np.float32</code>型的数组。如果训练数据不是此格式，将会copy一个数据集的副本。
                            </li>
                            <li>
                                如果输入矩阵X非常稀疏，建议你在进行模型拟合前将其转换为稀疏矩阵：<code>csc_matrix</code>；
                                在做预测前，转为<code>csr_matrix</code>矩阵。相比稠密矩阵，在有大量零元素的稀疏输入矩阵上进行训练可以
                                加快一个量级的训练时间。
                            </li>
                        </ul>
                    </div>
                </blockquote>
            </div>

            <div class="section" id="tree-algorithms-id3-c4-5-c5-0-and-cart">
                <h2 id="tree-algorithms">1.10.6. 决策树算法: ID3, C4.5, C5.0 和 CART<a>¶</a></h2>
                <p>
                    决策树算法的变种都有哪些呢？他们的区别在哪里？<strong>scikit-learn</strong>实现的是哪一种呢？
                </p>
                <p>
                    <a href="https://en.wikipedia.org/wiki/ID3_algorithm" target="_blank">ID3</a> 
                    (Iterative Dichotomiser 3) 在1986年由Ross Quinlan开发出来。 ID3算法创建一个多路树
                    (multiway tree), 以贪婪的方式为每个节点寻找分类特征以使得分类后的信息增益达到最大。
                    所有树都不断增长已达到他们的最大深度，然后使用剪枝操作来提升树在未知数据上的泛化能力。
                </p>
                <p>
                    <strong>C4.5</strong>是ID3算法的后继者。它通过动态定义离散属性(基于数值变量，将这些连续的属性变量
                    离散化成整型的离散集合)去除了特征必须是按类别的这一限制条件。 C4.5 将训练好的树(即ID3的输出)都转化成
                    <mark>if-then</mark> 决策规则。这些决策规则的准确性随后被评估出来并进一步决定这些规则被预测过程使用的顺序。 
                     Pruning is done by removing a rule’s precondition if the accuracy of the rule improves without it.
                </p>
                <p>
                    C5.0 是 Quinlan 的一个最新版本，受专利保护。 与C4.5相比，它使用更少的内存并且构建更小的决策集合但是依然能够达到
                    更高的准确度。
                </p>
                <p>
                    CART (Classification and Regression Trees) 与C4.5非常相似, 它们的区别在于CART支持数值变量(用于回归)
                    而且不产生显式的决策规则集合。CART在每一个节点上使用能够产生最大信息增益的特征和阈值构造二叉树。
                </p>
                <p>
                    <strong>scikit-learn</strong> 使用 <strong>CART</strong> 算法的优化版本.
                </p>
            </div>

            <div class="section" id="mathematical-formulation">
                <h2>1.10.7. 数学表述<a>¶</a></h2>
                <p>
                    给定训练向量集合<img class="math" src="./images/110fea905d9ed71ce598e82f51bf26a1661f8f5c.png" alt="x_i \in R^n" />, i=1,..., l 
                    和类标签集合<img class="math" src="./images/1ba3ffd927bbc923681949691879142eb7ddcb06.png" alt="y \in R^l" />, 
                    一个决策树递归的划分空间使得具有相同标签的样本被划分到同一类中。
                </p>
                <p>
                    让<img class="math" src="./images/7b1816c51f7d31275cd3ad400208fb7b3ce136a0.png" alt="Q" />
                    来表示节点<img class="math" src="./images/c4bb40dd65eae6c11b325989b14e0b8d35e4e3ef.png" alt="m" />
                    上的数据。对每一个由特征<img class="math" src="./images/d32c78b759903e3f4bd4fd2ce0b86358f7500c5d.png" alt="j" />
                    和阈值<img class="math" src="./images/168b41d971e0e2e6d9bf8b198eccfe76a5d71030.png" alt="t_m" />所构成的候选划分
                    <img class="math" src="./images/9d759be8e66ed0a164c0a65eb31389b7abf6de49.png" alt="\theta = (j, t_m)" />，
                    将数据Q划分成两个子集：
                    <img class="math" src="./images/e68adc400e8570b0f58d78c16290ba9bf84f0298.png" alt="Q_{left}(\theta)" /> 和
                     <img class="math" src="./images/77e715cd935ec58a6793c7282b25172264dde14b.png" alt="Q_{right}(\theta)" />。
                </p>
                <div class="math">
                    <p class="text-center">
                        <img src="./images/1376e8e59ba3b59ff5f4f9e7dd475684017ea210.png" alt="Q_{left}(\theta) = {(x, y) | x_j &lt;= t_m} Q_{right}(\theta) = Q \setminus Q_{left}(\theta)" />
                    </p>
                </div>
                <p>
                    在<img class="math" src="./images/c4bb40dd65eae6c11b325989b14e0b8d35e4e3ef.png" alt="m" />
                    节点的impurity(杂质)通过使用一个 impurity 函数
                    <img class="math" src="./images/7d6e812752f434e91c490703e9983f319610933c.png" alt="H()" />进行计算, 
                    具体选择哪个依赖于我们要求解的任务：回归 或 决策。
                </p>
                <div class="math">
                    <p class="text-center">
                        <img src="./images/7aaed5ba95d79fa52da0d2596f76ebdc25e8893f.png" alt="G(Q, \theta) = \frac{n_{left}}{N_m} H(Q_{left}(\theta))+ \frac{n_{right}}{N_m} H(Q_{right}(\theta))" />
                    </p>
                </div>
                <p>通过选择参数使得impurity最小化：</p>
                <div class="math">
                    <p class="text-center"><img src="./images/5d5c5d759bbce0758c8ad4e6a6d27fb6539b597e.png" alt="\theta^* = \operatorname{argmin}_\theta  G(Q, \theta)" /></p>
                </div>
                <p>
                    在子集<img class="math" src="./images/d93300e24558ddc7c23e5c2633c7f13605a41203.png" alt="Q_{left}(\theta^*)" />
                    和<img class="math" src="./images/66a9bcb2fb38a49967dec10ad4d7cdded003ee7d.png" alt="Q_{right}(\theta^*)" />上递归
                    直到达到规定的最大深度：
                    <img class="math" src="./images/dcf8a6d5a34c9ad0b9b22f7abf78053e907e0729.png" alt="N_m &lt; \min_{samples}" /> or
                    <img class="math" src="./images/a31baaf9fe13c9b90c5a8091bcd00e981cd8a49f.png" alt="N_m = 1" />.
                </p>

                <div class="section" id="classification-criteria">
                    <h3>1.10.7.1. 分类准则 <a>¶</a></h3>
                    <p>
                        如果目标是分类输出标签值0,1,...,K-1,
                        对节点<img class="math" src="./images/c4bb40dd65eae6c11b325989b14e0b8d35e4e3ef.png" alt="m" />,使用
                        <img class="math" src="./images/c4a8a2957d6d5970fef35cce3e86d31427eaa216.png" alt="N_m" />
                        个观测来表达一个区域<img class="math" src="./images/23fee98757d811d8b5ade0fd876948f60f7806d9.png" alt="R_m" />。
                        让：
                    </p>
                    <div class="math">
                        <p class="text-center"><img src="./images/ebb2cff7b67f3b8c4e06405d8385983efac9659c.png" alt="p_{mk} = 1/ N_m \sum_{x_i \in R_m} I(y_i = k)" /></p>
                    </div>
                    <p>
                        为类k的观测在节点<img class="math" src="./images/c4bb40dd65eae6c11b325989b14e0b8d35e4e3ef.png" alt="m" />
                        上的比例。
                    </p>
                    <p>对impurity的常见度量有三种：Gini</p>
                    <div class="math">
                        <p class="text-center"><img src="./images/6227164813ffb600135aa9f69b90000ace19ba1d.png" alt="H(X_m) = \sum_k p_{mk} (1 - p_{mk})" /></p>
                    </div><p>Cross-Entropy</p>
                    <div class="math">
                        <p class="text-center"><img src="./images/f87532cfb621c0b1934429a5c0987a259ba01364.png" alt="H(X_m) = - \sum_k p_{mk} \log(p_{mk})" /></p>
                    </div>
                    <p>和 Misclassification</p>
                    <div class="math">
                        <p class="text-center"><img src="./images/73565e0e02602c9b442a103dc229e5024e37ad67.png" alt="H(X_m) = 1 - \max(p_{mk})" /></p>
                    </div>
                </div>

                <div class="section" id="regression-criteria">
                    <h3>1.10.7.2. 回归准则<a>¶</a></h3>
                    <p>
                        如果要预测的目标变量是连续值，那么对于节点
                        <img class="math" src="./images/c4bb40dd65eae6c11b325989b14e0b8d35e4e3ef.png" alt="m" />,
                        使用<img class="math" src="./images/c4a8a2957d6d5970fef35cce3e86d31427eaa216.png" alt="N_m" />观测来表达
                        一个区域<img class="math" src="./images/23fee98757d811d8b5ade0fd876948f60f7806d9.png" alt="R_m" />。则，普遍适用的
                        准则是最小化均方误差(Mean Squared Error)。
                    </p>
                    <div class="math">
                        <p class="text-center">
                            <img src="./images/216499b7b74994c090829dbb0e24bfd8b8d177fc.png" alt="c_m = \frac{1}{N_m} \sum_{i \in N_m} y_iH(X_m) = \frac{1}{N_m} \sum_{i \in N_m} (y_i - c_m)^2" />
                        </p>
                    </div><div class="well well-sm">
                        <p >参考文献:</p>
                        <ul class="simple">
                            <li><a class="reference external" href="http://en.wikipedia.org/wiki/Decision_tree_learning">http://en.wikipedia.org/wiki/Decision_tree_learning</a></li>
                            <li><a class="reference external" href="http://en.wikipedia.org/wiki/Predictive_analytics">http://en.wikipedia.org/wiki/Predictive_analytics</a></li>
                            <li>
                                L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and
                                Regression Trees. Wadsworth, Belmont, CA, 1984.
                            </li>
                            <li>J.R. Quinlan. C4. 5: programs for machine learning. Morgan Kaufmann, 1993.</li>
                            <li>
                                T. Hastie, R. Tibshirani and J. Friedman.
                                Elements of Statistical Learning, Springer, 2009.
                            </li>
                        </ul>
                    </div>
                </div>

            </div>

        </div>

    </div>


    <hr />
    <div class="container">
        <div class="footer text-center">
            &copy; 2016-2100, 版权属于张金明博士 (BSD License).
        </div>
    </div>

</body>
</html>
