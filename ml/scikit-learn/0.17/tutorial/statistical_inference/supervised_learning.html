
<!DOCTYPE HTML PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
          "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />


    <title>监督学习: 从高维观测数据预测一个输出变量 &mdash; scikit-learn 0.17.1 文档</title>
    <!-- htmltitle is before nature.css - we use this hack to load bootstrap first -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" href="../../_static/css/bootstrap.min.css" media="screen" />
    <link rel="stylesheet" href="../../_static/css/bootstrap-responsive.css" />


    <link rel="stylesheet" href="../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/gallery.css" type="text/css" />

    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     '0.17.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../_static/js/copybutton.js"></script>
    <link rel="shortcut icon" href="../../_static/favicon.ico" />
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="top" title="scikit-learn 0.17.1 文档" href="../../index.html" />
    <link rel="up" title="用于科学数据处理的统计学习教程" href="index.html" />
    <link rel="next" title="模型选择: 选择 estimators 以及其参数" href="model_selection.html" />
    <link rel="prev" title="统计学习: 环境配置 与 estimator 对象" href="settings.html" />


    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script src="../../_static/js/bootstrap.min.js" type="text/javascript"></script>
    <link rel="canonical" href="http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html" />

    <script type="text/javascript">
    $("div.buttonNext, div.buttonPrevious").hover(
       function () {
           $(this).css('background-color', '#FF9C34');
       },
       function () {
           $(this).css('background-color', '#A7D6E2');
       }
    );
    </script>

</head>
<body role="document">

    <div class="header-wrapper">
        <div class="header">
            <p class="logo">
                <a href="../../index.html">
                    <img src="../../_static/scikit-learn-logo-small.png" alt="Logo" />
                </a>
            </p><div class="navbar">
                <ul>
                    <li><a href="../../index.html">主页</a></li>
                    <li><a href="../../install.html">安装</a></li>
                    <li class="btn-li">
                        <div class="btn-group">
                            <a href="../../documentation.html">文档</a>
                            <a class="btn dropdown-toggle" data-toggle="dropdown">
                                <span class="caret"></span>
                            </a>
                            <ul class="dropdown-menu">
                                <li class="link-title">Scikit-learn 0.17 (稳定版)</li>
                                <li><a href="../index.html">教程</a></li>
                                <li><a href="../../user_guide.html">用户指南</a></li>
                                <li><a href="../../modules/classes.html">API</a></li>
                                <li><a href="../../faq.html">FAQ</a></li>
                                <li><a href="../../developers/contributing.html">贡献</a></li>
                                <li class="divider"></li>
                                <li><a href="http://scikit-learn.org/dev/documentation.html">Scikit-learn 0.18 (开发中)</a></li>
                                <li><a href="http://scikit-learn.org/0.16/documentation.html">Scikit-learn 0.16</a></li>
                                <li><a href="../../_downloads/scikit-learn-docs.pdf">PDF 文档</a></li>
                            </ul>
                        </div>
                    </li>
                    <li><a href="../../auto_examples/index.html">实例</a></li>
                </ul>

                <div class="search_form">
                    <div id="cse" style="width: 100%;"></div>
                </div>
            </div> <!-- end navbar -->
        </div>
    </div>


    <!-- Github "fork me" ribbon -->
    <a href="https://github.com/scikit-learn/scikit-learn">
        <img class="fork-me"
             style="position: absolute; top: 0; right: 0; border: 0;"
             src="../../_static/img/forkme.png"
             alt="Fork me on GitHub" />
    </a>

    <div class="content-wrapper">
        <div class="sphinxsidebar">
            <div class="sphinxsidebarwrapper">
                <div class="rel">


                    <!-- rellinks[1:] is an ugly hack to avoid link to module
                    index -->
                    <div class="rellink">
                        <a href="settings.html"
                           accesskey="P">
                            前一页
                            <br />
                            <span class="smallrellink">
                                统计机器学习教程
                            </span>
                            <span class="hiddenrellink">
                                统计机器学习:环境配置与estimator对象
                            </span>
                        </a>
                    </div>

                    <!-- Ad a link to the 'up' page -->
                    <div class="spacer">
                        &nbsp;
                    </div>
                    <div class="rellink">
                        <a href="index.html">
                            上一页
                            <br />
                            <span class="smallrellink">
                                教程
                            </span>
                            <span class="hiddenrellink">
                                用于科学数据处理的统计机器学习教程
                            </span>

                        </a>
                    </div>
                </div>

                <p class="doc-version">此文档适用于 scikit-learn <strong>version 0.17.1</strong> &mdash; <a href="http://scikit-learn.org/stable/support.html#documentation-resources">Other versions</a></p>
                <p class="citing">如果你使用此软件，请考虑引用我们： <a href="../../about.html#citing-scikit-learn">citing scikit-learn</a>.</p>
                <ul>
                    <li>
                        <a class="reference internal" href="#">监督学习:从高维观测中预测一个输出变量</a><ul>
                            <li>
                                <a class="reference internal" href="#nearest-neighbor-and-the-curse-of-dimensionality">最近邻 与 维数灾难</a><ul>
                                    <li><a class="reference internal" href="#k-nearest-neighbors-classifier">K-最近邻分类器</a></li>
                                    <li><a class="reference internal" href="#the-curse-of-dimensionality">维数灾难</a></li>
                                </ul>
                            </li>
                            <li>
                                <a class="reference internal" href="#linear-model-from-regression-to-sparsity">线性模型: 从回归到稀疏</a><ul>
                                    <li><a class="reference internal" href="#linear-regression">线性回归</a></li>
                                    <li><a class="reference internal" href="#shrinkage">Shrinkage</a></li>
                                    <li><a class="reference internal" href="#sparsity">稀疏性</a></li>
                                    <li><a class="reference internal" href="#classification">分类</a></li>
                                </ul>
                            </li>
                            <li>
                                <a class="reference internal" href="#support-vector-machines-svms">支持向量机 (SVMs)</a><ul>
                                    <li><a class="reference internal" href="#linear-svms">线性 SVMs</a></li>
                                    <li><a class="reference internal" href="#using-kernels">使用核函数</a></li>
                                </ul>
                            </li>
                        </ul>
                    </li>
                </ul>

            </div>
        </div>

        <input type="checkbox" id="nav-trigger" class="nav-trigger" checked />
        <label for="nav-trigger"></label>
        
        <div class="content">

            <div class="documentwrapper">
                <div class="bodywrapper">
                    <div class="body" role="main">

                        <div class="section" id="supervised-learning-predicting-an-output-variable-from-high-dimensional-observations">
                            <span id="supervised-learning-tut"></span><h1>监督学习：从高维观测中预测一个输出变量<a class="headerlink" href="#supervised-learning-predicting-an-output-variable-from-high-dimensional-observations" title="Permalink to this headline">¶</a></h1>
                            <div class="topic">
                                <p class="topic-title first">监督学习想要解决的问题</p>
                                <p>
                                    <a class="reference internal" href="../../supervised_learning.html#supervised-learning"><span>监督学习</span></a>
                                    的核心在于学习两个数据集之间的联系，这两个数据集分别是 观测数据集(<code class="docutils literal"><span class="pre">X</span></code>) 和
                                    一个我们想要预测的外部变量(<code class="docutils literal"><span class="pre">y</span></code>)
                                    此外部变量通常情况下被称之为 目标值(&#8220;target&#8221), 或 类标签(&#8220;labels&#8221);
                                    大多数时候，<code class="docutils literal"><span class="pre">y</span></code> 是一个<code class="docutils literal"><span class="pre">n_samples</span></code>
                                    长度的一维数组.
                                </p>
                                <p>
                                    scikit-learn 中所有的监督学习算法<a class="reference external" href="http://en.wikipedia.org/wiki/Estimator">estimators</a>对象都实现了成员方法
                                     <code class="docutils literal"><span class="pre">fit(X,</span> <span class="pre">y)</span></code> 用于拟合数据模型，
                                    还实现了一个成员方法 <code class="docutils literal"><span class="pre">predict(X)</span></code> 用于预测数据集 <code class="docutils literal"><span class="pre">X</span></code> 
                                    中的未知标签的样本,并返回预测的标签 <code class="docutils literal"><span class="pre">y</span></code>.
                                </p>
                            </div>
                            <div class="topic">
                                <p class="topic-title first">词汇: 分类 与 回归</p>
                                <p>
                                    如果一个预测任务是在有限的标签集合中分类观测数据或者说是给每个观测对象起名，那么
                                    这样的预测任务就叫<strong>分类</strong>任务；另一方面，如果我们的目标是根据数据集预测一个连续变化的目标变量
                                    ，那这样的预测任务就叫<strong>回归</strong>任务。所以在我们的语境中，预测是一个包含了分类与回归的更广泛的语义。
                                </p>
                                <p>
                                    使用 scikit-learn 做分类任务时, <code class="docutils literal"><span class="pre">y</span></code> 是一个整型的或字符串型的数组。
                                </p>
                                <p>
                                    注意啦: 你可以在此处 <a class="reference internal" href="../basic/tutorial.html#introduction">
                                        <span>
                                            使用 scikit-learn 进行机器学习之导论
                                        </span>
                                    </a> 快速查看一下机器学习的基本词汇的含义。
                                </p>
                            </div>
                            <div class="section" id="nearest-neighbor-and-the-curse-of-dimensionality">
                                <h2>最近邻 与 维数灾难<a class="headerlink" href="#nearest-neighbor-and-the-curse-of-dimensionality" title="Permalink to this headline">¶</a></h2>
                                <div class="topic">
                                    <p class="topic-title first"><strong>分类鸢尾花:</strong></p>
                                    <a class="reference external image-reference" href="../../auto_examples/datasets/plot_iris_dataset.html"><img alt="../../_images/plot_iris_dataset_0011.png" class="align-right" src="../../_images/plot_iris_dataset_0011.png" style="width: 520.0px; height: 390.0px;" /></a>
                                    <p>
                                        鸢尾花数据集(iris dataset)是一个分类任务，目标是依据花萼与花瓣的长度和宽度辨识三种不同品种的鸢尾花 (Setosa, Versicolour, 和 Virginica):
                                    </p>
                                    <div class="highlight-python">
                                        <div class="highlight">
                                            <pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris_X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris_y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">iris_y</span><span class="p">)</span>
<span class="go">array([0, 1, 2])</span>
</pre>
                                        </div>
                                    </div>
                                </div>
                                <div class="section" id="k-nearest-neighbors-classifier">
                                    <h3>KNN 分类器<a class="headerlink" href="#k-nearest-neighbors-classifier" title="Permalink to this headline">¶</a></h3>
                                    <p>
                                        最简单的分类器就是
                                        <a class="reference external" href="http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm">最近邻分类器(Nearest Neighbor)</a>:
                                        给定一个新的观测：<code class="docutils literal"><span class="pre">X_test</span></code>, 在训练集合中寻找与该观测最近的那个特征向量。
                                        (关于此分类器详情请看 scikit-learn 文档的 <a class="reference internal" href="../../modules/neighbors.html#neighbors"><span>Nearest Neighbors 章节</span></a>。)
                                    </p>
                                    <div class="topic">
                                        <p class="topic-title first"><strong>训练集与测试集</strong></p>
                                        <p>
                                            当我们做实验研究一个学习算法的时候，请不要用拟合estimator的时候用过的数据去测试它的预测性能
                                            ，这一点非常重要。我们必须用新的数据<strong>new data</strong>来测试其预测性能，以便知道其泛化性究竟如何.
                                            这就是为什么数据集通常都被划分成训练集和测试集的原因了。
                                        </p>
                                    </div>
                                    <p><strong>KNN (k nearest neighbors) 分类器例子</strong>:</p>
                                    <a class="reference external image-reference" href="../../auto_examples/neighbors/plot_classification.html"><img alt="../../_images/plot_classification_0012.png" class="align-center" src="../../_images/plot_classification_0012.png" style="width: 560.0px; height: 420.0px;" /></a>
                                    <div class="highlight-python">
                                        <div class="highlight">
                                            <pre><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Split iris data in train and test data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># A random permutation, to split the data randomly</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">iris_X</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris_X_train</span> <span class="o">=</span> <span class="n">iris_X</span><span class="p">[</span><span class="n">indices</span><span class="p">[:</span><span class="o">-</span><span class="mi">10</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris_y_train</span> <span class="o">=</span> <span class="n">iris_y</span><span class="p">[</span><span class="n">indices</span><span class="p">[:</span><span class="o">-</span><span class="mi">10</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris_X_test</span>  <span class="o">=</span> <span class="n">iris_X</span><span class="p">[</span><span class="n">indices</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris_y_test</span>  <span class="o">=</span> <span class="n">iris_y</span><span class="p">[</span><span class="n">indices</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Create and fit a nearest-neighbor classifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris_X_train</span><span class="p">,</span> <span class="n">iris_y_train</span><span class="p">)</span> 
<span class="go">KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,</span>
<span class="go">           metric_params=None, n_jobs=1, n_neighbors=5, p=2,</span>
<span class="go">           weights=&#39;uniform&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">iris_X_test</span><span class="p">)</span>
<span class="go">array([1, 2, 1, 0, 0, 0, 2, 1, 2, 0])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris_y_test</span>
<span class="go">array([1, 1, 1, 0, 0, 0, 2, 1, 2, 0])</span>
</pre>
                                        </div>
                                    </div>
                                </div>
                                <div class="section" id="the-curse-of-dimensionality">
                                    <span id="curse-of-dimensionality"></span><h3>维数灾难<a class="headerlink" href="#the-curse-of-dimensionality" title="Permalink to this headline">¶</a></h3>
                                    <p>
                                        For an estimator to be effective, you need the distance between neighboring
                                        points to be less than some value <img class="math" src="../../_images/math/425d86ba2f2979d75b7535c2bcf92c33ed6b285a.png" alt="d" />, which depends on the problem.
                                        In one dimension, this requires on average <img class="math" src="../../_images/math/1149cb226589a7b7d18f15d51c492e99da4fae4c.png" alt="n ~ 1/d" /> points.
                                        In the context of the above <img class="math" src="../../_images/math/e9203da50e1059455123460d4e716c9c7f440cc3.png" alt="k" />-NN example, if the data is described by
                                        just one feature with values ranging from 0 to 1 and with <img class="math" src="../../_images/math/413f8a8e40062a9090d9d50b88bc7b551b314c26.png" alt="n" /> training
                                        observations, then new data will be no further away than <img class="math" src="../../_images/math/8ed7b101148182605feefc48fac92f577d32f6f2.png" alt="1/n" />.
                                        Therefore, the nearest neighbor decision rule will be efficient as soon as
                                        <img class="math" src="../../_images/math/8ed7b101148182605feefc48fac92f577d32f6f2.png" alt="1/n" /> is small compared to the scale of between-class feature variations.
                                    </p>
                                    <p>
                                        If the number of features is <img class="math" src="../../_images/math/3eca8557203e86160952e1c0f735f7417f3285b1.png" alt="p" />, you now require <img class="math" src="../../_images/math/1ce752cc0feafeca82a10347e47927f82d90254d.png" alt="n ~ 1/d^p" />
                                        points.  Let&#8217;s say that we require 10 points in one dimension: now <img class="math" src="../../_images/math/d6e79569893190df0327efadedba52812f6ec183.png" alt="10^p" />
                                        points are required in <img class="math" src="../../_images/math/3eca8557203e86160952e1c0f735f7417f3285b1.png" alt="p" /> dimensions to pave the <img class="math" src="../../_images/math/787ef8897f85416cae83d74ef5630a9c5973d996.png" alt="[0, 1]" /> space.
                                        As <img class="math" src="../../_images/math/3eca8557203e86160952e1c0f735f7417f3285b1.png" alt="p" /> becomes large, the number of training points required for a good
                                        estimator grows exponentially.
                                    </p>
                                    <p>
                                        For example, if each point is just a single number (8 bytes), then an
                                        effective <img class="math" src="../../_images/math/e9203da50e1059455123460d4e716c9c7f440cc3.png" alt="k" />-NN estimator in a paltry <img class="math" src="../../_images/math/b0baabc767a659b7f59f2d65ba14cbeff17d9d6b.png" alt="p~20" /> dimensions would
                                        require more training data than the current estimated size of the entire
                                        internet (±1000 Exabytes or so).
                                    </p>
                                    <p>
                                        This is called the
                                        <a class="reference external" href="http://en.wikipedia.org/wiki/Curse_of_dimensionality">curse of dimensionality</a>
                                        and is a core problem that machine learning addresses.
                                    </p>
                                </div>
                            </div>
                            <div class="section" id="linear-model-from-regression-to-sparsity">
                                <h2>线性模型: 从回归到稀疏<a class="headerlink" href="#linear-model-from-regression-to-sparsity" title="Permalink to this headline">¶</a></h2>
                                <div class="topic">
                                    <p class="topic-title first"><strong>糖尿病数据集(Diabetes dataset)</strong></p>
                                    <p>
                                        糖尿病数据集包含了442个病人的10个生理变量(年龄,性别，体重，血压)数据,
                                        还有一年后疾病发展状况:
                                    </p>
                                    <div class="highlight-python">
                                        <div class="highlight">
                                            <pre><span class="gp">&gt;&gt;&gt; </span><span class="n">diabetes</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_diabetes</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">diabetes_X_train</span> <span class="o">=</span> <span class="n">diabetes</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="o">-</span><span class="mi">20</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">diabetes_X_test</span>  <span class="o">=</span> <span class="n">diabetes</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="o">-</span><span class="mi">20</span><span class="p">:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">diabetes_y_train</span> <span class="o">=</span> <span class="n">diabetes</span><span class="o">.</span><span class="n">target</span><span class="p">[:</span><span class="o">-</span><span class="mi">20</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">diabetes_y_test</span>  <span class="o">=</span> <span class="n">diabetes</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="o">-</span><span class="mi">20</span><span class="p">:]</span>
</pre>
                                        </div>
                                    </div>
                                    <p>
                                        对于此数据集，我们的任务是根据生理变量指标来预测疾病进展状况。
                                    </p>
                                </div>
                                <div class="section" id="linear-regression">
                                    <h3>线性回归<a class="headerlink" href="#linear-regression" title="Permalink to this headline">¶</a></h3>
                                    <p>
                                        <a class="reference internal" href="../../modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression" title="sklearn.linear_model.LinearRegression"><code class="xref py py-class docutils literal"><span class="pre">LinearRegression</span></code></a>,
                                        线性回归的最简单形式是通过调节一个参数集合为数据集拟合一个线性模型,使得其残差平方和尽可能小。
                                    </p>
                                    <a class="reference external image-reference" href="../../auto_examples/linear_model/plot_ols.html"><img alt="../../_images/plot_ols_0012.png" class="align-right" src="../../_images/plot_ols_0012.png" style="width: 320.0px; height: 240.0px;" /></a>
                                    <p>线性模型: <img class="math" src="../../_images/math/da6d9feb881cb1b6599ef1aee85eb1131052017b.png" alt="y = X\beta + \epsilon" /></p>
                                    <blockquote>
                                        <div>
                                            <ul class="simple">
                                                <li><img class="math" src="../../_images/math/f026aecf11ec7f6141ab863f260d395f94b10f51.png" alt="X" />: 数据</li>
                                                <li><img class="math" src="../../_images/math/b124ff74afb0914bb434e8fb849eb56d734412f8.png" alt="y" />: 目标变量</li>
                                                <li><img class="math" src="../../_images/math/8ce03f78ed945f2ef3dac87c8799b55b393527e7.png" alt="\beta" />: 系数</li>
                                                <li><img class="math" src="../../_images/math/19bc0073dde1bcd1a8e6a32b251e80cced668f04.png" alt="\epsilon" />: 观测噪声</li>
                                            </ul>
                                        </div>
                                    </blockquote>
                                    <div class="highlight-python">
                                        <div class="highlight">
                                            <pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">,</span> <span class="n">diabetes_y_train</span><span class="p">)</span>
<span class="go">LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[   0.30349955 -237.63931533  510.53060544  327.73698041 -814.13170937</span>
<span class="go">  492.81458798  102.84845219  184.60648906  743.51961675   76.09517222]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The mean square error</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">diabetes_X_test</span><span class="p">)</span><span class="o">-</span><span class="n">diabetes_y_test</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="go">2004.56760268...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Explained variance score: 1 is perfect prediction</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># and 0 means that there is no linear relationship</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># between X and Y.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">diabetes_X_test</span><span class="p">,</span> <span class="n">diabetes_y_test</span><span class="p">)</span> 
<span class="go">0.5850753022690...</span>
</pre>
                                        </div>
                                    </div>
                                </div>
                                <div class="section" id="shrinkage">
                                    <span id="id2"></span><h3>Shrinkage<a class="headerlink" href="#shrinkage" title="Permalink to this headline">¶</a></h3>
                                    <p>
                                        如果每一维上的数据点都非常少，观测噪声将会引入很大的方差：
                                    </p>
                                    <a class="reference external image-reference" href="../../auto_examples/linear_model/plot_ols_ridge_variance.html"><img alt="../../_images/plot_ols_ridge_variance_0011.png" class="align-right" src="../../_images/plot_ols_ridge_variance_0011.png" style="width: 280.0px; height: 210.0px;" /></a>
                                    <div class="highlight-python">
                                        <div class="highlight">
                                            <pre><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span> <span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pylab</span> <span class="kn">as</span> <span class="nn">pl</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">pl</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span> 
<span class="gp">... </span>   <span class="n">this_X</span> <span class="o">=</span> <span class="o">.</span><span class="mi">1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="n">X</span>
<span class="gp">... </span>   <span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">this_X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">... </span>   <span class="n">pl</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test</span><span class="p">))</span> 
<span class="gp">... </span>   <span class="n">pl</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">this_X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>  
</pre>
                                        </div>
                                    </div>
                                    <p>
                                        高维统计学习中的一个解决方案是将回归系数缩小(<em>shrink</em>)到零:
                                        任何两个随机选择的观测集合都可能是不相关的。 这被叫做<a class="reference internal" href="../../modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-class docutils literal"><span class="pre">岭回归(Ridge regression)</span></code></a>:
                                    </p>
                                    <a class="reference external image-reference" href="../../auto_examples/linear_model/plot_ols_ridge_variance.html"><img alt="../../_images/plot_ols_ridge_variance_0021.png" class="align-right" src="../../_images/plot_ols_ridge_variance_0021.png" style="width: 280.0px; height: 210.0px;" /></a>
                                    <div class="highlight-python">
                                        <div class="highlight">
                                            <pre><span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=.</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pl</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span> 
<span class="gp">... </span>   <span class="n">this_X</span> <span class="o">=</span> <span class="o">.</span><span class="mi">1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="n">X</span>
<span class="gp">... </span>   <span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">this_X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">... </span>   <span class="n">pl</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test</span><span class="p">))</span> 
<span class="gp">... </span>   <span class="n">pl</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">this_X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span> 
</pre>
                                        </div>
                                    </div>
                                    <p>
                                        这是一个在偏差与方差之间折中(<strong>bias/variance tradeoff</strong>)的例子： 
                                        岭参数<code class="docutils literal"><span class="pre">alpha</span></code> 越大,偏置越大同时方差却越小。
                                    </p>
                                    <p>
                                        我们可以通过选择参数 <code class="docutils literal"><span class="pre">alpha</span></code> 来最小化左侧输出误差。
                                        这次的例子我们使用糖尿病数据集(diabetes dataset)而不是我们人工合成的数据:
                                    </p>
                                    <div class="highlight-python">
                                        <div class="highlight">
                                            <pre><span class="gp">&gt;&gt;&gt; </span><span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">([</span><span class="n">regr</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span>
<span class="gp">... </span>            <span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">,</span> <span class="n">diabetes_y_train</span><span class="p">,</span>
<span class="gp">... </span>            <span class="p">)</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">diabetes_X_test</span><span class="p">,</span> <span class="n">diabetes_y_test</span><span class="p">)</span> <span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">])</span> 
<span class="go">[0.5851110683883..., 0.5852073015444..., 0.5854677540698..., 0.5855512036503..., 0.5830717085554..., 0.57058999437...]</span>
</pre>
                                        </div>
                                    </div>
                                    <div class="admonition note">
                                        <p class="first admonition-title">注意</p>
                                        <p class="last">
                                            Capturing in the fitted parameters noise that prevents the model to
                                            generalize to new data is called
                                            <a class="reference external" href="http://en.wikipedia.org/wiki/Overfitting">过拟合(overfitting)</a>。 
                                            岭回归引入的偏置被称为
                                            <a class="reference external" href="http://en.wikipedia.org/wiki/Regularization_%28machine_learning%29">正则化(regularization)</a>。
                                        </p>
                                    </div>
                                </div>
                                <div class="section" id="sparsity">
                                    <span id="id3"></span><h3>稀疏性<a class="headerlink" href="#sparsity" title="Permalink to this headline">¶</a></h3>
                                    <p class="centered"><strong>仅仅拟合特征 1 和 2</strong></p>
                                    <p class="centered">
                                        <strong><a class="reference external" href="../../auto_examples/linear_model/plot_ols_3d.html"><img alt="diabetes_ols_1" src="../../_images/plot_ols_3d_0011.png" style="width: 260.0px; height: 195.0px;" /></a> <a class="reference external" href="../../auto_examples/linear_model/plot_ols_3d.html"><img alt="diabetes_ols_3" src="../../_images/plot_ols_3d_0031.png" style="width: 260.0px; height: 195.0px;" /></a> <a class="reference external" href="../../auto_examples/linear_model/plot_ols_3d.html"><img alt="diabetes_ols_2" src="../../_images/plot_ols_3d_0021.png" style="width: 260.0px; height: 195.0px;" /></a></strong>
                                    </p><div class="admonition note">
                                        <p class="first admonition-title">注意：</p>
                                        <p class="last">
                                            糖尿病数据集总共有11个维度(10个特征分量和一个目标变量)。 在这么高的维度上我们很难产生对数据集的直觉想想
                                            但是我们因该始终记住这个数据集的分布空间其实是一个相当稀疏的空间(<em>empty</em> space)。
                                        </p>
                                    </div>
                                    <p>
                                        我们可以从图中看到，尽管第二个特征分量在完整模型中有很强的系数,
                                        但是与第一个特征分量相比，它承载了关于<code class="docutils literal"><span class="pre">y</span></code>的非常少的信息。
                                    </p>
                                    <p>
                                        为了改善问题的求解条件(也就是 减轻
                                        <a class="reference internal" href="#curse-of-dimensionality"><span>维数灾难</span></a>), 
                                        仅仅选择那些信息量大的特征而把几乎不能提供有用信息的特征(比如x_2)置为0的做法将十分有趣。
                                        岭回归(Ridge regression)虽然能够降低那些垃圾特征(non-informative features)的贡献,但是它无法把他们的影响
                                        完全归零。另外一种惩罚方法被称之为 最小绝对缩减与选择算子<a class="reference internal" href="../../modules/linear_model.html#lasso"><span>Lasso</span></a> (least absolute shrinkage and
                                        selection operator), 可以将某些系数设置为0. 这样的方法被称为稀疏方法(<strong>sparse method</strong>)。
                                        稀疏性原则可以被视为是奥坎姆剃刀原则的一个应用。
                                    </p>
                                    <div class="highlight-python">
                                        <div class="highlight">
                                            <pre><span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Lasso</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="p">[</span><span class="n">regr</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span>
<span class="gp">... </span>            <span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">,</span> <span class="n">diabetes_y_train</span>
<span class="gp">... </span>            <span class="p">)</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">diabetes_X_test</span><span class="p">,</span> <span class="n">diabetes_y_test</span><span class="p">)</span>
<span class="gp">... </span>       <span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">best_alpha</span> <span class="o">=</span> <span class="n">alphas</span><span class="p">[</span><span class="n">scores</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">scores</span><span class="p">))]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">best_alpha</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">diabetes_X_train</span><span class="p">,</span> <span class="n">diabetes_y_train</span><span class="p">)</span>
<span class="go">Lasso(alpha=0.025118864315095794, copy_X=True, fit_intercept=True,</span>
<span class="go">   max_iter=1000, normalize=False, positive=False, precompute=False,</span>
<span class="go">   random_state=None, selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[   0.         -212.43764548  517.19478111  313.77959962 -160.8303982    -0.</span>
<span class="go"> -187.19554705   69.38229038  508.66011217   71.84239008]</span>
</pre>
                                        </div>
                                    </div>
                                    <div class="topic">
                                        <p class="topic-title first"><strong>针对同一问题的不同算法</strong></p>
                                        <p>
                                            同一个数学问题可以使用不同的算法来求解。例如，在scikit-learn中<code class="docutils literal"><span class="pre">Lasso</span></code> 对象
                                            使用对大数据集特别有效率的<a class="reference external" href="http://en.wikipedia.org/wiki/Coordinate_descent">坐标下降法(coordinate decent)</a>解决了lasso回归问题；
                                            然而，scikit-learn也提供了 <a class="reference internal" href="../../modules/generated/sklearn.linear_model.LassoLars.html#sklearn.linear_model.LassoLars" title="sklearn.linear_model.LassoLars"><code class="xref py py-class docutils literal"><span class="pre">LassoLars</span></code></a> 对象
                                            使用的是 <em>LARS</em> 算法,该算法对那些估计出的权值向量非常稀疏(通常是因为观测数据特别少造成的)的问题非常有效。
                                        </p>
                                    </div>
                                </div>
                                <div class="section" id="classification">
                                    <span id="clf-tut"></span><h3>分类<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h3>
                                    <a class="reference external image-reference" href="../../auto_examples/linear_model/plot_logistic.html"><img alt="../../_images/plot_logistic_0011.png" class="align-right" src="../../_images/plot_logistic_0011.png" style="width: 260.0px; height: 195.0px;" /></a>
                                    <p>
                                        对于分类问题，比如标记鸢尾花的
                                        <a class="reference external" href="http://en.wikipedia.org/wiki/Iris_flower_data_set">iris</a> 任务, 线性回归不再是一个正确的方法，
                                        因为他将会给那些远离决策前沿(decision frontier)的数据点分配太大的权重。一种可用的线性方法是去拟合一个
                                        <strong>sigmoid</strong>函数或<strong>logistic</strong>函数。 
                                    </p>
                                    <div class="math">
                                        <p>
                                            <img src="../../_images/math/3fcabd4a94b24f0fb2bd4b2bb79703cf8f5eb496.png" alt="y = \textrm{sigmoid}(X\beta - \textrm{offset}) + \epsilon =
\frac{1}{1 + \textrm{exp}(- X\beta + \textrm{offset})} + \epsilon" />
                                        </p>
                                    </div><div class="highlight-python">
                                        <div class="highlight">
                                            <pre><span class="gp">&gt;&gt;&gt; </span><span class="n">logistic</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1e5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logistic</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris_X_train</span><span class="p">,</span> <span class="n">iris_y_train</span><span class="p">)</span>
<span class="go">LogisticRegression(C=100000.0, class_weight=None, dual=False,</span>
<span class="go">          fit_intercept=True, intercept_scaling=1, max_iter=100,</span>
<span class="go">          multi_class=&#39;ovr&#39;, n_jobs=1, penalty=&#39;l2&#39;, random_state=None,</span>
<span class="go">          solver=&#39;liblinear&#39;, tol=0.0001, verbose=0, warm_start=False)</span>
</pre>
                                        </div>
                                    </div>
                                    <p>这就是众所周知的逻辑斯特回归 <a class="reference internal" href="../../modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-class docutils literal"><span class="pre">LogisticRegression</span></code></a>.</p>
                                    <a class="reference external image-reference" href="../../auto_examples/linear_model/plot_iris_logistic.html"><img alt="../../_images/plot_iris_logistic_0011.png" src="../../_images/plot_iris_logistic_0011.png" style="width: 332.0px; height: 249.0px;" /></a>
                                    <div class="topic">
                                        <p class="topic-title first">多类别分类</p>
                                        <p>
                                            如果你要预测多个类，一个经常使用的方法是去拟合多个一对多分类器(one-versus-all classifiers)
                                            ，然后使用投票启发式的方法做出最终的决策。
                                        </p>
                                    </div>
                                    <div class="topic">
                                        <p class="topic-title first">逻辑斯特回归中的 Shrinkage 和 sparsity</p>
                                        <p>
                                            参数 <code class="docutils literal"><span class="pre">C</span></code> 控制着<a class="reference internal" href="../../modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-class docutils literal"><span class="pre">LogisticRegression</span></code></a> 
                                            对象的正则化的量： <code class="docutils literal"><span class="pre">C</span></code> 的值越大，正则化的量越小。
                                            惩罚因子<code class="docutils literal"><span class="pre">penalty=&quot;l2&quot;</span></code> 给出 <a class="reference internal" href="#shrinkage"><span>Shrinkage</span></a> (i.e. non-sparse coefficients), 而
                                            <code class="docutils literal"><span class="pre">penalty=&quot;l1&quot;</span></code> 给出 <a class="reference internal" href="#sparsity"><span>Sparsity</span></a>.
                                        </p>
                                    </div>
                                    <div class="green topic">
                                        <p class="topic-title first"><strong>练习题</strong></p>
                                        <p>
                                            尝试使用最近邻算法和线性模型分类digits数据集。留出数据集中最后的10%然后测试模型在这些观测数据
                                            上的预测性能。
                                        </p>
                                        <div class="highlight-python">
                                            <div class="highlight">
                                                <pre><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">neighbors</span><span class="p">,</span> <span class="n">linear_model</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_digits</span><span class="p">()</span>
<span class="n">X_digits</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">data</span>
<span class="n">y_digits</span> <span class="o">=</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span>
</pre>
                                            </div>
                                        </div>
                                        <p>Solution: <a class="reference download internal" href="../../_downloads/digits_classification_exercise.py"><code class="xref download docutils literal"><span class="pre">../../auto_examples/exercises/digits_classification_exercise.py</span></code></a></p>
                                    </div>
                                </div>
                            </div>
                            <div class="section" id="support-vector-machines-svms">
                                <h2>支持向量机 (SVMs)<a class="headerlink" href="#support-vector-machines-svms" title="Permalink to this headline">¶</a></h2>
                                <div class="section" id="linear-svms">
                                    <h3>线性 SVMs<a class="headerlink" href="#linear-svms" title="Permalink to this headline">¶</a></h3>
                                    <p>
                                        <a class="reference internal" href="../../modules/svm.html#svm"><span>支持向量机(Support Vector Machines)</span></a> 
                                        属于判别模型的家族(the discriminant model family): 他们试图找到一些样本组合来构建一个能够最大化两类之间的边缘的分离平面。
                                        正则化项用参数 <code class="docutils literal"><span class="pre">C</span></code> 进行设置: 
                                        a small value for <code class="docutils literal"><span class="pre">C</span></code> means the margin
                                        is calculated using many or all of the observations around the separating line
                                        (more regularization);
                                        a large value for <code class="docutils literal"><span class="pre">C</span></code> means the margin is calculated on observations close to
                                        the separating line (less regularization).
                                    </p>
                                    <table border="1" class="centered docutils">
                                        <colgroup>
                                            <col width="49%" />
                                            <col width="51%" />
                                        </colgroup>
                                        <thead valign="bottom">
                                            <tr class="row-odd">
                                                <th class="head"><strong>无正则化 SVM</strong></th>
                                                <th class="head"><strong>正则化 SVM (默认)</strong></th>
                                            </tr>
                                        </thead>
                                        <tbody valign="top">
                                            <tr class="row-even">
                                                <td><a class="reference external" href="../../auto_examples/svm/plot_svm_margin.html"><img alt="svm_margin_unreg" src="../../_images/plot_svm_margin_0011.png" style="width: 280.0px; height: 210.0px;" /></a></td>
                                                <td><a class="reference external" href="../../auto_examples/svm/plot_svm_margin.html"><img alt="svm_margin_reg" src="../../_images/plot_svm_margin_0021.png" style="width: 280.0px; height: 210.0px;" /></a></td>
                                            </tr>
                                        </tbody>
                                    </table>
                                    <div class="topic">
                                        <p class="topic-title first">例子:</p>
                                        <ul class="simple">
                                            <li><a class="reference internal" href="../../auto_examples/svm/plot_iris.html#example-svm-plot-iris-py"><span>Plot different SVM classifiers in the iris dataset</span></a></li>
                                        </ul>
                                    </div>
                                    <p>
                                        SVMs 既可以被用于回归问题 &#8211;<a class="reference internal" href="../../modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code class="xref py py-class docutils literal"><span class="pre">SVR</span></code></a> 
                                        (Support Vector Regression)&#8211;, 也可以被用于分类 &#8211;<a class="reference internal" href="../../modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><code class="xref py py-class docutils literal"><span class="pre">SVC</span></code></a> (Support Vector Classification).
                                    </p>
                                    <div class="highlight-python">
                                        <div class="highlight">
                                            <pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris_X_train</span><span class="p">,</span> <span class="n">iris_y_train</span><span class="p">)</span>    
<span class="go">SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,</span>
<span class="go">    decision_function_shape=None, degree=3, gamma=&#39;auto&#39;, kernel=&#39;linear&#39;,</span>
<span class="go">    max_iter=-1, probability=False, random_state=None, shrinking=True,</span>
<span class="go">    tol=0.001, verbose=False)</span>
</pre>
                                        </div>
                                    </div>
                                    <div class="admonition warning">
                                        <p class="first admonition-title">警告：</p>
                                        <p><strong>归一化数据</strong></p>
                                        <p class="last">
                                            对于很多 estimators 对象, 包括 SVMs, 为了获得好的预测结果，
                                            使得数据集中每一个特征分量具有单位标准差(unit standard deviation for each feature)是非常重要的。
                                        </p>
                                    </div>
                                </div>
                                <div class="section" id="using-kernels">
                                    <span id="using-kernels-tut"></span><h3>使用核函数<a class="headerlink" href="#using-kernels" title="Permalink to this headline">¶</a></h3>
                                    <p>
                                        多个类别在其特征空间并不总是线性可分的。解决的办法是使用多项式决策函数而不是线性决策函数。
                                        这可以通过使用核函数(<em>kernel trick</em>) that can be seen as
                                        通过将核函数放置在观测数据集上创造一个决策能量(creating a decision energy by positioning <em>kernels</em> on observations):
                                    </p>
                                    <table border="1" class="centered docutils">
                                        <colgroup>
                                            <col width="50%" />
                                            <col width="50%" />
                                        </colgroup>
                                        <tbody valign="top">
                                            <tr class="row-odd">
                                                <td><strong>Linear kernel</strong></td>
                                                <td><strong>Polynomial kernel</strong></td>
                                            </tr>
                                            <tr class="row-even">
                                                <td><a class="reference external" href="../../auto_examples/svm/plot_svm_kernels.html"><img alt="svm_kernel_linear" src="../../_images/plot_svm_kernels_0011.png" style="width: 260.0px; height: 195.0px;" /></a></td>
                                                <td><a class="reference external" href="../../auto_examples/svm/plot_svm_kernels.html"><img alt="svm_kernel_poly" src="../../_images/plot_svm_kernels_0021.png" style="width: 260.0px; height: 195.0px;" /></a></td>
                                            </tr>
                                            <tr class="row-odd">
                                                <td>
                                                    <div class="first last highlight-python">
                                                        <div class="highlight">
                                                            <pre><span class="gp">&gt;&gt;&gt; </span><span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span>
</pre>
                                                        </div>
                                                    </div>
                                                </td>
                                                <td>
                                                    <div class="first last highlight-python">
                                                        <div class="highlight">
                                                            <pre><span class="gp">&gt;&gt;&gt; </span><span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;poly&#39;</span><span class="p">,</span>
<span class="gp">... </span>              <span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># degree: polynomial degree</span>
</pre>
                                                        </div>
                                                    </div>
                                                </td>
                                            </tr>
                                        </tbody>
                                    </table>
                                    <table border="1" class="centered docutils">
                                        <colgroup>
                                            <col width="100%" />
                                        </colgroup>
                                        <tbody valign="top">
                                            <tr class="row-odd">
                                                <td><strong>RBF kernel (径向基函数)</strong></td>
                                            </tr>
                                            <tr class="row-even">
                                                <td><a class="reference external" href="../../auto_examples/svm/plot_svm_kernels.html"><img alt="svm_kernel_rbf" src="../../_images/plot_svm_kernels_0031.png" style="width: 260.0px; height: 195.0px;" /></a></td>
                                            </tr>
                                            <tr class="row-odd">
                                                <td>
                                                    <div class="first last highlight-python">
                                                        <div class="highlight">
                                                            <pre><span class="gp">&gt;&gt;&gt; </span><span class="n">svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># gamma: inverse of size of</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># radial kernel</span>
</pre>
                                                        </div>
                                                    </div>
                                                </td>
                                            </tr>
                                        </tbody>
                                    </table>
                                    <div class="topic">
                                        <p class="topic-title first"><strong>交互式例子</strong></p>
                                        <p>
                                            请看 <a class="reference internal" href="../../auto_examples/applications/svm_gui.html#example-applications-svm-gui-py"><span>SVM GUI</span></a> 下载
                                            <code class="docutils literal"><span class="pre">svm_gui.py</span></code>; 使用鼠标左键与右键分别为两个类添加数据点，然后拟合模型。可以不断的改变数据和参数观看分类效果。
                                        </p>
                                    </div>
                                    <a class="reference external image-reference" href="../../auto_examples/datasets/plot_iris_dataset.html"><img alt="../../_images/plot_iris_dataset_0011.png" class="align-right" src="../../_images/plot_iris_dataset_0011.png" style="width: 560.0px; height: 420.0px;" /></a>
                                    <div class="green topic">
                                        <p class="topic-title first"><strong>练习</strong></p>
                                        <p>
                                            尝试使用SVMs分类iris数据集中的类1与类2，只用前两个特征。
                                            为每个类留出10%的测试样本去测试分类器的效果。
                                        </p>
                                        <p>
                                            <strong>警告</strong>: 类的数据是有序存放的，不要留出最后的10%，那样你将只在一类上进行了测试而已。
                                        </p>
                                        <p>
                                            <strong>提示</strong>: 你可以将 <code class="docutils literal"><span class="pre">decision_function</span></code> 方法应用到一个网格上去获得启发式信息。
                                        </p>
                                        <div class="highlight-python">
                                            <div class="highlight">
                                                <pre><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">y</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">]</span>
</pre>
                                            </div>
                                        </div>
                                        <p>答案: <a class="reference download internal" href="../../_downloads/plot_iris_exercise.py"><code class="xref download docutils literal"><span class="pre">../../auto_examples/exercises/plot_iris_exercise.py</span></code></a></p>
                                    </div>
                                </div>
                            </div>
                        </div>


                    </div>
                </div>
            </div>
            <div class="clearer"></div>
        </div>
    </div>

    <div class="footer">
        &copy; 2010 - 2014, scikit-learn developers (BSD License).
        <a href="../../_sources/tutorial/statistical_inference/supervised_learning.txt" rel="nofollow">Show this page source</a>
    </div>
    <div class="rel">

        <div class="buttonPrevious">
            <a href="settings.html">
                Previous
            </a>
        </div>

    </div>


    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-22606712-2']);
      _gaq.push(['_trackPageview']);

      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>


    <script src="http://www.google.com/jsapi" type="text/javascript"></script>
    <script type="text/javascript">
 google.load('search', '1',
        {language : 'en'}); google.setOnLoadCallback(function() {
            var customSearchControl = new
            google.search.CustomSearchControl('016639176250731907682:tjtqbvtvij0');
            customSearchControl.setResultSetSize(google.search.Search.FILTERED_CSE_RESULTSET);
            var options = new google.search.DrawOptions();
            options.setAutoComplete(true);
            customSearchControl.draw('cse', options); }, true);
    </script>
</body>
</html>
