<!DOCTYPE html>
<html lang="zh-cn">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <link rel="stylesheet" href="../../../../css-fonts-js/css/bootstrap.min.css" />
    <link rel="stylesheet" href="../../../../css-fonts-js/css/bootstrap-responsive.css" />
    <link rel="stylesheet" href="../../../../css-fonts-js/css/gallery.css" />
    <link rel="stylesheet" href="../../../../css-fonts-js/css/nature.css" />
    <link rel="stylesheet" href="../../../../css-fonts-js/css/pygments.css" />

    <style>
        p {
            font-size: large;
        }

        li {
            font-size: large;
        }
    </style>

    <script src="../../../../css-fonts-js/js/jquery.min.js"></script>
    <script src="../../../../css-fonts-js/js/jquery.js"></script>
    <script src="../../../../css-fonts-js/js/copybutton.js"></script>
    <script src="../../../../css-fonts-js/js/doctools.js"></script>
    <script src="../../../../css-fonts-js/js/jquery.maphilight.js"></script>
    <script src="../../../../css-fonts-js/js/jquery.maphilight.min.js"></script>
    <script src="../../../../css-fonts-js/js/underscore.js"></script>
    <script src="../../../../css-fonts-js/js/bootstrap.min.js"></script>


    <title>人工智能研究网</title>
    <!--网站访问量统计-->
    <script>
        var _hmt = _hmt || [];
        (function () {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?e7836e37a4cb7584127a787e9b44e3f1";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>
</head>


<body>

    <nav class="navbar navbar-inverse">
        <div class="container">
            <div class="navbar-header">
                <a class="navbar-brand" href="../../../../index.html">studyai.cn</a>
            </div>
            <ul class="nav navbar-nav">
                <li class="active"><a href="../../../../index.html">网站主页</a></li>
                <li><a href="../../../../webmaster.html">站长风采</a></li>
                <li><a href="../../../../sponsor.html">赞助我们</a></li>
                <li><a href="../../../index.html">机器学习主页</a></li>
            </ul>
        </div>
    </nav>

    <div class="container">

        <div class="section" id="stochastic-gradient-descent">
            <h1>1.5. 随机梯度下降法(SGD)<a>¶</a></h1>
            <p>
                <strong>随机梯度下降 (SGD)</strong> 是一个简单但非常高效的方法，用于在凸损失函数
                (比如线性<a class="reference external" href="http://en.wikipedia.org/wiki/Support_vector_machine">
                SVMs</a> 和
                 <a class="reference external" href="http://en.wikipedia.org/wiki/Logistic_regression"> Logistic  Regression
                    </a>)下的线性分类器的判别学习。
                尽管SGD已经出现在机器学习算法中很长时间了，但是由于其在大规模学习中的出色表现直到
                最近才获得了相当大的关注。
            </p>
            <p>
                SGD已经被成功应用于大规模的和稀疏的机器学习任务中，比如文本分类以及自然语言处理等领域。
                由于数据的稀疏性，在此模块中的分类器们能够轻松应对超过10^5的训练样本和超过10^5的特征个数。 
            </p>
            <p>随机梯度下降SGD的优点:</p>
            <blockquote>
                <div>
                    <ul class="simple">
                        <li>高效率.</li>
                        <li>实现简单(lots of opportunities for code tuning).</li>
                    </ul>
                </div>
            </blockquote>
            <p>随机梯度下降SGD的缺点:</p>
            <blockquote>
                <div>
                    <ul class="simple">
                        <li>
                            SGD需要一些超参数比如正则化参数以及迭代次数。
                        </li>
                        <li>SGD对特征规模的变化比较敏感.</li>
                    </ul>
                </div>
            </blockquote>

            <div class="section" id="classification">
                <h2>1.5.1. 分类<a>¶</a></h2>
                <div class="admonition warning">
                    <p class="first admonition-title">警告</p>
                    <p class="last">
                        务必要在拟合模型前将你的训练数据随机重排列；或者使用
                         <code class="docutils literal"><span class="pre">shuffle=True</span></code>
                        在每次迭代完毕后都重新洗牌.
                    </p>
                </div>
                <p>
                    <code class="xref py py-class docutils literal"><span class="pre">SGDClassifier</span></code>
                    类实现了一个用于分类的简单的随机梯度下降学习过程并且支持不同类型的损失函数和正则化罚函数。
                </p>
                <div class="figure align-center">
                    <a class="reference external image-reference" href="../auto_examples/linear_model/plot_sgd_separating_hyperplane.html">
                        <img alt="./images/plot_sgd_separating_hyperplane_0011.png" src="./images/plot_sgd_separating_hyperplane_0011.png"
                             style="width: 600.0px; height: 450.0px;" />
                    </a>
                </div>
                <p>
                    像其他分类器一样，SGD也必须用两个输入数组进行拟合：数组 X
                    of size [n_samples, n_features] 存储训练样本, 数组 Y of size [n_samples] 
                    存储目标变量(每个训练样本的类标签):
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;hinge&quot;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;l2&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,</span>
<span class="go">       eta0=0.0, fit_intercept=True, l1_ratio=0.15,</span>
<span class="go">       learning_rate=&#39;optimal&#39;, loss=&#39;hinge&#39;, n_iter=5, n_jobs=1,</span>
<span class="go">       penalty=&#39;l2&#39;, power_t=0.5, random_state=None, shuffle=True,</span>
<span class="go">       verbose=0, warm_start=False)</span>
</pre>
                    </div>
                </div>
                <p>当模型拟合好以后，就可以用于预测新样本的类别标签:</p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>
<span class="go">array([1])</span>
</pre>
                    </div>
                </div>
                <p>
                    SGD拟合一个线性模型来训练数据。其成员变量
                    <code class="docutils literal"><span class="pre">coef_</span></code> 包含了拟合得到的模型参数。
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span>                                         
<span class="go">array([[ 9.9...,  9.9...]])</span>
</pre>
                    </div>
                </div>
                <p>成员变量 <code class="docutils literal"><span class="pre">intercept_</span></code> 包含了截距 (也可以叫 偏移或偏置):
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span>                                    
<span class="go">array([-9.9...])</span>
</pre>
                    </div>
                </div>
                <p>
                    模型是否要使用截距或者说是一个带偏置的超平面，是由
                    <code class="docutils literal"><span class="pre">fit_intercept</span></code>
                    参数控制的。 
                </p>
                <p>为了获得到超平面的标记距离，可以使用
                <code class="xref py py-meth docutils literal"><span class="pre">SGDClassifier.decision_function</span></code>:</p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">decision_function</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>                 
<span class="go">array([ 29.6...])</span>
</pre>
                    </div>
                </div>
                <p>
                    具体的损失函数可以通过参数<code class="docutils literal"><span class="pre">loss</span></code>
                    来控制。<code class="xref py py-class docutils literal"><span class="pre">SGDClassifier</span></code>
                    支持以下损失函数:
                </p>
                <blockquote>
                    <div>
                        <ul class="simple">
                            <li><code class="docutils literal"><span class="pre">loss=&quot;hinge&quot;</span></code>: (soft-margin) linear Support Vector Machine,</li>
                            <li><code class="docutils literal"><span class="pre">loss=&quot;modified_huber&quot;</span></code>: smoothed hinge loss,</li>
                            <li><code class="docutils literal"><span class="pre">loss=&quot;log&quot;</span></code>: logistic regression,</li>
                            <li>and all regression losses below.</li>
                        </ul>
                    </div>
                </blockquote>
                <p>
                    前两个损失函数是比较懒的，它们仅仅在有样本违反了边界约束的时候才会更新模型参数。这个懒汉特性
                    使得训练过程非常高效并且会产生一个稀疏模型，即使我们使用的是L2罚函数。
                </p>
                <p>
                    使用 <code class="docutils literal"><span class="pre">loss=&quot;log&quot;</span></code> 或 
                    <code class="docutils literal"><span class="pre">loss=&quot;modified_huber&quot;</span></code>
                    使得
                    <code class="docutils literal"><span class="pre">predict_proba</span></code>方法变得可用,
                    该方法给出一个概率向量的估计
                    <img class="math" src="./images/25a9728b809a691ce3374eaeab6255410cbff5d9.png" alt="P(y|x)" /> 
                    对每一个样本 <img class="math" src="./images/188c175aac0a8a9c22499336711b5d7256407254.png" 
                                    alt="x" />:
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;log&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span>                      
<span class="go">array([[ 0.00...,  0.99...]])</span>
</pre>
                    </div>
                </div>
                <p>
                    具体的罚函数可以通过参数
                    <code class="docutils literal"><span class="pre">penalty</span></code> 来设置。
                    SGD支持以下的惩罚项:
                </p>
                <blockquote>
                    <div>
                        <ul class="simple">
                            <li><code class="docutils literal"><span class="pre">penalty=&quot;l2&quot;</span></code>: L2 norm penalty on <code class="docutils literal"><span class="pre">coef_</span></code>.</li>
                            <li><code class="docutils literal"><span class="pre">penalty=&quot;l1&quot;</span></code>: L1 norm penalty on <code class="docutils literal"><span class="pre">coef_</span></code>.</li>
                            <li>
                                <code class="docutils literal"><span class="pre">penalty=&quot;elasticnet&quot;</span></code>: Convex combination of L2 and L1;
                                <code class="docutils literal"><span class="pre">(1</span> <span class="pre">-</span> <span class="pre">l1_ratio)</span> <span class="pre">*</span> <span class="pre">L2</span> <span class="pre">+</span> <span class="pre">l1_ratio</span> <span class="pre">*</span> <span class="pre">L1</span></code>.
                            </li>
                        </ul>
                    </div>
                </blockquote>
                <p>
                    惩罚因子的默认值是<code class="docutils literal"><span class="pre">penalty=&quot;l2&quot;</span></code>.
                    L1惩罚因子将会导致一个稀疏解，使得很多的系数都变成0。弹性网(Elastic Net)可以解决一些
                    L1惩罚因子在高度相关的属性的表示方面的缺点。
                    参数<code class="docutils literal"><span class="pre">l1_ratio</span></code>
                    用来控制L1 penalty和L2 penalty的凸组合。
                </p>
                <p>
                    <code class="xref py py-class docutils literal"><span class="pre">SGDClassifier</span></code>
                    支持多类别分类任务，通过组合多个二类分类器在一对全部(&#8220;one versus all&#8221; )(OVA)的框架下。
                    对<img class="math" src="./images/28e003020d0ae96250b302d7d779c791f183f707.png" alt="K" />
                    个类中的每一个类，一个二类分类器被训练用于区分该类和剩余的其他所有
                    <img class="math" src="./images/da6935a2384c32204e1462bc2b64c3b0c02aaee0.png" alt="K-1" />类。
                    在测试阶段，我们对每个分类器计算置信度得分(也就是到超平面的标记距离)并且选择具有最高置信度得分的类
                    作为被测试样本的类别标签。下图展示了 在鸢尾花数据集上使用 OVA 方法得到的结果。
                    The dashed lines 代表三个OVA分类器; 背景色显示了被三个分类器划定的决策平面。
                </p>
                <div class="figure align-center">
                    <a class="reference external image-reference" href="../auto_examples/linear_model/plot_sgd_iris.html">
                    <img alt="./images/plot_sgd_iris_0011.png" src="./images/plot_sgd_iris_0011.png" style="width: 600.0px; height: 450.0px;" /></a>
                </div>
                <p>
                    在多类分类任务中，<code>coef_</code> 是一个具有<code>shape=[n_classes,n_features]</code>的二维数组，而
                    <code>intercept_</code>是一个具有<code>shape=[n_classes]</code>的一维数组。
                    <code>coef_</code>的第i行包含了OVA分类器对于第i个类的权值向量；
                    类按照降序索引(请看属性<code>classes_</code>).
                    请注意，原则上因为SGD允许创建概率模型，所以<code>loss=&quot;log&quot;</code>和
                    <code>loss=&quot;modified_huber&quot;</code>更加适合于one-vs-all 类型的分类任务。
                </p>
                <p>
                    <code class="xref py py-class docutils literal"><span class="pre">SGDClassifier</span></code>
                    同时支持加权类(weighted classes)与加权实例(weighted instances),通过参数<code>class_weight</code>
                    和<code>sample_weight</code>来实现。请看下面的例子和<code>SGDClassifier.fit</code>的API详细解释。 
                </p>
                <div class="topic">
                    <p class="topic-title first">Examples:</p>
                    <ul class="simple">
                        <li><a class="reference internal" href="../auto_examples/linear_model/plot_sgd_separating_hyperplane.html#example-linear-model-plot-sgd-separating-hyperplane-py"><span>SGD: Maximum margin separating hyperplane</span></a>,</li>
                        <li><a class="reference internal" href="../auto_examples/linear_model/plot_sgd_iris.html#example-linear-model-plot-sgd-iris-py"><span>Plot multi-class SGD on the iris dataset</span></a></li>
                        <li><a class="reference internal" href="../auto_examples/linear_model/plot_sgd_weighted_samples.html#example-linear-model-plot-sgd-weighted-samples-py"><span>SGD: Weighted samples</span></a></li>
                        <li><a class="reference internal" href="../auto_examples/linear_model/plot_sgd_comparison.html#example-linear-model-plot-sgd-comparison-py"><span>Comparing various online solvers</span></a></li>
                        <li><a class="reference internal" href="../auto_examples/svm/plot_separating_hyperplane_unbalanced.html#example-svm-plot-separating-hyperplane-unbalanced-py"><span>SVM: Separating hyperplane for unbalanced classes</span></a> (See the <cite>Note</cite>)</li>
                    </ul>
                </div>
                <p>
                    <code>SGDClassifier</code> 支持平均SGD运行模式(ASGD). 平均模式可以用参数<code>`average=True`</code>
                    来开启。当使用ASGD时，学习率可以被调的很大或者干脆给个固定值，这可以带来训练加速的效果。
                </p>
                <p>
                    如果在SGD中使用logistic损失函数用于分类，另外一个平均SGD的版本叫做（Stochastic Average Gradient (SAG)）
                    也可以使用。你可以在<code>LogisticRegression</code>中找到。
                </p>

            </div>


            <div class="section" id="regression">
                <h2>1.5.2. 回归<a>¶</a></h2>
                <p>
                    <code>SGDRegressor</code>类实现了一个用于拟合线性回归模型的简单的随机梯度下降学习过程
                    并且支持不同类型的损失函数和正则化罚函数。 
                    <code>SGDRegressor</code> 非常适用于大规模训练样本(&gt; 10.000)的回归问题。对于其他问题，我们推荐
                    使用 
                   <code class="xref py py-class docutils literal"><span class="pre">Ridge</span></code>,
                    <code >Lasso</code>, 或者 <code>ElasticNet</code>.
                </p>
                <p>
                    具体的损失函数可以通过参数 loss 来控制。<code>SGDRegressor</code>支持以下损失函数:
                </p>
                <blockquote>
                    <div>
                        <ul class="simple">
                            <li><code class="docutils literal"><span class="pre">loss=&quot;squared_loss&quot;</span></code>: Ordinary least squares,</li>
                            <li><code class="docutils literal"><span class="pre">loss=&quot;huber&quot;</span></code>: Huber loss for robust regression,</li>
                            <li><code class="docutils literal"><span class="pre">loss=&quot;epsilon_insensitive&quot;</span></code>: linear Support Vector Regression.</li>
                        </ul>
                    </div>
                </blockquote>
                <p>
                    Huber 和 epsilon-insensitive 损失函数可被用于鲁棒回归。不敏感区域的宽度可以通过参数<code>epsilon</code>
                    来指定。此参数依赖于目标变量的scale。
                </p>
                <p>
                    <code>SGDRegressor</code> 支持平均SGD 作为 <code>SGDClassifier</code>。 
                    平均策略可以通过设置参数<code>`average=True`</code>来开启。
                </p>
                <p>
                    对于使用 squared loss 和 L2 penalty的回归，另一种带有平均策略的SGD版本
                    （Stochastic Average  Gradient (SAG)）是可用的，可以在Ridge中找到。
                </p>
            </div>


            <div class="section" id="stochastic-gradient-descent-for-sparse-data">
                <h2>1.5.3. SGD用于稀疏数据<a>¶</a></h2>
                <div class="admonition note">
                    <p class="first admonition-title">注意</p>
                    <p class="last">
                        由于截距或偏置(intercept)的学习率的缩减，SGD的稀疏实现产生与SGD的稠密实现稍微不一样的结果。 
                    </p>
                </div>
                <p>
                    SGD的稀疏实现对于<a class="reference external" href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.html">scipy.sparse</a>
                    所支持的任意格式的矩阵提供的稀疏数据都提供了内在支持。然而，为了最大程度的提高效率，使用
                    <a class="reference external" href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html">scipy.sparse.csr_matrix</a>
                    所定义的CSR矩阵格式。
                </p>
                <div class="topic">
                    <p class="topic-title first">例子:</p>
                    <ul class="simple">
                        <li><a class="reference internal" href="../auto_examples/text/document_classification_20newsgroups.html#example-text-document-classification-20newsgroups-py"><span>Classification of text documents using sparse features</span></a></li>
                    </ul>
                </div>
            </div>


            <div class="section" id="complexity">
                <h2>1.5.4. 复杂度<a>¶</a></h2>
                <p>
                    SGD的主要优点是高效率,时间花费基本上与训练样本的数量呈线性关系。如果X 是一个(n, p)矩阵，
                    训练时间花费是<img class="math" src="./images/93aa7f61c5ca120b643faa21a01cf5c504851f8c.png" alt="O(k n \bar p)" />, 
                    其中 k 是迭代次数，<img class="math" src="./images/2c5d2ee447e5be49d4e3b1e7d40c49c76329e686.png" alt="\bar p" />
                    是每个样本中非零属性的平均数量。
                </p>
                <p>
                    然而，最近的理论研究表明获得理想精度所花费的计算时间并不会随着训练集的规模的增加而增加。
                </p>
            </div>


            <div class="section" id="tips-on-practical-use">
                <h2>1.5.5. 应用实践指南<a>¶</a></h2>
                <blockquote>
                    <div>
                        <ul>
                            <li>
                                <p class="first">
                                    随机梯度下降算法对特征尺度比较敏感，所以我们高度建议你调整数据的坐标尺度。
                                    比如，将输入向量X中的每个属性的取值范围调整到[0,1]或[-1,+1];亦或归一化为零均值，方差为1。
                                    需要特别注意的是，你必须将<em>相同</em>的归一化操作应用在测试数据集上以获得有意义的结果。
                                    这可以通过使用<code>StandardScaler</code>轻松的做到:
                                </p>
                                <div class="highlight-python">
                                    <div class="highlight">
                                        <pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>  <span class="c1"># Don&#39;t cheat - fit only on training data</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>  <span class="c1"># apply same transformation to test data</span>
</pre>
                                    </div>
                                </div>
                                <p>
                                    如果你的属性有一个内在的尺度（比如说单词频率或指示器特征），那么就不再需要
                                    重新调整属性的坐标尺度了。
                                </p>
                            </li>
                            <li>
                                <p class="first">
                                    寻找合理的正则化参数<img class="math" src="./images/ad59b6e24a4a00ac621801f8d7513d68be654ab5.png" alt="\alpha" /> is
                                    的最佳做法是使用网格搜索交叉验证<code>GridSearchCV</code>, 通常情况下，其取值范围在
                                     <code class="docutils literal"><span class="pre">10.0**-np.arange(1,7)</span></code>。
                                </p>
                            </li>
                            <li>
                                <p class="first">
                                    Empirically, we found that SGD converges after observing
                                    approx. 10^6 training samples. Thus, a reasonable first guess
                                    for the number of iterations is <code class="docutils literal">
                                    <span class="pre">n_iter</span> <span class="pre">=</span> 
                                    <span class="pre">np.ceil(10**6</span> <span class="pre">/</span> 
                                    <span class="pre">n)</span></code>,
                                    where <code class="docutils literal"><span class="pre">n</span></code>
                                     is the size of the training set.
                                </p>
                            </li>
                            <li>
                                <p class="first">
                                    If you apply SGD to features extracted using PCA we found that
                                    it is often wise to scale the feature values by some constant <cite>c</cite>
                                    such that the average L2 norm of the training data equals one.
                                </p>
                            </li>
                            <li>
                                <p class="first">
                                    We found that Averaged SGD works best with a larger number of features
                                    and a higher eta0
                                </p>
                            </li>
                        </ul>
                    </div>
                </blockquote>
                <div class="topic">
                    <p class="topic-title first">参考文献:</p>
                    <ul class="simple">
                        <li>
                            <a class="reference external" href="yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">&#8220;Efficient BackProp&#8221;</a>
                            Y. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: Tricks
                            of the Trade 1998.
                        </li>
                    </ul>
                </div>
            </div>

            <div class="section" id="mathematical-formulation">
                <h2>1.5.6. 数学形式<a>¶</a></h2>
                <p>
                    给定训练样本集合 <img class="math" src="./images/4af6492b1bf93d4be0e1f0c75f85f65170ed94c0.png" alt="(x_1, y_1), \ldots, (x_n, y_n)" /> ，
                    其中，
                    <img class="math" src="./images/ed7238746ec0fb27be38187ff0576c83ecc0df8f.png" alt="x_i \in \mathbf{R}^n" /> 和
                     <img class="math" src="./images/bb280b0cec96f69df3ad1c8befd46a0a95e843ac.png" alt="y_i \in \{-1,1\}" />,
                    我们的目标是去学习一个线性得分函数
                    <img class="math" src="./images/3bd8e83af63059e1332f577c656fdff3e8158e74.png" alt="f(x) = w^T x + b" />
                     ，该函数的模型参数是
                    <img class="math" src="./images/e0f293430b347291b63495a02f47d30e35fa7229.png" alt="w \in \mathbf{R}^m" /> 
                    和截距 <img class="math" src="./images/8b75b89abc229b96382c47277d6f475f634940d9.png" alt="b \in \mathbf{R}" />. 
                    为了进行预测，我们只需要查看<img class="math" src="./images/14546c27a7b929642f7840acca5f851c503ea109.png" alt="f(x)" />的正负号
                    就可以了。
                    一个通用的寻找模型参数的方法是通过最小化带有正则化项的训练误差，由下式给出：
                </p>
                <div class="math">
                    <p class="text-center">
                    <img src="./images/4782697c94f74995ee99624f00633bba53f5245f.png" alt="E(w,b) = \frac{1}{n}\sum_{i=1}^{n} L(y_i, f(x_i)) + \alpha R(w)" />
                    </p>
                </div>
                <p>
                    其中， <img class="math" src="./images/0a5711c7a37994043b2bc3bb374adca232491762.png" alt="L" /> 
                    是一个用于测量模型拟合误差的损失函数，而
                    <img class="math" src="./images/9d86170e7de539c0ff999de09621ee0c7b6c8ed0.png" alt="R" />
                     是一个正则化项用于对模型的复杂性添加惩罚；
                    <img class="math" src="./images/7f2c98bf462cba6083cf18483ba9510e3c2fd3d3.png"
                                      alt="\alpha &gt; 0" />是一个非负超参数。
                </p>
                <p>
                    <img class="math" src="./images/0a5711c7a37994043b2bc3bb374adca232491762.png" alt="L" />
                    的不同选择会产生不同的分类器： 
                </p>
                <blockquote>
                    <div>
                        <ul class="simple">
                            <li>Hinge: (soft-margin) 支持向量分类器(SVM).</li>
                            <li>Log:   Logistic 回归.</li>
                            <li>Least-Squares:岭回归.</li>
                            <li>Epsilon-Insensitive: (soft-margin)支持向量回归器(SVR).</li>
                        </ul>
                    </div>
                </blockquote>
                <p>
                    所有上述损失函数都可以被认为是在分类误差(0-1损失)上的上界，如下所示：
                </p>
                <div class="figure align-center">
                    <a class="reference external image-reference" href="../auto_examples/linear_model/plot_sgd_loss_functions.html">
                    <img alt="./images/plot_sgd_loss_functions_0011.png" src="./images/plot_sgd_loss_functions_0011.png"
                          style="width: 600.0px; height: 450.0px;" /></a>
                </div>
                <p>
                    正则化项 <img class="math" src="./images/9d86170e7de539c0ff999de09621ee0c7b6c8ed0.png" alt="R" /> 有如下几种可选方案:
                </p>
                <blockquote>
                    <div>
                        <ul class="simple">
                            <li>L2 norm: <img class="math" src="./images/a16847a5124181d9b7752831345def32c7688718.png" alt="R(w) := \frac{1}{2} \sum_{i=1}^{n} w_i^2" />,</li>
                            <li>
                                L1 norm: <img class="math" src="./images/9dcd00d73a520a96fcccfa317471aab11d8b1ce7.png" alt="R(w) := \sum_{i=1}^{n} |w_i|" />, 
                                将会带来稀疏解。
                            </li>
                            <li>Elastic Net: <img class="math" src="./images/6d786bc0d556134fd2cd2ff743dd4b1a711bb541.png" alt="R(w) := \frac{\rho}{2} \sum_{i=1}^{n} w_i^2 + (1-\rho) \sum_{i=1}^{n} |w_i|" />, L2和L1的凸组合, 其中
                             <img class="math" src="./images/f574498915fa9e02eeb5141c24835d077eba3e75.png" alt="\rho" /> 由 <code class="docutils literal"><span class="pre">1</span> <span class="pre">-</span> <span class="pre">l1_ratio</span></code>给出。
                            </li>
                        </ul>
                    </div>
                </blockquote>
                <p>
                    下图展示了当<img class="math" src="./images/c12181d671649ed5892696252202bba516d28d3a.png" alt="R(w) = 1" />
                    时，在参数空间中的不同的正则化项的边界。
                </p>
                <div class="figure align-center">
                    <a class="reference external image-reference" href="../auto_examples/linear_model/plot_sgd_penalties.html">
                    <img alt="./images/plot_sgd_penalties_0011.png" src="./images/plot_sgd_penalties_0011.png" style="width: 600.0px; height: 450.0px;" /></a>
                </div>
                <div class="section">
                    <h3>1.5.6.1. SGD<a>¶</a></h3>
                    <p>
                        随机梯度下降是用于解决无约束优化问题的优化方法。与批量梯度下降法相对比，SGD通过每次只考虑单个训练样本的
                        方式来逼近<img class="math" src="./images/d5d48823181b7f7a9e96c9cbd3bb70d04abdbd9d.png" alt="E(w,b)" />
                        的真实梯度值。
                    </p>
                    <p>
                        <code >SGDClassifier</code> 实现了一个一阶的SGD学习过程。
                        算法在训练样本上迭代并且对每一个样本依据以下的更新规则更新模型参数：
                    </p>
                    <div class="math">
                        <p class="text-center">
                            <img src="./images/98cdc3aed40cb93594dbaaf045ea3e1abbd8edcb.png" alt="w \leftarrow w - \eta (\alpha \frac{\partial R(w)}{\partial w}
+ \frac{\partial L(w^T x_i + b, y_i)}{\partial w})" />
                        </p>
                    </div><p>
                        其中，<img class="math" src="./images/9172efc523d7488d4c3ed299d0be813de01503b8.png" alt="\eta" /> 
                    是学习率参数，用于控制算法在参数空间的搜索步长。截距 <img class="math" src="./images/5e87bf41a96deddf6cb485ff530f153f2590e9cc.png" alt="b" />
                    也以同样的方法被更新但是没有正则化项。
                    </p>
                    <p>
                        学习率 <img class="math" src="./images/9172efc523d7488d4c3ed299d0be813de01503b8.png" alt="\eta" />
                        可以设为常量也可以设为一个逐渐减小的量。 默认的学习率调度方法如下所示：
                         (<code class="docutils literal"><span class="pre">learning_rate='optimal'</span></code>)
                    </p>
                    <div class="math">
                        <p class="text-center"><img src="./images/f12f7e91b0d9d96b5e417194a1a348c718b981b1.png" alt="\eta^{(t)} = \frac {1}{\alpha  (t_0 + t)}" /></p>
                    </div>
                    <p>
                        其中 <img class="math" src="./images/ef9270877405055756d345facd044e4ab297f858.png" alt="t" />是时间步(总共有时间步：<cite>n_samples * n_iter</cite>
                        个), <img class="math" src="./images/fb761c2e199ad45ccf14767f48b88169479d840f.png" alt="t_0" /> 由一个启发式方法来确定：
                        such that the expected initial updates are comparable with the expected
                        size of the weights (this assuming that the norm of the training samples is
                        approx. 1). 它的精确定义可以在这儿找到：<code>_init_t</code>
                        在<code>BaseSGD</code>.
                    </p>
                    <p>
                        对于回归问题，默认的学习率调度方法是 inverse scaling
                        (<code class="docutils literal"><span class="pre">learning_rate='invscaling'</span></code>), 由下式给出：
                    </p>
                    <div class="math">
                        <p class="text-center"><img src="./images/1e8904a391eec39276c9794f76d1b3a247e5fdf0.png" alt="\eta^{(t)} = \frac{eta_0}{t^{power\_t}}" /></p>
                    </div>
                    <p>
                        其中， <img class="math" src="./images/17ed5419fbfe317f160a2be3da9c6f0ce53b5b86.png" alt="eta_0" /> 和
                         <img class="math" src="./images/138a8dd99b6da12de6f02b76afeda6e5885a7f7c.png" alt="power\_t" /> 
                        是超参数通过使用 <code>eta0</code> 和 <code>power_t</code> 来分别确定。
                    </p>
                    <p>
                        对于一个固定的学习率，使用<code>learning_rate='constant'</code> 
                        和<code>eta0</code>来指定学习率。
                    </p>
                    <p>
                        模型参数可以通过成员变量
                        <code class="docutils literal"><span class="pre">coef_</span></code> 和
                        <code class="docutils literal"><span class="pre">intercept_</span></code>获得:
                    </p>
                    <blockquote>
                        <div>
                            <ul class="simple">
                                <li>成员变量 <code class="docutils literal"><span class="pre">coef_</span></code> 存放权重信息 <img class="math" src="./images/8659700e6646cd91bc02c32affaa5ec046ee9935.png" alt="w" /></li>
                                <li>成员变量 <code class="docutils literal"><span class="pre">intercept_</span></code> 存放 <img class="math" src="./images/5e87bf41a96deddf6cb485ff530f153f2590e9cc.png" alt="b" /></li>
                            </ul>
                        </div>
                    </blockquote>
                    <div class="topic">
                        <p class="topic-title first">参考文献:</p>
                        <ul class="simple">
                            <li>
                                <a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.58.7377">
                                    &#8220;Solving large scale linear prediction problems using stochastic
                                    gradient descent algorithms&#8221;
                                </a>
                                T. Zhang - In Proceedings of ICML &#8216;04.
                            </li>
                            <li>
                                <a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.124.4696">&#8220;Regularization and variable selection via the elastic net&#8221;</a>
                                H. Zou, T. Hastie - Journal of the Royal Statistical Society Series B,
                                67 (2), 301-320.
                            </li>
                            <li>
                                <a class="reference external" href="http://arxiv.org/pdf/1107.2490v2.pdf">
                                    &#8220;Towards Optimal One Pass Large Scale Learning with
                                    Averaged Stochastic Gradient Descent&#8221;
                                </a>
                                Xu, Wei
                            </li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="section" id="implementation-details">
                <h2>1.5.7. 实现细节<a>¶</a></h2>
                <p>
                    The implementation of SGD is influenced by the <a class="reference external" href="http://leon.bottou.org/projects/sgd">Stochastic Gradient SVM</a>  of Léon Bottou. Similar to SvmSGD,
                    the weight vector is represented as the product of a scalar and a vector
                    which allows an efficient weight update in the case of L2 regularization.
                    In the case of sparse feature vectors, the intercept is updated with a
                    smaller learning rate (multiplied by 0.01) to account for the fact that
                    it is updated more frequently. Training examples are picked up sequentially
                    and the learning rate is lowered after each observed example. We adopted the
                    learning rate schedule from Shalev-Shwartz et al. 2007.
                    For multi-class classification, a &#8220;one versus all&#8221; approach is used.
                    We use the truncated gradient algorithm proposed by Tsuruoka et al. 2009
                    for L1 regularization (and the Elastic Net).
                    The code is written in Cython.
                </p>
                <div class="topic">
                    <p class="topic-title first">参考文献:</p>
                    <ul class="simple">
                        <li><a class="reference external" href="http://leon.bottou.org/projects/sgd">&#8220;Stochastic Gradient Descent&#8221;</a> L. Bottou - Website, 2010.</li>
                        <li><a class="reference external" href="http://leon.bottou.org/slides/largescale/lstut.pdf">&#8220;The Tradeoffs of Large Scale Machine Learning&#8221;</a> L. Bottou - Website, 2011.</li>
                        <li>
                            <a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.74.8513">&#8220;Pegasos: Primal estimated sub-gradient solver for svm&#8221;</a>
                            S. Shalev-Shwartz, Y. Singer, N. Srebro - In Proceedings of ICML &#8216;07.
                        </li>
                        <li>
                            <a class="reference external" href="http://www.aclweb.org/anthology/P/P09/P09-1054.pdf">&#8220;Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty&#8221;</a>
                            Y. Tsuruoka, J. Tsujii, S. Ananiadou -  In Proceedings of the AFNLP/ACL &#8216;09.
                        </li>
                    </ul>
                </div>
            </div>
        </div>
                
    </div>

    <div class="container">
        <div class="footer text-center">
            &copy; 2016-2100, 版权属于张金明博士 (BSD License).
        </div>
    </div>

</body>
</html>
