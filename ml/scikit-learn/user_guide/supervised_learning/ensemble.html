<!DOCTYPE html>
<html lang="zh-cn">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <link rel="stylesheet" href="../../../../css-fonts-js/css/bootstrap.min.css" />
    <link rel="stylesheet" href="../../../../css-fonts-js/css/nature.css" />
    <link rel="stylesheet" href="../../../../css-fonts-js/css/pygments.css" />

    <style>
        p {
            font-size: large;
        }
        li {
            font-size: large;
        }
    </style>

    <script src="../../../../css-fonts-js/js/jquery.min.js"></script>
    <script src="../../../../css-fonts-js/js/jquery.js"></script>
    <script src="../../../../css-fonts-js/js/bootstrap.min.js"></script>


    <title>人工智能研究网</title>
    <!--网站访问量统计-->
    <script>
        var _hmt = _hmt || [];
        (function () {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?e7836e37a4cb7584127a787e9b44e3f1";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>
</head>


<body>

    <nav class="navbar navbar-inverse">
        <div class="container">
            <div class="navbar-header">
                <a class="navbar-brand" href="../../../../index.html">studyai.cn</a>
            </div>
            <ul class="nav navbar-nav">
                <li class="active"><a href="../../../../index.html">网站主页</a></li>
                <li><a href="../../../../webmaster.html">站长风采</a></li>
                <li><a href="../../../../sponsor.html">赞助我们</a></li>
                <li><a href="../../../index.html">机器学习主页</a></li>
            </ul>
        </div>
    </nav>

    <div class="container">

        <div class="section">
            <h1 class="page-header">1.11 集成方法(Ensemble Methods)<a>¶</a></h1>

            <p>
                <strong>集成方法</strong> 的目标是组合若干基础估计器(base estimator由给定的学习算法构建)
                的预测来提高只使用单个估计器无法达到的泛化性/鲁棒性。
            </p>
            <p>集成方法通常有两个家族:</p>
            <ul>
                <li>
                    <p class="first">
                        在 <strong>平均方法(averaging methods)</strong>中, 驱动原则是构建若干独立的估计器然后对所有估计器的
                        输出做平均。从平均意义上来说，组合估计器通常总是比任意单个基础估计器更好，因为组合估计器的输出的方差
                        被减小了。
                    </p>
                    <p><strong>例子:</strong>
                     <a class="reference internal" href="#bagging"><span>Bagging methods</span></a>,
                     <a class="reference internal" href="#forest"><span>Forests of randomized trees</span></a>,
                     ...</p>
                </li>
                <li>
                    <p class="first">
                        作为对比, 在<strong>推举方法(boosting methods)</strong>中, 
                        若干基础估计器被序列化构建并且我们试图减小组合估计器的偏差。主要动机是
                        通过组合若干弱模型来获得一个强大的集成估计器。
                    </p>
                    <p><strong>例子:</strong> 
                    <a class="reference internal" href="#adaboost"><span>自适应推举(AdaBoost)</span></a>, 
                    <a class="reference internal" href="#gradient-boosting"><span>梯度树推举(Gradient Tree Boosting)</span></a>,
                     ...</p>
                </li>
            </ul>
            
            <div class="section" id="classification">
                <h2>1.11.1 Bagging meta-estimator<a>¶</a></h2>
                <p>
                    在集成方法中, bagging 方法形成了一类算法，这些算法在原始训练集的随机子集上构造一种黑盒估计器的若干
                    实例，然后总体合计这些估计器实例各自的预测结果产生一个最终的预测。这些方法用来减小基础估计器(如决策树)
                    的预测输出的方差。这通过引入随机性到它的构造过程中然后从这些带有随机性的估计器实例中制造出一个集成方法来实现。
                    在很多情况下，bagging方法可被视为是一种提高单个模型的预测能力的非常简单的方法，而且无需改动那个单个模型。
                    由于它们提供了一种减小过拟合的方法，
                    与那些通常只使用弱模型(比如shallow decision trees)的推举方法(boosting methods)相比，bagging方法配合强而复
                    杂的模型(例如fully developed decision trees)能够工作的很好。
                </p>
                <p>
                    Bagging方法有很多种，但大多数时候，他们之间的区别在于从训练集中抽取随机子集的方法不同：
                </p>
                <blockquote>

                    <div>
                        <ul class="simple">
                            <li>
                                当数据集的随机子集被抽取作为样本随机子集的时候，
                                这种方法被称为 Pasting <a class="reference internal" href="#b1999" id="id1">[B1999]</a>.
                            </li>
                            <li>
                                当样本以替换的方式被抽取时，这种方法被称作
                                Bagging <a class="reference internal" href="#b1996" id="id2">[B1996]</a>.
                            </li>
                            <li>
                                当数据集的随机子集被抽取作为特征的随机子集的时候，这种方法被称作 Random Subspaces <a class="reference internal" href="#h1998" id="id3">[H1998]</a>.
                            </li>
                            <li>
                                最后，当若干基础估计器构建在样本子集和特征子集上时，该方法被称为 Random Patches <a class="reference internal" href="#lg2012" id="id4">[LG2012]</a>.
                            </li>
                        </ul>
                    </div>
                </blockquote>
                <p>
                    在 scikit-learn 中, bagging 方法以统一的
                     BaggingClassifier meta-estimator的形式提供 (resp. BaggingRegressor), 
                    以用户指定的基础估计器和用于指定抽取随机子集的策略的参数作为输入。
                    特别的，max_samples 和 max_features 分别控制样本子集和特征子集的大小；而
                    bootstrap和bootstrap_features控制着样本和特征是否以替换方式被抽取出来。
                    当使用可用样本的子集时，泛化误差可以通过设置oob_score=True用out-of-bag的样本来估计。
                    作为一个例子，下面的片段展示了如何以KNeighborsClassifier为基础估计器来
                    实例化一个bagging集成估计器。构建每一个基础估计器使用了所有样本的50%的子集和所有特征的50%的子集。
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bagging</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">KNeighborsClassifier</span><span class="p">(),</span>
<span class="gp">... </span>                            <span class="n">max_samples</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre>
                    </div>
                </div>
                <div class="topic">
                    <p class="topic-title first">例子:</p>
                    <ul class="simple">
                        <li><a class="reference internal" href="../auto_examples/ensemble/plot_bias_variance.html#example-ensemble-plot-bias-variance-py"><span>Single estimator versus bagging: bias-variance decomposition</span></a></li>
                    </ul>
                </div>
                <div class="topic">
                    <p class="topic-title first">参考文献：</p>
                    <table class="docutils citation" frame="void" id="b1999" rules="none">
                        <colgroup><col class="label" /><col /></colgroup>
                        <tbody valign="top">
                            <tr>
                                <td class="label"><a class="fn-backref" href="#id1">[B1999]</a></td>
                                <td>
                                    L. Breiman, &#8220;Pasting small votes for classification in large
                                    databases and on-line&#8221;, Machine Learning, 36(1), 85-103, 1999.
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table class="docutils citation" frame="void" id="b1996" rules="none">
                        <colgroup><col class="label" /><col /></colgroup>
                        <tbody valign="top">
                            <tr>
                                <td class="label"><a class="fn-backref" href="#id2">[B1996]</a></td>
                                <td>
                                    L. Breiman, &#8220;Bagging predictors&#8221;, Machine Learning, 24(2),
                                    123-140, 1996.
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table class="docutils citation" frame="void" id="h1998" rules="none">
                        <colgroup><col class="label" /><col /></colgroup>
                        <tbody valign="top">
                            <tr>
                                <td class="label"><a class="fn-backref" href="#id3">[H1998]</a></td>
                                <td>
                                    T. Ho, &#8220;The random subspace method for constructing decision
                                    forests&#8221;, Pattern Analysis and Machine Intelligence, 20(8), 832-844,
                                    1998.
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table class="docutils citation" frame="void" id="lg2012" rules="none">
                        <colgroup><col class="label" /><col /></colgroup>
                        <tbody valign="top">
                            <tr>
                                <td class="label"><a class="fn-backref" href="#id4">[LG2012]</a></td>
                                <td>
                                    G. Louppe and P. Geurts, &#8220;Ensembles on Random Patches&#8221;,
                                    Machine Learning and Knowledge Discovery in Databases, 346-361, 2012.
                                </td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <div class="section" id="regression">
                <h2>1.11.2. Forests of randomized trees<a>¶</a></h2>
                <p>
                    scikit-learn的<code>sklearn.ensemble</code> 模块包含两个基于随机决策树的<strong>平均算法</strong>：
                    随机森林算法(RandomForest algorithm)和极大树(Extra-Trees method)方法。这两个算法都是特别为树算法设计的
                    扰动-组合(perturb-and-combine)技术<a class="reference internal" href="#b1998" id="id5">[B1998]</a>。
                    这意味着在分类器的构造过程中引入的随机性将会产生一组不同分类器的集合。最后，集成分类器的预测是将
                    各个独立分类器的预测做平均然后输出。
                </p>
                <p>
                    就像其他分类器一样，森林分类器必须要在两个数组上拟合：一个大小为<code class="docutils literal"><span class="pre">[n_samples,</span> <span class="pre">n_features]</span></code>
                    的包含训练样本的稀疏或稠密的数组X;以及一个大小为<code class="docutils literal"><span class="pre">[n_samples]</span></code>
                    的包含了目标变量(类标签)的数组Y:
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre>
                    </div>
                </div>
                <p>
                    与 <a class="reference internal" href="tree.html#tree"><span>决策树</span></a>一样, 森林算法也
                    被扩展用于多输出问题(<a class="reference internal" href="tree.html#tree-multioutput"><span>multi-output problems</span></a>)
                    :数组Y的大小为<code class="docutils literal"><span class="pre">[n_samples,</span> <span class="pre">n_outputs]</span></code>。
                </p>

                <div class="section" id="random-forests">
                    <h3>1.11.2.1. 随机森林(Random Forests)<a >¶</a></h3>
                    <p>
                        在随机森林(<code>RandomForestClassifier</code>类和<code>RandomForestRegressor</code>类)中，
                        森林中的每个树都是构造于一个样本，该样本以置换方式(replacement--> i.e., a bootstrap sample)从训练集
                        中抽取。另外，在划分节点构造树的过程中，选择的划分也不再是所有特征上的最好划分。相反的，选择的划分是在
                        所有特征的一个随机子集上的最好划分。作为此随机性的一个结果，随机森林的偏差相比于那些单个无随机性决策树
                        的偏差来说稍微有些大；但是，由于最后的平均操作，随机森林的预测方差也会降低(通常足以弥补偏差的增加)，
                        因此产生了整体上比较好的模型。
                    </p>
                    <p>
                        与原始的版本<a class="reference internal" href="#b2001" id="id6">[B2001]</a>对比 , 
                        scikit-learn 通过平均单个分类器的概率性预测来组合若干单个分类器去实现随机森林，而不是让每个分类器为单个类投票。
                    </p>
                </div>

                <div class="section" id="extremely-randomized-trees">
                    <h3>1.11.2.2. 极大随机树(Extremely Randomized Trees)<a>¶</a></h3>
                    <p>
                        在极大随机树(<code>ExtraTreesClassifier</code>类和<code>ExtraTreesRegressor</code>类)中，
                        在划分(or 分裂 splits)的计算方式上随机性又更进一步。就像在随机森林中那样，极大随机树也使用
                        候选特征集合的一个随机子集。与寻找最有判别力的阈值的策略不同，在极大随机树中每一个候选特征的阈值
                        也是随机抽取的。然后，这些随机产生的阈值中的最好的阈值将被用到分裂规则中去。这种策略通常能够
                        使得模型方差降的更低，但代价是模型的偏差也会稍微增加。
                    </p>
                    <div class="highlight-python">
                        <div class="highlight">
                            <pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">ExtraTreesClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>                             
<span class="go">0.97...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>                             
<span class="go">0.999...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">ExtraTreesClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.999</span>
<span class="go">True</span>
</pre>
                        </div>
                    </div>
                    <p class="text-center">
                        <a><img alt="./images/plot_forest_iris_0011.png" src="./images/plot_forest_iris_0011.png"
                                 style="width: 600.0px; height: 450.0px;" /></a>
                    </p>
                </div>

                <div class="section" id="parameters">
                    <h3>1.11.2.3. Parameters<a class="headerlink" href="#parameters" title="Permalink to this headline">¶</a></h3>
                    <p>
                        The main parameters to adjust when using these methods is <code class="docutils literal"><span class="pre">n_estimators</span></code>
                        and <code class="docutils literal"><span class="pre">max_features</span></code>. The former is the number of trees in the forest. The
                        larger the better, but also the longer it will take to compute. In
                        addition, note that results will stop getting significantly better
                        beyond a critical number of trees. The latter is the size of the random
                        subsets of features to consider when splitting a node. The lower the
                        greater the reduction of variance, but also the greater the increase in
                        bias. Empirical good default values are <code class="docutils literal"><span class="pre">max_features=n_features</span></code>
                        for regression problems, and <code class="docutils literal"><span class="pre">max_features=sqrt(n_features)</span></code> for
                        classification tasks (where <code class="docutils literal"><span class="pre">n_features</span></code> is the number of features
                        in the data). Good results are often achieved when setting <code class="docutils literal"><span class="pre">max_depth=None</span></code>
                        in combination with <code class="docutils literal"><span class="pre">min_samples_split=1</span></code> (i.e., when fully developing the
                        trees). Bear in mind though that these values are usually not optimal, and
                        might result in models that consume a lot of ram. The best parameter values
                        should always be cross-validated. In addition, note that in random forests,
                        bootstrap samples are used by default (<code class="docutils literal"><span class="pre">bootstrap=True</span></code>)
                        while the default strategy for extra-trees is to use the whole dataset
                        (<code class="docutils literal"><span class="pre">bootstrap=False</span></code>).
                        When using bootstrap sampling the generalization error can be estimated
                        on the left out or out-of-bag samples. This can be enabled by
                        setting <code class="docutils literal"><span class="pre">oob_score=True</span></code>.
                    </p>
                </div>


                <div class="section" id="parallelization">
                    <h3>1.11.2.4. Parallelization<a class="headerlink" href="#parallelization" title="Permalink to this headline">¶</a></h3>
                    <p>
                        Finally, this module also features the parallel construction of the trees
                        and the parallel computation of the predictions through the <code class="docutils literal"><span class="pre">n_jobs</span></code>
                        parameter. If <code class="docutils literal"><span class="pre">n_jobs=k</span></code> then computations are partitioned into
                        <code class="docutils literal"><span class="pre">k</span></code> jobs, and run on <code class="docutils literal"><span class="pre">k</span></code> cores of the machine. If <code class="docutils literal"><span class="pre">n_jobs=-1</span></code>
                        then all cores available on the machine are used. Note that because of
                        inter-process communication overhead, the speedup might not be linear
                        (i.e., using <code class="docutils literal"><span class="pre">k</span></code> jobs will unfortunately not be <code class="docutils literal"><span class="pre">k</span></code> times as
                        fast). Significant speedup can still be achieved though when building
                        a large number of trees, or when building a single tree requires a fair
                        amount of time (e.g., on large datasets).
                    </p>
                    <div class="topic">
                        <p class="topic-title first">Examples:</p>
                        <ul class="simple">
                            <li><a class="reference internal" href="../auto_examples/ensemble/plot_forest_iris.html#example-ensemble-plot-forest-iris-py"><span>Plot the decision surfaces of ensembles of trees on the iris dataset</span></a></li>
                            <li><a class="reference internal" href="../auto_examples/ensemble/plot_forest_importances_faces.html#example-ensemble-plot-forest-importances-faces-py"><span>Pixel importances with a parallel forest of trees</span></a></li>
                            <li><a class="reference internal" href="../auto_examples/plot_multioutput_face_completion.html#example-plot-multioutput-face-completion-py"><span>Face completion with a multi-output estimators</span></a></li>
                        </ul>
                    </div>
                    <div class="topic">
                        <p class="topic-title first">References</p>
                        <table class="docutils citation" frame="void" id="b2001" rules="none">
                            <colgroup><col class="label" /><col /></colgroup>
                            <tbody valign="top">
                                <tr>
                                    <td class="label"><a class="fn-backref" href="#id6">[B2001]</a></td>
                                    <td>
                                        <ol class="first last upperalpha simple" start="12">
                                            <li>Breiman, &#8220;Random Forests&#8221;, Machine Learning, 45(1), 5-32, 2001.</li>
                                        </ol>
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                        <table class="docutils citation" frame="void" id="b1998" rules="none">
                            <colgroup><col class="label" /><col /></colgroup>
                            <tbody valign="top">
                                <tr>
                                    <td class="label"><a class="fn-backref" href="#id5">[B1998]</a></td>
                                    <td>
                                        <ol class="first last upperalpha simple" start="12">
                                            <li>Breiman, &#8220;Arcing Classifiers&#8221;, Annals of Statistics 1998.</li>
                                        </ol>
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                        <table class="docutils citation" frame="void" id="gew2006" rules="none">
                            <colgroup><col class="label" /><col /></colgroup>
                            <tbody valign="top">
                                <tr>
                                    <td class="label">[GEW2006]</td>
                                    <td>
                                        P. Geurts, D. Ernst., and L. Wehenkel, &#8220;Extremely randomized
                                        trees&#8221;, Machine Learning, 63(1), 3-42, 2006.
                                    </td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>

                <div class="section" id="feature-importance-evaluation">
                    <span id="random-forest-feature-importance"></span><h3>1.11.2.5. Feature importance evaluation<a class="headerlink" href="#feature-importance-evaluation" title="Permalink to this headline">¶</a></h3>
                    <p>
                        The relative rank (i.e. depth) of a feature used as a decision node in a
                        tree can be used to assess the relative importance of that feature with
                        respect to the predictability of the target variable. Features used at
                        the top of the tree are used contribute to the final prediction decision
                        of a larger fraction of the input samples. The <strong>
                            expected fraction of
                            the samples
                        </strong> they contribute to can thus be used as an estimate of the
                        <strong>relative importance of the features</strong>.
                    </p>
                    <p>
                        By <strong>averaging</strong> those expected activity rates over several randomized
                        trees one can <strong>reduce the variance</strong> of such an estimate and use it
                        for feature selection.
                    </p>
                    <p>
                        The following example shows a color-coded representation of the relative
                        importances of each individual pixel for a face recognition task using
                        a <a class="reference internal" href="generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier" title="sklearn.ensemble.ExtraTreesClassifier"><code class="xref py py-class docutils literal"><span class="pre">ExtraTreesClassifier</span></code></a> model.
                    </p>
                    <div class="figure align-center">
                        <a><img alt=".//images//plot_forest_importances_faces_0011.png" src="./images/plot_forest_importances_faces_0011.png" style="width: 450.0px; height: 450.0px;" /></a>
                    </div>
                    <p>
                        In practice those estimates are stored as an attribute named
                        <code class="docutils literal"><span class="pre">feature_importances_</span></code> on the fitted model. This is an array with shape
                        <code class="docutils literal"><span class="pre">(n_features,)</span></code> whose values are positive and sum to 1.0. The higher
                        the value, the more important is the contribution of the matching feature
                        to the prediction function.
                    </p>
                    <div class="topic">
                        <p class="topic-title first">Examples:</p>
                        <ul class="simple">
                            <li><a class="reference internal" href="../auto_examples/ensemble/plot_forest_importances_faces.html#example-ensemble-plot-forest-importances-faces-py"><span>Pixel importances with a parallel forest of trees</span></a></li>
                            <li><a class="reference internal" href="../auto_examples/ensemble/plot_forest_importances.html#example-ensemble-plot-forest-importances-py"><span>Feature importances with forests of trees</span></a></li>
                        </ul>
                    </div>
                </div>

                <div class="section" id="totally-random-trees-embedding">
                    <span id="random-trees-embedding"></span><h3>1.11.2.6. Totally Random Trees Embedding<a class="headerlink" href="#totally-random-trees-embedding" title="Permalink to this headline">¶</a></h3>
                    <p>
                        <a class="reference internal" href="generated/sklearn.ensemble.RandomTreesEmbedding.html#sklearn.ensemble.RandomTreesEmbedding" title="sklearn.ensemble.RandomTreesEmbedding"><code class="xref py py-class docutils literal"><span class="pre">RandomTreesEmbedding</span></code></a> implements an unsupervised transformation of the
                        data.  Using a forest of completely random trees, <a class="reference internal" href="generated/sklearn.ensemble.RandomTreesEmbedding.html#sklearn.ensemble.RandomTreesEmbedding" title="sklearn.ensemble.RandomTreesEmbedding"><code class="xref py py-class docutils literal"><span class="pre">RandomTreesEmbedding</span></code></a>
                        encodes the data by the indices of the leaves a data point ends up in.  This
                        index is then encoded in a one-of-K manner, leading to a high dimensional,
                        sparse binary coding.
                        This coding can be computed very efficiently and can then be used as a basis
                        for other learning tasks.
                        The size and sparsity of the code can be influenced by choosing the number of
                        trees and the maximum depth per tree. For each tree in the ensemble, the coding
                        contains one entry of one. The size of the coding is at most <code class="docutils literal">
                            <span class="pre">n_estimators</span> <span class="pre">*</span> <span class="pre">2</span>
                            <span class="pre">**</span> <span class="pre">max_depth</span>
                        </code>, the maximum number of leaves in the forest.
                    </p>
                    <p>
                        As neighboring data points are more likely to lie within the same leaf of a tree,
                        the transformation performs an implicit, non-parametric density estimation.
                    </p>
                    <div class="topic">
                        <p class="topic-title first">Examples:</p>
                        <ul class="simple">
                            <li><a class="reference internal" href="../auto_examples/ensemble/plot_random_forest_embedding.html#example-ensemble-plot-random-forest-embedding-py"><span>Hashing feature transformation using Totally Random Trees</span></a></li>
                            <li>
                                <a class="reference internal" href="../auto_examples/manifold/plot_lle_digits.html#example-manifold-plot-lle-digits-py"><span>Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...</span></a> compares non-linear
                                dimensionality reduction techniques on handwritten digits.
                            </li>
                            <li>
                                <a class="reference internal" href="../auto_examples/ensemble/plot_feature_transformation.html#example-ensemble-plot-feature-transformation-py"><span>Feature transformations with ensembles of trees</span></a> compares
                                supervised and unsupervised tree based feature transformations.
                            </li>
                        </ul>
                    </div>
                    <div class="admonition seealso">
                        <p class="first admonition-title">See also</p>
                        <p class="last">
                            <a class="reference internal" href="manifold.html#manifold"><span>Manifold learning</span></a> techniques can also be useful to derive non-linear
                            representations of feature space, also these approaches focus also on
                            dimensionality reduction.
                        </p>
                    </div>
                </div>

            </div>

            <div class="section" id="multi-output-problems">
                <h2>1.11.3. AdaBoost<a>¶</a></h2>

            </div>

            <div class="section" id="complexity">
                <h2>1.11.4 Gradient Tree Boosting<a>¶</a></h2>

            </div>

            <div class="section" id="tips-on-practical-use">
                <h2>1.11.5 VotingClassifier<a>¶</a></h2>

            </div>


        </div>

    </div>


    <hr />
    <div class="container">
        <div class="footer text-center">
            &copy; 2016-2100, 版权属于张金明博士 (BSD License).
        </div>
    </div>

</body>
</html>
