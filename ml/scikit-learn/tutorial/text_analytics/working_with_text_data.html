<!DOCTYPE html>
<html lang="zh-cn">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <link rel="stylesheet" href="../../../../css-fonts-js/css/bootstrap.min.css" />
    <link rel="stylesheet" href="../../../../css-fonts-js/css/bootstrap-responsive.css" />
    <link rel="stylesheet" href="../../../../css-fonts-js/css/nature.css" />
    <link rel="stylesheet" href="../../../../css-fonts-js/css/pygments.css" />
    <link rel="stylesheet" href="../../../../css-fonts-js/css/gallery.css" />

    <script type="text/javascript" src="../../../../css-fonts-js/js/jquery.min.js"></script>
    <script type="text/javascript" src="../../../../css-fonts-js/js/jquery.js"></script>
    <script type="text/javascript" src="../../../../css-fonts-js/js/underscore.js"></script>
    <script type="text/javascript" src="../../../../css-fonts-js/js/doctools.js"></script>
    <script type="text/javascript" src="../../../../css-fonts-js/js/copybutton.js"></script>
    <script type="text/javascript" src="../../../../css-fonts-js/js/bootstrap.min.js"></script>

    <title>人工智能研究网/机器学习</title>
</head>

<body>

    <div class="header">
        <nav class="navbar navbar-inverse">
            <div class="container">
                <div class="navbar-header">
                    <a class="navbar-brand" href="#">studyai.cn</a>
                </div>
                <ul class="nav navbar-nav">
                    <li class="active"><a href="../../../../index.html">网站主页</a></li>
                    <li><a href="../../../../webmaster.html">站长风采</a></li>
                    <li><a href="../../../../contact.html">联系我们</a></li>
                    <li><a href="../../../../sponsor.html">赞助我们</a></li>
                    <li><a href="../../../index.html">机器学习主页</a></li>
                </ul>
                <ul class="nav navbar-nav navbar-right">
                    <li><a href="#"><span class="glyphicon glyphicon-user"></span> 登 录 </a></li>
                    <li><a href="#"><span class="glyphicon glyphicon-log-in"></span> 退 出 </a></li>
                </ul>
            </div>
        </nav>
    </div>
    
    <div class="container">

        <div class="section" id="working-with-text-data">
            <span id="text-data-tutorial"></span><h1 class="text-center bg-primary">在文本数据上进行机器学习<a class="headerlink" href="#working-with-text-data" title="Permalink to this headline">¶</a></h1>
            <p>
                这个教程的目的是探索一些主要的 <code class="docutils literal"><span class="pre">scikit-learn</span></code> 工具，
                用于以下单一的实际任务：在20个不同的话题上分析文本文档(新闻组的帖子)的集合。
            </p>
            <p>在这一章节我们将学到以下内容:</p>
            <blockquote>
                <div>
                    <ul class="simple">
                        <li>加载文件内容与类别信息</li>
                        <li>抽取适合于机器学习的特征向量</li>
                        <li>训练一个线性模型来执行分类任务</li>
                        <li>
                            使用网格搜索策略为特征抽取方法与分类器找到一个足够好的组合配置
                        </li>
                    </ul>
                </div>
            </blockquote>
            <div class="section" id="tutorial-setup">
                <h2 class="text-left bg-info">教程设置<a class="headerlink" href="#tutorial-setup" title="Permalink to this headline">¶</a></h2>
                <p>
                    想要开始学习本教程，你必须将<em>scikit-learn</em>以及其他的依赖包全部安装好。
                </p>
                <p>
                    请参考这里 <a class="reference internal" href="../../install.html#installation-instructions"><span>安装指南</span></a>
                    获得更多的信息。
                </p>
                <p>
                    这一教程的源码可以在你的 scikit-learn 文件夹中找得到:
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span>scikit-learn/doc/tutorial/text_analytics/
</pre>
                    </div>
                </div>
                <p>该教程的文件夹应该包括这勰文件夹:</p>
                <blockquote>
                    <div>
                        <ul class="simple">
                            <li><code class="docutils literal"><span class="pre">*.rst</span> <span class="pre">files</span></code> - 本教程的帮助文档源码</li>
                            <li><code class="docutils literal"><span class="pre">data</span></code> - 用于存放数据集的文件夹</li>
                            <li><code class="docutils literal"><span class="pre">skeletons</span></code> - 由于练习题的一些不完整脚本代码</li>
                            <li><code class="docutils literal"><span class="pre">solutions</span></code> - 练习题的答案</li>
                        </ul>
                    </div>
                </blockquote>
                <p>
                    为了不改动原始的文件，你可以拷贝一份到一个新文件夹，并起一个新的工作空间名称
                    <code class="docutils literal"><span class="pre">sklearn_tut_workspace</span></code>。
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span>% cp -r skeletons work_directory/sklearn_tut_workspace
</pre>
                    </div>
                </div>
                <p>
                    机器学习算法需要数据. 到每个 <code class="docutils literal"><span class="pre">$TUTORIAL_HOME/data</span></code>
                    子文件夹下面运行 <code class="docutils literal"><span class="pre">fetch_data.py</span></code> 脚本获取数据。
                </p>
                <p>例如:</p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span>% cd $TUTORIAL_HOME/data/languages
% less fetch_data.py
% python fetch_data.py
</pre>
                    </div>
                </div>
            </div>
            <div class="section" id="loading-the-20-newsgroups-dataset">
                <h2 class="text-left bg-info">加载 20 新闻组 数据集<a class="headerlink" href="#loading-the-20-newsgroups-dataset" title="Permalink to this headline">¶</a></h2>
                <p>
                    该数据集叫做 &#8220;Twenty Newsgroups&#8221;. 这儿使它的官方描述
                     <a class="reference external" href="http://people.csail.mit.edu/jrennie/20Newsgroups/">website</a>:
                </p>
                <blockquote>
                    <div>
                        20 Newsgroups 数据集是一个近似20,000个新闻组的文档集合，被划分为20种不同种类的新闻。
                        它一开始是由Ken Lang收集的, 用于他自己的论文 &#8220;Newsweeder: Learning to filter
                        netnews,&#8221;中。 现在，20 newsgroup 已经成为一个在机器学习技术在文本应用方面做
                        实验用的非常广泛的数据集，比如文本分类和文本聚类。
                    </div>
                </blockquote>
                <p>
                    下面我们将使用scikit-learn内建的数据加载工具来加载 20 newsgroups。你也可以从它的官网
                    自己手动下载数据集并使用<a class="reference internal" href="../../modules/generated/
                                   sklearn.datasets.load_files.html#sklearn.datasets.load_files" 
                                   title="sklearn.datasets.load_files"><code class="xref py py-func 
                                                                             docutils literal">
                    <span class="pre">sklearn.datasets.load_files</span></code></a>
                    函数指向你下载的数据集<code class="docutils literal"><span class="pre">20news-bydate-train</span></code>
                    子文件夹去解压缩包文件。
                </p>
                <p>
                    为了在第一个例子上获得加快的运行速度，我门将使用里面只包含 4个类的一部分数据集（总共有20个类）。 
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">categories</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;alt.atheism&#39;</span><span class="p">,</span> <span class="s1">&#39;soc.religion.christian&#39;</span><span class="p">,</span>
<span class="gp">... </span>              <span class="s1">&#39;comp.graphics&#39;</span><span class="p">,</span> <span class="s1">&#39;sci.med&#39;</span><span class="p">]</span>
</pre>
                    </div>
                </div>
                <p>我们可以以下面的方式加载我们想要的类别的那些数据集:</p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">twenty_train</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre>
                    </div>
                </div>
                <p>
                    该函数返回的对象是一个<code class="docutils literal"><span class="pre">scikit-learn</span></code> &#8220;bunch&#8221;:
                    一个简单的句柄对象，不仅可以用python <code class="docutils literal"><span class="pre">dict</span></code>
                    keys 来访问，还可以以 <code class="docutils literal"><span class="pre">object</span></code> attributes来访问,
                    例如
                    <code class="docutils literal"><span class="pre">target_names</span></code> 保存了类名称:
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">twenty_train</span><span class="o">.</span><span class="n">target_names</span>
<span class="go">[&#39;alt.atheism&#39;, &#39;comp.graphics&#39;, &#39;sci.med&#39;, &#39;soc.religion.christian&#39;]</span>
</pre>
                    </div>
                </div>
                <p>
                    这些文件被加载到内存中 <code class="docutils literal"><span class="pre">data</span></code> 
                    。用于参考的文件名也可以获得：
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">twenty_train</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="go">2257</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">twenty_train</span><span class="o">.</span><span class="n">filenames</span><span class="p">)</span>
<span class="go">2257</span>
</pre>
                    </div>
                </div>
                <p>我们输出一下第一个文件的第一行:</p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">twenty_train</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)[:</span><span class="mi">3</span><span class="p">]))</span>
<span class="go">From: sd345@city.ac.uk (Michael Collier)</span>
<span class="go">Subject: Converting images to HP LaserJet III?</span>
<span class="go">Nntp-Posting-Host: hampton</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">twenty_train</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">twenty_train</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
<span class="go">comp.graphics</span>
</pre>
                    </div>
                </div>
                <p>
                    监督学习算法需要为每一个训练样本添加一个类别标签。在这个案例中，类别标签是新闻组的名字
                    ，这个名字也正是保存该类的所有独立文档的文件夹的名字。
                </p>
                <p>
                    考虑到速度与存储效率，<code class="docutils literal"><span class="pre">scikit-learn</span></code> 
                    为每一个目标属性分配一个整数，此整数与类别标签的名称
                    (<code class="docutils literal"><span class="pre">target_names</span></code>)列表的索引相一致。
                    每一个样本的类别整数id被存储在<code class="docutils literal"><span class="pre">target</span></code>属性中:
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">twenty_train</span><span class="o">.</span><span class="n">target</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>
<span class="go">array([1, 1, 3, 3, 3, 3, 3, 2, 2, 2])</span>
</pre>
                    </div>
                </div>
                <p>我们可以使用整数类别id来索引对应的类别的字符串名称:</p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">twenty_train</span><span class="o">.</span><span class="n">target</span><span class="p">[:</span><span class="mi">10</span><span class="p">]:</span>
<span class="gp">... </span>    <span class="k">print</span><span class="p">(</span><span class="n">twenty_train</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
<span class="gp">...</span>
<span class="go">comp.graphics</span>
<span class="go">comp.graphics</span>
<span class="go">soc.religion.christian</span>
<span class="go">soc.religion.christian</span>
<span class="go">soc.religion.christian</span>
<span class="go">soc.religion.christian</span>
<span class="go">soc.religion.christian</span>
<span class="go">sci.med</span>
<span class="go">sci.med</span>
<span class="go">sci.med</span>
</pre>
                    </div>
                </div>
                <p>
                    你可以看到样本顺序已经被随机打乱了。当你想先在一个子集上训练和测试模型，然后将
                    子集上学习到的模型应用在整体数据集上去接着学习的时候，这是非常有用的。
                </p>
            </div>
            <div class="section" id="extracting-features-from-text-files">
                <h2 class="text-left bg-info">从文本文件抽取特征<a class="headerlink" href="#extracting-features-from-text-files" title="Permalink to this headline">¶</a></h2>
                <p >
                    为了在文本文件上执行机器学习算法，我们首要的工作就是将文本内容转换成数值类型的特征向量。
                </p>
                <div class="section" id="bags-of-words">
                    <h3>Bags of words(词袋)<a class="headerlink" href="#bags-of-words" title="Permalink to this headline">¶</a></h3>
                    <p>要做到这一点的最直观的方法就是使用<strong>词袋</strong>来表达本文内容(bags of words representation)：</p>
                    <blockquote>
                        <div>
                            <ol class="arabic simple">
                                <li>
                                    为训练集的所有文本文件中出现的单词分配一个整数型的id号：
                                    (比如构造一个从单词到整数索引的字典)。
                                </li>
                                <li>
                                    对每一个文件 <code class="docutils literal"><span class="pre">#i</span></code>, 计算它里面每一个单词<code class="docutils literal"><span class="pre">w</span></code>出现的次数，
                                    并把它存储到 <code class="docutils literal"><span class="pre">X[i,</span> <span class="pre">j]</span></code>里面作为特征
                                    <code class="docutils literal"><span class="pre">#j</span></code>的取值，其中 特征分量<code class="docutils literal"><span class="pre">j</span></code>
                                     正是单词 <code class="docutils literal"><span class="pre">w</span></code> 在字典中的索引。
                                </li>
                            </ol>
                        </div>
                    </blockquote>
                    <p>
                        <strong>词袋</strong>表达方式隐含的一个事实是：<code class="docutils literal"><span class="pre">n_features</span></code>
                        恰好是出现在所有文件中不同单词的数量。该数量通常大于100,000.
                    </p>
                    <p>
                        如果 <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">==</span> <span class="pre">10000</span></code>, 
                        将<code class="docutils literal"><span class="pre">X</span></code>存储为float32型的numpy数组，将需要
                        10000 x 100000 x 4 字节 = <strong>4GB in RAM</strong>。这在当今的计算机上是很难做到的。
                    </p>
                    <p>
                        Fortunately, <strong>most values in X will be zeros</strong> since for a given
                        document less than a couple thousands of distinct words will be
                        used. For this reason we say that bags of words are typically
                        <strong>high-dimensional sparse datasets</strong>. We can save a lot of memory by
                        only storing the non-zero parts of the feature vectors in memory.
                    </p>
                    <p>
                        <code class="docutils literal"><span class="pre">scipy.sparse</span></code> matrices are data structures that do exactly this,
                        and <code class="docutils literal"><span class="pre">scikit-learn</span></code> has built-in support for these structures.
                    </p>
                </div>
                <div class="section" id="tokenizing-text-with-scikit-learn">
                    <h3>使用<code class="docutils literal"><span class="pre">scikit-learn</span></code>标记文本<a class="headerlink" href="#tokenizing-text-with-scikit-learn" title="Permalink to this headline">¶</a></h3>
                    <p>
                        Text preprocessing, tokenizing and filtering of stopwords are included in a high level component that is able to build a
                        dictionary of features and transform documents to feature vectors:
                    </p>
                    <div class="highlight-python">
                        <div class="highlight">
                            <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">count_vect</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train_counts</span> <span class="o">=</span> <span class="n">count_vect</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">twenty_train</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train_counts</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(2257, 35788)</span>
</pre>
                        </div>
                    </div>
                    <p>
                        <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-class docutils literal"><span class="pre">CountVectorizer</span></code></a> supports counts of N-grams of words or consequective characters.
                        Once fitted, the vectorizer has built a dictionary of feature indices:
                    </p>
                    <div class="highlight-python">
                        <div class="highlight">
                            <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">count_vect</span><span class="o">.</span><span class="n">vocabulary_</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">u&#39;algorithm&#39;</span><span class="p">)</span>
<span class="go">4690</span>
</pre>
                        </div>
                    </div>
                    <p>
                        The index value of a word in the vocabulary is linked to its frequency
                        in the whole training corpus.
                    </p>
                </div>
                <div class="section" id="from-occurrences-to-frequencies">
                    <h3>从频数到频率<a class="headerlink" href="#from-occurrences-to-frequencies" title="Permalink to this headline">¶</a></h3>
                    <p>
                        Occurrence count is a good start but there is an issue: longer
                        documents will have higher average count values than shorter documents,
                        even though they might talk about the same topics.
                    </p>
                    <p>
                        To avoid these potential discrepancies it suffices to divide the
                        number of occurrences of each word in a document by the total number
                        of words in the document: these new features are called <code class="docutils literal"><span class="pre">tf</span></code> for Term
                        Frequencies.
                    </p>
                    <p>
                        Another refinement on top of tf is to downscale weights for words
                        that occur in many documents in the corpus and are therefore less
                        informative than those that occur only in a smaller portion of the
                        corpus.
                    </p>
                    <p>
                        This downscaling is called <a class="reference external" href="http://en.wikipedia.org/wiki/Tf–idf">tf–idf</a> for &#8220;Term Frequency times
                        Inverse Document Frequency&#8221;.
                    </p>
                    <p>Both <strong>tf</strong> and <strong>tf–idf</strong> can be computed as follows:</p>
                    <div class="highlight-python">
                        <div class="highlight">
                            <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfTransformer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tf_transformer</span> <span class="o">=</span> <span class="n">TfidfTransformer</span><span class="p">(</span><span class="n">use_idf</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_counts</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train_tf</span> <span class="o">=</span> <span class="n">tf_transformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train_counts</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train_tf</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(2257, 35788)</span>
</pre>
                        </div>
                    </div>
                    <p>
                        In the above example-code, we firstly use the <code class="docutils literal"><span class="pre">fit(..)</span></code> method to fit our
                        estimator to the data and secondly the <code class="docutils literal"><span class="pre">transform(..)</span></code> method to transform
                        our count-matrix to a tf-idf representation.
                        These two steps can be combined to achieve the same end result faster
                        by skipping redundant processing. This is done through using the
                        <code class="docutils literal"><span class="pre">fit_transform(..)</span></code> method as shown below, and as mentioned in the note
                        in the previous section:
                    </p>
                    <div class="highlight-python">
                        <div class="highlight">
                            <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tfidf_transformer</span> <span class="o">=</span> <span class="n">TfidfTransformer</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train_tfidf</span> <span class="o">=</span> <span class="n">tfidf_transformer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train_counts</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train_tfidf</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(2257, 35788)</span>
</pre>
                        </div>
                    </div>
                </div>
            </div>
            <div class="section" id="training-a-classifier">
                <h2 class="text-left bg-info">训练一个分类器<a class="headerlink" href="#training-a-classifier" title="Permalink to this headline">¶</a></h2>
                <p>
                    Now that we have our features, we can train a classifier to try to predict
                    the category of a post. Let&#8217;s start with a <a class="reference internal" href="../../modules/naive_bayes.html#naive-bayes"><span>naïve Bayes</span></a>
                    classifier, which
                    provides a nice baseline for this task. <code class="docutils literal"><span class="pre">scikit-learn</span></code> includes several
                    variants of this classifier; the one most suitable for word counts is the
                    multinomial variant:
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_tfidf</span><span class="p">,</span> <span class="n">twenty_train</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre>
                    </div>
                </div>
                <p>
                    To try to predict the outcome on a new document we need to extract
                    the features using almost the same feature extracting chain as before.
                    The difference is that we call <code class="docutils literal"><span class="pre">transform</span></code> instead of <code class="docutils literal"><span class="pre">fit_transform</span></code>
                    on the transformers, since they have already been fit to the training set:
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">docs_new</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;God is love&#39;</span><span class="p">,</span> <span class="s1">&#39;OpenGL on the GPU is fast&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new_counts</span> <span class="o">=</span> <span class="n">count_vect</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">docs_new</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new_tfidf</span> <span class="o">=</span> <span class="n">tfidf_transformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_new_counts</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predicted</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new_tfidf</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">doc</span><span class="p">,</span> <span class="n">category</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">docs_new</span><span class="p">,</span> <span class="n">predicted</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">%r</span><span class="s1"> =&gt; </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">twenty_train</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">category</span><span class="p">]))</span>
<span class="gp">...</span>
<span class="go">&#39;God is love&#39; =&gt; soc.religion.christian</span>
<span class="go">&#39;OpenGL on the GPU is fast&#39; =&gt; comp.graphics</span>
</pre>
                    </div>
                </div>
            </div>
            <div class="section" id="building-a-pipeline">
                <h2 class="text-left bg-info">构建一个管道流<a class="headerlink" href="#building-a-pipeline" title="Permalink to this headline">¶</a></h2>
                <p>
                    In order to make the vectorizer =&gt; transformer =&gt; classifier easier
                    to work with, <code class="docutils literal"><span class="pre">scikit-learn</span></code> provides a <code class="docutils literal"><span class="pre">Pipeline</span></code> class that behaves
                    like a compound classifier:
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text_clf</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s1">&#39;vect&#39;</span><span class="p">,</span> <span class="n">CountVectorizer</span><span class="p">()),</span>
<span class="gp">... </span>                     <span class="p">(</span><span class="s1">&#39;tfidf&#39;</span><span class="p">,</span> <span class="n">TfidfTransformer</span><span class="p">()),</span>
<span class="gp">... </span>                     <span class="p">(</span><span class="s1">&#39;clf&#39;</span><span class="p">,</span> <span class="n">MultinomialNB</span><span class="p">()),</span>
<span class="gp">... </span><span class="p">])</span>
</pre>
                    </div>
                </div>
                <p>
                    The names <code class="docutils literal"><span class="pre">vect</span></code>, <code class="docutils literal"><span class="pre">tfidf</span></code> and <code class="docutils literal"><span class="pre">clf</span></code> (classifier) are arbitrary.
                    We shall see their use in the section on grid search, below.
                    We can now train the model with a single command:
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">text_clf</span> <span class="o">=</span> <span class="n">text_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">twenty_train</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">twenty_train</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre>
                    </div>
                </div>
            </div>
            <div class="section" id="evaluation-of-the-performance-on-the-test-set">
                <h2 class="text-left bg-info">在测试集上评估性能<a class="headerlink" href="#evaluation-of-the-performance-on-the-test-set" title="Permalink to this headline">¶</a></h2>
                <p>Evaluating the predictive accuracy of the model is equally easy:</p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">twenty_test</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;test&#39;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">docs_test</span> <span class="o">=</span> <span class="n">twenty_test</span><span class="o">.</span><span class="n">data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predicted</span> <span class="o">=</span> <span class="n">text_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">docs_test</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">twenty_test</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>            
<span class="go">0.834...</span>
</pre>
                    </div>
                </div>
                <p>
                    I.e., we achieved 83.4% accuracy. Let&#8217;s see if we can do better with a
                    linear <a class="reference internal" href="../../modules/svm.html#svm"><span>support vector machine (SVM)</span></a>,
                    which is widely regarded as one of
                    the best text classification algorithms (although it&#8217;s also a bit slower
                    than naïve Bayes). We can change the learner by just plugging a different
                    classifier object into our pipeline:
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text_clf</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s1">&#39;vect&#39;</span><span class="p">,</span> <span class="n">CountVectorizer</span><span class="p">()),</span>
<span class="gp">... </span>                     <span class="p">(</span><span class="s1">&#39;tfidf&#39;</span><span class="p">,</span> <span class="n">TfidfTransformer</span><span class="p">()),</span>
<span class="gp">... </span>                     <span class="p">(</span><span class="s1">&#39;clf&#39;</span><span class="p">,</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;hinge&#39;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span>
<span class="gp">... </span>                                           <span class="n">alpha</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)),</span>
<span class="gp">... </span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">text_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">twenty_train</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">twenty_train</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predicted</span> <span class="o">=</span> <span class="n">text_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">docs_test</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">twenty_test</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>            
<span class="go">0.912...</span>
</pre>
                    </div>
                </div>
                <p>
                    <code class="docutils literal"><span class="pre">scikit-learn</span></code> further provides utilities for more detailed performance
                    analysis of the results:
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification_report</span><span class="p">(</span><span class="n">twenty_test</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">predicted</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">target_names</span><span class="o">=</span><span class="n">twenty_test</span><span class="o">.</span><span class="n">target_names</span><span class="p">))</span>
<span class="gp">... </span>                                        
<span class="go">                        precision    recall  f1-score   support</span>
<span class="go">           alt.atheism       0.95      0.81      0.87       319</span>
<span class="go">         comp.graphics       0.88      0.97      0.92       389</span>
<span class="go">               sci.med       0.94      0.90      0.92       396</span>
<span class="go">soc.religion.christian       0.90      0.95      0.93       398</span>
<span class="go">           avg / total       0.92      0.91      0.91      1502</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">twenty_test</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span>
<span class="go">array([[258,  11,  15,  35],</span>
<span class="go">       [  4, 379,   3,   3],</span>
<span class="go">       [  5,  33, 355,   3],</span>
<span class="go">       [  5,  10,   4, 379]])</span>
</pre>
                    </div>
                </div>
                <p>
                    As expected the confusion matrix shows that posts from the newsgroups
                    on atheism and christian are more often confused for one another than
                    with computer graphics.
                </p>
            </div>
            <div class="section" id="parameter-tuning-using-grid-search">
                <h2 class="text-left bg-info">使用网格搜索调节参数<a class="headerlink" href="#parameter-tuning-using-grid-search" title="Permalink to this headline">¶</a></h2>
                <p>
                    We&#8217;ve already encountered some parameters such as <code class="docutils literal"><span class="pre">use_idf</span></code> in the
                    <code class="docutils literal"><span class="pre">TfidfTransformer</span></code>. Classifiers tend to have many parameters as well;
                    e.g., <code class="docutils literal"><span class="pre">MultinomialNB</span></code> includes a smoothing parameter <code class="docutils literal"><span class="pre">alpha</span></code> and
                    <code class="docutils literal"><span class="pre">SGDClassifier</span></code> has a penalty parameter <code class="docutils literal"><span class="pre">alpha</span></code> and configurable loss
                    and penalty terms in the objective function (see the module documentation,
                    or use the Python <code class="docutils literal"><span class="pre">help</span></code> function, to get a description of these).
                </p>
                <p>
                    Instead of tweaking the parameters of the various components of the
                    chain, it is possible to run an exhaustive search of the best
                    parameters on a grid of possible values. We try out all classifiers
                    on either words or bigrams, with or without idf, and with a penalty
                    parameter of either 0.01 or 0.001 for the linear SVM:
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.grid_search</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;vect__ngram_range&#39;</span><span class="p">:</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)],</span>
<span class="gp">... </span>              <span class="s1">&#39;tfidf__use_idf&#39;</span><span class="p">:</span> <span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">),</span>
<span class="gp">... </span>              <span class="s1">&#39;clf__alpha&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">),</span>
<span class="gp">... </span><span class="p">}</span>
</pre>
                    </div>
                </div>
                <p>
                    Obviously, such an exhaustive search can be expensive. If we have multiple
                    CPU cores at our disposal, we can tell the grid searcher to try these eight
                    parameter combinations in parallel with the <code class="docutils literal"><span class="pre">n_jobs</span></code> parameter. If we give
                    this parameter a value of <code class="docutils literal"><span class="pre">-1</span></code>, grid search will detect how many cores
                    are installed and uses them all:
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gs_clf</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">text_clf</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre>
                    </div>
                </div>
                <p>
                    The grid search instance behaves like a normal <code class="docutils literal"><span class="pre">scikit-learn</span></code>
                    model. Let&#8217;s perform the search on a smaller subset of the training data
                    to speed up the computation:
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gs_clf</span> <span class="o">=</span> <span class="n">gs_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">twenty_train</span><span class="o">.</span><span class="n">data</span><span class="p">[:</span><span class="mi">400</span><span class="p">],</span> <span class="n">twenty_train</span><span class="o">.</span><span class="n">target</span><span class="p">[:</span><span class="mi">400</span><span class="p">])</span>
</pre>
                    </div>
                </div>
                <p>
                    The result of calling <code class="docutils literal"><span class="pre">fit</span></code> on a <code class="docutils literal"><span class="pre">GridSearchCV</span></code> object is a classifier
                    that we can use to <code class="docutils literal"><span class="pre">predict</span></code>:
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">twenty_train</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">gs_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="s1">&#39;God is love&#39;</span><span class="p">])]</span>
<span class="go">&#39;soc.religion.christian&#39;</span>
</pre>
                    </div>
                </div>
                <p>
                    but otherwise, it&#8217;s a pretty large and clumsy object. We can, however, get the
                    optimal parameters out by inspecting the object&#8217;s <code class="docutils literal"><span class="pre">grid_scores_</span></code> attribute,
                    which is a list of parameters/score pairs. To get the best scoring attributes,
                    we can do:
                </p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">best_parameters</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">gs_clf</span><span class="o">.</span><span class="n">grid_scores_</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param_name</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">parameters</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
<span class="gp">... </span>    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">: </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">best_parameters</span><span class="p">[</span><span class="n">param_name</span><span class="p">]))</span>
<span class="gp">...</span>
<span class="go">clf__alpha: 0.001</span>
<span class="go">tfidf__use_idf: True</span>
<span class="go">vect__ngram_range: (1, 1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">score</span>                                              
<span class="go">0.900...</span>
</pre>
                    </div>
                </div>
                <div class="section" id="exercises">
                    <h3>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h3>
                    <p>
                        To do the exercises, copy the content of the &#8216;skeletons&#8217; folder as
                        a new folder named &#8216;workspace&#8217;:
                    </p>
                    <div class="highlight-python">
                        <div class="highlight">
                            <pre><span></span>% cp -r skeletons workspace
</pre>
                        </div>
                    </div>
                    <p>
                        You can then edit the content of the workspace without fear of loosing
                        the original exercise instructions.
                    </p>
                    <p>Then fire an ipython shell and run the work-in-progress script with:</p>
                    <div class="highlight-python">
                        <div class="highlight">
                            <pre><span></span>[1] %run workspace/exercise_XX_script.py arg1 arg2 arg3
</pre>
                        </div>
                    </div>
                    <p>
                        If an exception is triggered, use <code class="docutils literal"><span class="pre">%debug</span></code> to fire-up a post
                        mortem ipdb session.
                    </p>
                    <p>Refine the implementation and iterate until the exercise is solved.</p>
                    <p>
                        <strong>
                            For each exercise, the skeleton file provides all the necessary import
                            statements, boilerplate code to load the data and sample code to evaluate
                            the predictive accurracy of the model.
                        </strong>
                    </p>
                </div>
            </div>
            <div class="section" id="exercise-1-language-identification">
                <h2 class="text-left bg-info">练习 1: 语言识别<a class="headerlink" href="#exercise-1-language-identification" title="Permalink to this headline">¶</a></h2>
                <ul class="simple">
                    <li>
                        Write a text classification pipeline using a custom preprocessor and
                        <code class="docutils literal"><span class="pre">CharNGramAnalyzer</span></code> using data from Wikipedia articles as training set.
                    </li>
                    <li>Evaluate the performance on some held out test set.</li>
                </ul>
                <p>ipython command line:</p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span>%run workspace/exercise_01_language_train_model.py data/languages/paragraphs/
</pre>
                    </div>
                </div>
            </div>
            <div class="section" id="exercise-2-sentiment-analysis-on-movie-reviews">
                <h2 class="text-left bg-info">练习 2: 电影评论的情感分析<a class="headerlink" href="#exercise-2-sentiment-analysis-on-movie-reviews" title="Permalink to this headline">¶</a></h2>
                <ul class="simple">
                    <li>
                        Write a text classification pipeline to classify movie reviews as either
                        positive or negative.
                    </li>
                    <li>Find a good set of parameters using grid search.</li>
                    <li>Evaluate the performance on a held out test set.</li>
                </ul>
                <p>ipython command line:</p>
                <div class="highlight-python">
                    <div class="highlight">
                        <pre><span></span>%run workspace/exercise_02_sentiment.py data/movie_reviews/txt_sentoken/
</pre>
                    </div>
                </div>
            </div>
            <div class="section" id="exercise-3-cli-text-classification-utility">
                <h2 class="text-left bg-info">练习 3: CLI 文本分类应用<a class="headerlink" href="#exercise-3-cli-text-classification-utility" title="Permalink to this headline">¶</a></h2>
                <p>
                    Using the results of the previous exercises and the <code class="docutils literal"><span class="pre">cPickle</span></code>
                    module of the standard library, write a command line utility that
                    detects the language of some text provided on <code class="docutils literal"><span class="pre">stdin</span></code> and estimate
                    the polarity (positive or negative) if the text is written in
                    English.
                </p>
                <p>
                    Bonus point if the utility is able to give a confidence level for its
                    predictions.
                </p>
            </div>
            <div class="section" id="where-to-from-here">
                <h2 class="text-left bg-info">以此为基础，继续研究吧<a class="headerlink" href="#where-to-from-here" title="Permalink to this headline">¶</a></h2>
                <p>
                    Here are a few suggestions to help further your scikit-learn intuition
                    upon the completion of this tutorial:
                </p>
                <ul class="simple">
                    <li>
                        Try playing around with the <code class="docutils literal"><span class="pre">analyzer</span></code> and <code class="docutils literal"><span class="pre">token</span> <span class="pre">normalisation</span></code> under
                        <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-class docutils literal"><span class="pre">CountVectorizer</span></code></a>
                    </li>
                    <li>
                        If you don&#8217;t have labels, try using
                        <a class="reference internal" href="../../auto_examples/text/document_clustering.html#example-text-document-clustering-py"><span>Clustering</span></a>
                        on your problem.
                    </li>
                    <li>
                        If you have multiple labels per document, e.g categories, have a look
                        at the <a class="reference internal" href="../../modules/multiclass.html#multiclass"><span>Multiclass and multilabel section</span></a>
                    </li>
                    <li>
                        Try using <a class="reference internal" href="../../modules/decomposition.html#lsa"><span>Truncated SVD</span></a> for
                        <a class="reference external" href="http://en.wikipedia.org/wiki/Latent_semantic_analysis">latent semantic analysis</a>.
                    </li>
                    <li>
                        Have a look at using
                        <a class="reference internal" href="../../auto_examples/applications/plot_out_of_core_classification.html#example-applications-plot-out-of-core-classification-py"><span>Out-of-core Classification</span></a> to
                        learn from data that would not fit into the computer main memory.
                    </li>
                    <li>
                        Have a look at the <a class="reference internal" href="../../modules/feature_extraction.html#hashing-vectorizer"><span>Hashing Vectorizer</span></a>
                        as a memory efficient alternative to <a class="reference internal" href="../../modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-class docutils literal"><span class="pre">CountVectorizer</span></code></a>.
                    </li>
                </ul>
            </div>
        </div>

    </div>
   
</body>
</html>